---
layout: default
---

<!-- Information -->
<div class="info">
  <h2 style="color: #303030;">Hi there!</h2>
  <p>
    &nbsp; 안녕하세요 :) &nbsp;   
    인공지능에 매료되어, 인공지능 기술을 통한 현실의 문제 해결에 집중하고 있는  
    Machine Learning Engineer, Moon입니다.
    다양한 인공지능 활용 분야들 중에서도, 자연어 처리에 초점을 맞추고 있습니다. 
    사람과 자연스럽게 의사소통 가능한 인공지능 모델 개발을 목표로 삼고 있습니다. 
    목표 달성을 위해 다양한 연구 주제를 직접 선정하고 구현하며, 
    끊임없이 한 발짝씩 목표에 다가가는 중입니다.
  </p>
  <div class="btn-block">
    <a href="{{ '/resume' | relative_url }}" class="btn-type-1">Resume</a>
    <a href="https://github.com/moon23k" class="btn-type-1">Github</a>
    <a href="https://shy-vole-f74.notion.site/Hello-I-m-Moon-e1ecc2e40b32405e997713cfb44e4f3c?pvs=4" class="btn-type-1">Study Log</a>
  </div>
</div>


<!-- First Research -->
<div class="container">
  
  <div class="full-width">
    <h3>Model Structure</h3>
    <p>
      &nbsp; 머신러닝 엔지니어링에서 모델의 구조적 디자인은 성능에 결정적인 영향을 미치는 주요 요소입니다. 특히 Encoder와 Decoder가 모두 필요한 Sequence to Sequence 구조에서, 모델 디자인의 중요성은 더욱 강조됩니다. 아래의 프로젝트들은 학습 방법 및 데이터 처리 등 다른 변수를 일정하게 유지하면서 모델 구조에 중점을 두어, 각 구조의 특징을 파악하는데 집중합니다.
    </p>
  </div>

  <div class="parallel">
    <div class='parallel-item'>
      <h3>RNN Seq2Seq</h3>
      <ul>
        <li>
          <p> &nbsp;
            세 가지 순환 신경망(RNN, LSTM, GRU)을 활용한 Seq2Seq 모델을 구현하고, 각 네트워크 모델 별 자연어 생성 과제 성능 비교 검증
          </p>
          <div class="btn-list">
            <a href="{{ '/rnn_seq2seq' | relative_url }}" class="btn-type-1">Detail</a>
            <a href="https://github.com/moon23k/RNN_Seq2Seq" class="btn-type-1">Code</a>
          </div>

        </li>
      </ul>
    </div>

    <div class='parallel-item'>
      <h3>RNN Seq2Seq with Attention</h3>
      <ul>
        <li>
          <p> &nbsp;
            GRU Seq2Seq 모델에서 Additive, Dot Product, Scaled Dot Product 어텐션 메커니즘 사용에 따른 자연어 생성 성능 비교 검증
          </p>
          <div class="btn-list">
            <a href="{{ '/rnn_seq2seq_attn' | relative_url }}" class="btn-type-1">Detail</a>
            <a href="https://github.com/moon23k/RNN_Seq2Seq_Attention" class="btn-type-1">Code</a>
          </div>

        </li>
      </ul>
    </div>

  </div>

  <div class="parallel">
    <div class='parallel-item'>
      <h3>Transformer Study</h3>
      <ul>
        <li>
          <p> &nbsp;
            Transformer의 모든 구성 요소를 처음부터 직접 구현하고, Pytorch 라이브러리에서 제공하는 Transformer 모델과 자연어 생성 성능 비교 검증
          </p>
          <div class="btn-list">
            <a href="{{ '/transformer' | relative_url }}" class="btn-type-1">Detail</a>
            <a href="https://github.com/moon23k/Transformer" class="btn-type-1">Code</a>
          </div>

        </li>
      </ul>
    </div>

    <div class='parallel-item'>
      <h3>Transformer Balance Research</h3>
      <ul>
        <li>
          <p> &nbsp;
            Transformer의 Encoder와 Decoder를 Depth, Width, Diversity라는 세 가지 측면에서 불균형하게 디자인하고, 각 디자인 별 성능 검증
          </p>
          <div class="btn-list">
            <a href="{{ '/transformer_balance' | relative_url }}" class="btn-type-1">Detail</a>
            <a href="https://github.com/moon23k/Balance" class="btn-type-1">Code</a>
          </div>

        </li>
      </ul>
    </div>

  </div>

  <div class="parallel">
    <div class='parallel-item'>
      <h3>Transformer Variants</h3>
      <ul>
        <li>
          <p> &nbsp;
            레이어간 재귀적 연결 방식을 사용한 <b>Recurrent Transformer</b>와 NAS 방식으로 고도화 된 <b>Evolved Transformer</b>를 구현하고, <b>Standard Transformer</b>와 자연어 생성 능력 비교 검증
          </p>
          <div class="btn-list">
            <a href="{{ '/transformer_variants' | relative_url }}" class="btn-type-1">Detail</a>
            <a href="https://github.com/moon23k/Transformer_Variants" class="btn-type-1">Code</a>
          </div>

        </li>
      </ul>
    </div>

    <div class='parallel-item'>
      <h3>S.A.M.P.L.E.</h3>
      <ul>
        <li>
          <p> &nbsp;
            <b>S</b>eq2Seq <b>A</b>pplication <b>M</b>ethods of <b>P</b>retrained <b>L</b>anguage <b>E</b>ncoder model의 약어로, 사전 학습 인코더를 자연어 생성 과제에서 활용하기 위한 방법론을 탐구
          </p>
          <div class="btn-list">
            <a href="{{ '/sample' | relative_url }}" class="btn-type-1">Detail</a>
            <a href="https://github.com/moon23k/SAMPLE" class="btn-type-1">Code</a>
          </div>

        </li>
      </ul>
    </div>

  </div>

</div>



<!-- Second Research -->
<div class="container">
  
  <div class="full-width">
    <h3>Training Strategy</h3>
    <p>
      &nbsp; 앞선 프로젝트들은 모델 디자인을 집중적으로 탐구했습니다. 하지만 동일한 디자인의 딥러닝 모델을 사용할지라도, 학습 방식에 따라 모델의 성능을 제대로 이끌어 낼수도, 그렇지 못할 수도 있습니다. 이번 장에서는 다양한 딥러닝 모델의 학습 방법론을 직접 구현해보고, 각 방법론 마다의 실효성을 비교 검증합니다.
    </p>
  </div>

  <div class="parallel">
    <div class='parallel-item'>
      <h3>Auxiliary Training</h3>
      <ul>
        <li>
          <p> &nbsp; 
            자연어 생성 모델의 고질적인 문제인 Exposure Bias를 해결하기 위해 보조 학습 목표를 활용한 학습 전략을 적용하며, 그 실효성을 검증
          </p>
          <div class="btn-list">
            <a href="{{ '/aux_train' | relative_url }}" class="btn-type-1">Detail</a>
            <a href="https://github.com/moon23k/Aux_Train" class="btn-type-1">Code</a>
          </div>

        </li>
      </ul>
    </div>

    <div class='parallel-item'>
      <h3>Scheduled Sampling</h3>
      <ul>
        <li>
          <p> &nbsp; 
            RNN Seq2Seq 모델에서 주로 활용되는 Scheduled Sampling을 Transformer에 적용하며 자연어 생성 과제에서 실효성을 검증
          </p>
          <div class="btn-list">
            <a href="{{ '/scheduled_sampling' | relative_url }}" class="btn-type-1">Detail</a>
            <a href="https://github.com/moon23k/Scheduled_Sampling" class="btn-type-1">Code</a>
          </div>

        </li>
      </ul>
    </div>

  </div>

  <div class="parallel">
    <div class='parallel-item'>
      <h3>Generative Training</h3>
      <ul>
        <li>
          <p> &nbsp; 
            가용한 데이터가 매우 한정적인 경우, 생성적 학습이 효과적일 수 있다는 가설을 검증하고, 데이터 증강을 통한 일반적 학습법과 비교
          </p>
          <div class="btn-list">
            <a href="{{ '/gen_train' | relative_url }}" class="btn-type-1">Detail</a>
            <a href="https://github.com/moon23k/Gen_Train" class="btn-type-1">Code</a>
          </div>

        </li>
      </ul>
    </div>

    <div class='parallel-item'>
      <h3>SeqGAN Training</h3>
      <ul>
        <li>
          <p> &nbsp; 
            CV 분야에서 활발하게 사용되는 적대적 생성 학습을 자연어 생성 과제에서도 사용하기 위한 SeqGAN기법을 구현하고, 그 효용성을 검증
          </p>
          <div class="btn-list">
            <a href="{{ '/gan_train' | relative_url }}" class="btn-type-1">Detail</a>
            <a href="https://github.com/moon23k/SeqGAN" class="btn-type-1">Code</a>
          </div>

        </li>
      </ul>
    </div>
  </div>


  <div class="parallel">
    <div class='parallel-item' style="width: 47%">
      <h3>Customized Pretraining</h3>
      <ul>
        <li>
          <p> &nbsp; 목표로하는 Down Stream Task에만 초점을 맞춘 커스텀 사전학습을 통해 모델 성능의 향상성 검증
          </p>
          <div class="btn-list">
            <a href="{{ '/custom_pretrain' | relative_url }}" class="btn-type-1">Detail</a>
            <a href="https://github.com/moon23k/Custom_Pretrain" class="btn-type-1">Code</a>
          </div>

        </li>
      </ul>
    </div>



  </div>
</div>



<!-- Third Research -->
<div class="container">
  
  <div class="full-width">
    <h3>Efficiency</h3>
    <p>
      &nbsp; 다량의 파라미터로 이루어진 대규모 딥러닝 모델일수록 좋은 성능을 보여주는 경향성이 있습니다. 하지만 대규모 모델을 다루기 위해서는 많은 컴퓨팅 자원이 필요하며, 일반적으로 이러한 조건을 충족시키기 매우 어렵다는 한계점이 존재합니다. 한정적인 컴퓨팅 자원에서도 대규모 모델을 활용하기 위해 효율성에 초점을 맞춘 프로젝트를 진행하고, 인사이트를 공유합니다.
    </p>
  </div>

  <div class="parallel">
    <div class='parallel-item'>
      <h3>Efficient Training</h3>
      <ul>
        <li>
          <p> &nbsp; 대규모 사전학습 모델은 그대로 유지한채, 학습과정에서의 효율성을 증대시키기 위한 다양한 방법론의 성능 및 효율성 검증
          </p>
          <div class="btn-list">
            <a href="{{ '/eff_train' | relative_url }}" class="btn-type-1">Detail</a>
            <a href="https://github.com/moon23k/Eff_Train" class="btn-type-1">Code</a>
          </div>

        </li>
      </ul>
    </div>

    <div class='parallel-item'>
      <h3>Efficient Pretrained Models</h3>
      <ul>
        <li>
          <p> &nbsp; 모델 경량화와 어텐션 메커니즘의 고도화로 효율성을 증진시킨 사전학습 모델들의 실제 효용성 비교 검증
          </p>
          <div class="btn-list">
            <a href="{{ '/eff_model' | relative_url }}" class="btn-type-1">Detail</a>
            <a href="https://github.com/moon23k/Eff_Model" class="btn-type-1">Code</a>
          </div>

        </li>
      </ul>
    </div>

  </div>

  <div class="parallel">
    <div class='parallel-item' style="width: 47%">
      <h3>Parameter Efficient Fine-Tuning</h3>
      <ul>
        <li>
          <p> &nbsp; 한정된 자원에서 대규모 사전학습 모델을 효과적으로 미세조정하기 위한 PEFT 방식 마다의 실효성 비교 검증
          </p>
          <div class="btn-list">
            <a href="{{ '/peft' | relative_url }}" class="btn-type-1">Detail</a>
            <a href="https://github.com/moon23k/PEFT" class="btn-type-1">Code</a>
          </div>

        </li>
      </ul>
    </div>

  </div>
</div>


<!-- Fourth Research -->
<div class="container">
  
  <div class="full-width">
    <h3>Neural Machine Translation</h3>
    <p>
      &nbsp; 기계 번역은 딥러닝 모델을 활용해 Source Language 시퀀스를 동일한 의미의 Target Language 시퀀스로 변환하는 대표적인 자연어 생성 과제입니다. 최근 다양한 모델들이 좋은 기계번역 성과를 보여주고는 있지만, 더욱 향상된 기계번역 모델을 만들어 내기 위한 다양한 주제별 실험을 진행하고, 인사이트를 공유합니다.
    </p>
  </div>

  <div class="parallel">
    <div class='parallel-item'>
      <h3>Back Translation</h3>
      <ul>
        <li>
          <p> &nbsp; 
            충분한 학습이 이뤄지고, 그 이상의 성능 개선을 위한 데이터 증강 요법인 Back Translation이, 학습 데이터가 부족한 상황에서 어떤 실효성을 지니는지 확인 
          </p>
          <div class="btn-list">
            <a href="{{ '/back_translation' | relative_url }}" class="btn-type-1">Detail</a>
            <a href="https://github.com/moon23k/BackTranslation" class="btn-type-1">Code</a>
          </div>

        </li>
      </ul>
    </div>

    <div class='parallel-item'>
      <h3>Multi-Lingual Translation</h3>
      <ul>
        <li>
          <p> &nbsp; 
            다언어-단일언어, 단일언어-다언어, 다언어-다언어 번역 모델을 학습 및 평가하며, 세 가지 단언어 번역에 대한 학습 결과를 직접 확인 및 개선 여부 파악
          </p>
          <div class="btn-list">
            <a href="{{ '/multilingual' | relative_url }}" class="btn-type-1">Detail</a>
            <a href="https://github.com/moon23k/Multi_Lingual" class="btn-type-1">Code</a>
          </div>

        </li>
      </ul>
    </div>

  </div>



  <div class="parallel">
    <div class='parallel-item'>
      <h3>Code Translation</h3>
      <ul>
        <li>
          <p> &nbsp; 
            사람의 자연어와 컴퓨터 프로그래밍 언어 간의 번역을 위한 일련의 실험 및 결과를 통한 인사이트를 공유
          </p>
          <div class="btn-list">
            <a href="{{ '/code_translation' | relative_url }}" class="btn-type-1">Detail</a>
            <a href="https://github.com/moon23k/code_translation" class="btn-type-1">Code</a>
          </div>

        </li>
      </ul>
    </div>

    <div class='parallel-item'>
      <h3>NMT Blend</h3>
      <ul>
        <li>
          <p> &nbsp; 
            앞선 연구들에서의 긍정적인 방법론만을 취합해, 가장 좋은 기계번역 모델 엔지니어링 방식을 조합하고, 그 실효성 검증 
          </p>
          <div class="btn-list">
            <a href="{{ '/nmt_blend' | relative_url }}" class="btn-type-1">Detail</a>
            <a href="https://github.com/moon23k/NMT_Blend" class="btn-type-1">Code</a>
          </div>

        </li>
      </ul>
    </div>

  </div>
</div>


<!-- Fifth Research -->
<div class="container">
  
  <div class="full-width">
    <h3>Dialogue Generation</h3>
    <p>
      &nbsp; 대화 생성은 사람과 컴퓨터 간 자연스러운 상호작용을 위한 대표적 자연어 생성 과제 입니다. 다양한 주제 별로, 대화 생성 모델을 향상시키위한 일련의 실험을 진행하고, 그 결과를 공유합니다.
    </p>
  </div>

  <div class="parallel">
    <div class='parallel-item'>
      <h3>Characteristic Generation</h3>
      <ul>
        <li>
          <p> &nbsp; 
            SeqGAN의 기법을 활용해, Pretrained GPT 모델에게 특정 개성을 부여하고, 개성의 학습을 검증
          </p>
          <div class="btn-list">
            <a href="{{ '/character' | relative_url }}" class="btn-type-1">Detail</a>
            <a href="https://github.com/moon23k/Character" class="btn-type-1">Code</a>
          </div>

        </li>
      </ul>
    </div>

    <div class='parallel-item'>
      <h3>SimEnt Utilization</h3>
      <ul>
        <li>
          <p> &nbsp; 
            대화 다양성을 증진시키기 위한 SimEnt 기법을 직접 구현하고, 실제 대화의 다양성 개선 여부를 검증
          </p>
          <div class="btn-list">
            <a href="{{ '/siment' | relative_url }}" class="btn-type-1">Detail</a>
            <a href="https://github.com/moon23k/SimEnt" class="btn-type-1">Code</a>
          </div>

        </li>
      </ul>
    </div>

  </div>

  <div class="parallel">
    <div class='parallel-item'>
      <h3>Multi-Turn Dialogue Generation</h3>
      <ul>
        <li>
          <p> &nbsp; 
            인코더 구조를 개선시켜 Multi-Turn 대화 생성 모델을 구현하고, 실제 Multi-Turn 대화 생성 능력을 검증 
          </p>
          <div class="btn-list">
            <a href="{{ '/multiturn' | relative_url }}" class="btn-type-1">Detail</a>
            <a href="https://github.com/moon23k/MultiTurn" class="btn-type-1">Code</a>
          </div>

        </li>
      </ul>
    </div>

    <div class='parallel-item'>
      <h3>Dialogue Generation Blend</h3>
      <ul>
        <li>
          <p> &nbsp; 
            앞선 연구들에서의 긍정적인 방법론만을 취합해, 가장 좋은 대화 생성 모델 엔지니어링 방식을 조합하고, 그 실효성 검증
          </p>
          <div class="btn-list">
            <a href="{{ '/dialog_blend' | relative_url }}" class="btn-type-1">Detail</a>
            <a href="https://github.com/moon23k/Dialogue_Blend" class="btn-type-1">Code</a>
          </div>

        </li>
      </ul>
    </div>

  </div>

</div>


<!-- Sixth Research -->
<div class="container">
  
  <div class="full-width">
    <h3>Abstractive Text Summarization</h3>
    <p>
      &nbsp; 추상적 문서 요약은 길고 복잡한 입력 시퀀스를 바탕으로 짧게 요약된 시퀀스를 생성하는 대표적인 자연어 생성 과제입니다. 다양한 주제별로 추상적 문서 요약 모델의 성능을 향상시키기 위한 실험을 진행하고, 그 결과를 공유합니다. 
    </p>
  </div>

  <div class="parallel">
    <div class='parallel-item'>
      <h3>Hierarchical Encoder</h3>
      <ul>
        <li>
          <p> &nbsp; 
            긴 입력시퀀스를 잘 이해할 수 있도록 계층적 인코더 모델을 구현하고, 기존 방식의 모델과 요약 성능을 비교
          </p>
          <div class="btn-list">
            <a href="{{ '/hierarchical' | relative_url }}" class="btn-type-1">Detail</a>
            <a href="https://github.com/moon23k/Sum_HierEncoder" class="btn-type-1">Code</a>
          </div>

        </li>
      </ul>
    </div>

    <div class='parallel-item'>
      <h3>Sparse Attention</h3>
      <ul>
        <li>
          <p> &nbsp; 
            Full Attention 연산의 복잡도를 개선시킨 Sparse Attention 기반의 모델을 구현하고, 기존 방식의 모델과 요약 성능을 비교 
          </p>
          <div class="btn-list">
            <a href="{{ '/sparse_attention' | relative_url }}" class="btn-type-1">Detail</a>
            <a href="https://github.com/moon23k/Sum_Sparse" class="btn-type-1">Code</a>
          </div>

        </li>
      </ul>
    </div>

  </div>

  <div class="parallel">
    <div class='parallel-item' style="width: 47%">
      <h3>Summarization Blend</h3>
      <ul>
        <li>
          <p>
            앞선 연구들에서의 긍정적인 방법론만을 취합해, 가장 좋은 문서 요약 모델 엔지니어링 방식을 조합하고, 그 실효성 검증
          </p>
          <div class="btn-list">
            <a href="{{ '/sum_blend' | relative_url }}" class="btn-type-1">Detail</a>
            <a href="https://github.com/moon23k/Sum_Blend" class="btn-type-1">Code</a>
          </div>

        </li>
      </ul>
    </div>
  </div>

</div>


<script src="./assets/js/script.js"></script>