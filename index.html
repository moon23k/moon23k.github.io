---
layout: default
---


<div class="info">
  <h2 style="color: #303030;">Hi there!</h2>
  <p>
    &nbsp; 안녕하세요 :) &nbsp;   
    인공지능에 매료되어, 인공지능 기술을 통한 현실의 문제 해결에 집중하고 있는  
    Machine Learning Engineer, Moon입니다.
    다양한 인공지능 활용 분야들 중에서도 특히, 자연어 처리에 초점을 맞추고 있습니다. 
    사람과 자연스럽게 의사소통 가능한 인공지능 모델 개발을 목표로 삼고 있습니다. 
    목표 달성을 위해 다양한 연구 주제를 직접 선정하고 구현하며, 
    끊임없이 한 발짝씩 목표에 다가가는 중입니다.  
  </p>
  <div class="btn-block">
    <a href="{{ '/resume' | relative_url }}" class="btn-type-1">Resume</a>
    <a href="https://github.com/moon23k" class="btn-type-1">Github</a>
    <a href="https://shy-vole-f74.notion.site/Hello-I-m-Moon-e1ecc2e40b32405e997713cfb44e4f3c?pvs=4" class="btn-type-1">Notion</a>
  </div>
</div>



<div class="project-list">
  <dl  class="type-right">
    <dt><h3>자연어 생성을 위한 세 가지 트랜스포머 모델 비교 분석</h3></dt>
    <dd>
      <ul>
        <li>
          <div class="inner">
            <div class="tags">
              <span>Transformer</span>
              <span>Machine Translation</span>
              <span>Dialogue Generation</span>
              <span>Text Summarization</span>
            </div>
            <div class="btn-list">
              <a href="{{ '/project_01' | relative_url }}" class="btn-type-1">Detail</a>
              <a href="https://github.com/moon23k" class="btn-type-1">Code</a>
            </div>
          </div>
        </li>
        <li>
          &nbsp; Transformer 모델 구조에 기반한 다양한 연구들이 존재하지만, Transformer 구조 자체에 집중한 자연어 생성 과제에서의 성능 비교 실험은 부재합니다. 이를 해결하기 위해 세 가지 Transformer 모델을 직접 구현하고, 세 가지 자연어 생성 과제에서 성능을 비교해봅니다. 이 실험을 통해 레이어의 재귀적 연결로 Inductive BIas를 향상시킬 수 있다는 것과, 섬세한 레이어 연결의 변화를 통해 더 높은 성장가능성을 직접 확인할 수 있습니다. 
        </li>
      </ul>
    </dd>
  </dl>


  <dl class="type-right">
    <dt><h3>문서요약을 위한 BERT 활용 방법론 별 성능 결과 비교</h3></dt>
    <dd>
      <ul>
        <li>
          <div class="inner">
            <div class="tags">
              <span>BERT</span>
              <span>BERT SUM</span>
              <span>Text Summarization</span>
              <span>Encoder-Decoder Connection</span>
            </div>
            <div class="btn-list">
              <a href="{{ '/project_02' | relative_url }}" class="btn-type-1">Detail</a>
              <a href="#" class="btn-type-1">Code</a>
            </div>
          </div>
        </li>
        <li>
          &nbsp; BERT는 다양한 자연어 이해 과제에서 우수한 성능을 보이는 대표적 사전학습 모델입니다. 하지만 BERT를 자연어 생성 과제에 효과적으로 적용하는 방법에 대한 탐구는 상대적으로 부족합니다. 특히나 입력 시퀀스의 길이가 BERT 사전학습 시 마주했던 시퀀스 길이보다 긴 문서 요약 과제에 있어서는 더욱 그러합니다.
          물론 이런 한계점을 극복하기 위한 각각의 방법론 연구들이 있어왔지만, 다양한 방법론들을 통합적으로 사용하는 연구는 부족합니다. 때문에 본 프로젝트에서는 BERT를 문서요약 과제에서 효과적으로 사용하기 위한 네 가지 방법론을 조합해, 총 세가지 모델을 직접 구현하고 각 모델의 성능을 비교 분석합니다.
        </li>
      </ul>
    </dd>
  </dl>


  <dl class="type-right">
    <dt><h3>대화 생성 모델의 개성 부여를 위한 SeqGAN의 활용</h3></dt>
    <dd>
      <ul>
        <li>
          <div class="inner">
            <div class="tags">
              <span>SeqGAN</span>
              <span>Dialogue Generation</span>
              <span>Characteristic Dialogue</span>
              <span>Data Web Crawling</span>
            </div>
            <div class="btn-list">
              <a href="{{ '/project_03' | relative_url }}" class="btn-type-1">Detail</a>
              <a href="https://github.com/moon23k" class="btn-type-1">Code</a>
            </div>
          </div>
        </li>
        <li>
          &nbsp; 대화 생성 모델의 학습 과정에서, 모델은 학습 데이터의 확률분포 잘 모방할 수 있는 방향으로 파라미터를 갱신합니다. 이는 자연어 생성 과정을 위한 가장 일반적인 학습 방법이지만, 대화 생성 과제에서는 지나치게 일반적이고 재미없는 답변만을 반복하기 쉽도록 모델을 편향시킨다는 단점이 있습니다. 물론 대량의 다양한 데이터로 학습하며 이런 문제를 해결할 수도 있지만, 현실적으로 그런 양질의 대량 데이터를 확보하는 것은 어렵습니다. 때문에 이런 문제점을 학습 방식의 고도화를 통해 해결하고, 개성있는 대화를 생성하기 위해 SeqGAN을 활용한 방법론을 직접 구현하고 실험 결과를 공유합니다.
        </li>
      </ul>
    </dd>
  </dl>


  <dl class="type-right">
    <dt><h3>의미적으로 다양한 대화 생성을 위한 학습 방법론 구현</h3></dt>
    <dd>
      <ul>
        <li>
          <div class="inner">
            <div class="tags">
              <span>Semantic Clustering</span>
              <span>Dialogue Generation</span>
              <span>Dialogue Diversity</span>
            </div>
            <div class="btn-list">
              <a href="{{ '/project_04' | relative_url }}" class="btn-type-1">Detail</a>
              <a href="https://github.com/moon23k" class="btn-type-1">Code</a>
            </div>
          </div>
        </li>
        <li>
          &nbsp; 대화 생성 모델의 학습 과정에서, 모델은 학습 데이터의 확률분포 잘 모방할 수 있는 방향으로 파라미터를 갱신합니다. 이는 자연어 생성 과정을 위한 가장 일반적인 학습 방식이지만, 대화 생성 과제에서는 지나치게 일반적이고 재미없는 답변만을 반복하기 쉽도록 모델을 편향시키기 쉽다는 단점이 있습니다. 물론 대량의 다양한 데이터로 학습하며 이런 문제를 해결할 수도 있지만, 현실적으로 그런 양질의 대량 데이터를 확보하는 것은 쉽지 않습니다. 때문에 이런 문제점을 학습방식의 고도화를 통해 해결하고 개성있는 대화를 생성하기 위해 SemEnt에서 제시한 Semantic Clustering을 활용한 방법론을 직접 구현하고 실험 결과를 공유합니다.
        </li>
      </ul>
    </dd>
  </dl>


  <dl class="type-right">
    <dt><h3>컴퓨팅 자원의 효율적 사용을 위한 딥러닝 모델 학습 방법론 별 성능 비교</h3></dt>
    <dd>
      <ul>
        <li>
          <div class="inner">
            <div class="tags">
              <span>Efficient Training</span>
              <span>Gradient Accumulating</span>
              <span>Gradient Checkpointing</span>
              <span>Mixed Precision Training</span>
              <span>Adafactor Optimizer</span>
            </div>
            <div class="btn-list">
              <a href="{{ '/project_05' | relative_url }}" class="btn-type-1">Detail</a>
              <a href="https://github.com/moon23k" class="btn-type-1">Code</a>
            </div>
          </div>
        </li>
        <li>
          &nbsp; 좋은 딥러닝 모델의 학습을 위해서는, 많은 데이터 가능한 큰 모델 규모, 그리고 다량의 반복학습 등 컴퓨팅 자원을 많이 필요로 합니다. 하지만 일반적인 사용자의 입장에서 이를 만족시킬만한 환경을 구축하는 것은 매우 어렵습니다. 때문에 한정된 자원에서 좋은 모델을 만들기 위해 학습 과정에서의 효율성을 제고시키는 것이 매우 중요합니다. 이를 위한 네 가지 방안을 직접 구현 및 적용한 실험 결과를 공유합니다. 
        </li>
      </ul>
    </dd>
  </dl>


  <dl class="type-right" style="margin-bottom: 8em;">
    <dt><h3>데이터 부족 해결을 위한 Back Translation 조건 별 성능 비교</h3></dt>
    <dd>
      <ul>
        <li>
          <div class="inner">
            <div class="tags">
              <span>Data Augmentation</span>
              <span>Back Translation</span>
              <span>Machine Translation</span>
            </div>
            <div class="btn-list">
              <a href="{{ '/project_06' | relative_url }}" class="btn-type-1">Detail</a>
              <a href="https://github.com/moon23k" class="btn-type-1">Code</a>
            </div>
          </div>
        </li>
        <li>
          &nbsp; 딥러닝 모델을 성공적으로 학습시키기 위해서는 양질의 대량 데이터가 필수적입니다. 하지만 대량 데이터 확보를 위해서는 큰 비용과 많은 시간이 소요됩니다. 이런 데이터 부족의 문제점을 딥러닝 모델 활용을 통해 해결할수 있는 대표적인 방법으로 Back Translation이 있습니다. Back Translation에 관한 포괄적인 연구논문이 있긴하지만, 영어라는 언어에 국한되어있습니다. 때문에 이 프로젝트에서는 영어 데이터보다 수적으로 부족한 한국어 데이터의 증강을 위한 Back Translation의 활용을 위한 일련의 실험을 직접 수행하고 결과를 공유합니다.   
        </li>
      </ul>
    </dd>
  </dl>


</div>
<script src="./assets/js/script.js"></script>