---
layout: default
---


<!-- Information -->
<div class="info">
  <h2 style="color: #303030; letter-spacing: 2px;">Hi, there!</h2>
  <p>
    &ensp; 인공지능 모델을 통한 창의적 가치 실현을 목표로 하는 머신러닝 엔지니어 송문기입니다. 목표 달성을 위해 다양한 연구 주제를 직접 선정&sdot; 구현&sdot; 검증하는 과정을 즐깁니다. 총 12개의 주요 프로젝트를 주제 별로 정리해 그 과정 및 결과를 본문에서 공유합니다. 저에 대한 정보가 궁금하시다면 아래의 링크를 참조해주세요 :) 

  </p>
  <div class="btn-block">
    <a href="{{ '/resume' | relative_url }}" class="btn-type-1">Resume</a>
    <a href="https://github.com/moon23k" class="btn-type-1">Github</a>
    <a href="https://www.notion.so/Hello-I-m-Moon-e1ecc2e40b32405e997713cfb44e4f3c?pvs=4" class="btn-type-1">Study Log</a>
  </div>
</div>


<!-- Model Architecture -->
<div class="container">
  
  <div class="full-width">
    <h3>&hairsp; Model&hairsp; Architecture</h3>
    <p>&nbsp; 머신러닝 엔지니어링에서 모델의 구조적 디자인은 성능에 결정적인 영향을 미치는 주요 요소입니다. 특히 인코더와 디코더를 모두 필요로 하는 Sequence to Sequence 구조에서, 모델 디자인은 더더욱 중요합니다. 이 장에서는 데이터 처리 및 학습 방식은 고정하고, 자연어 생성을 위한 모델의 디자인 변화에 따른 성능 변화를 파악하는데 집중합니다.
    </p>
  </div>

  <div class="parallel">

    <div class='parallel-item'>
      <h3>Transformer&hairsp; Balance</h3>
      <ul>
        <li>
          <p>표준 트랜스포머의 균형잡힌 모델 디자인을 의도적으로 탈피해, <b>깊이 &middot; 너비 &middot; 다양성</b> 측면의 디자인 불균형에 따른 성능 변화를 탐색하고, 과제 특성에 따른 균형 조정 기준 설립 
          </p>
          <div class="btn-list">
            <a href="{{ '/transformer_balance' | relative_url }}" class="btn-type-1">Detail</a>
            <a href="https://github.com/moon23k/Transformer_Balance" class="btn-type-1">Code</a>
          </div>

        </li>
      </ul>
    </div>

    <div class='parallel-item'>
      <h3>Transformer&hairsp; Variants</h3>
      <ul>
        <li>
          <p><b>Recurrent Transformer, Evolved Transformer</b>라는 두 가지 트랜스포머 변형 모델을 직접 구현하고, 표준 트랜스포머와 자연어 생성 능력 비교 검증
          </p>
          <div class="btn-list">
            <a href="{{ '/transformer_variants' | relative_url }}" class="btn-type-1">Detail</a>
            <a href="https://github.com/moon23k/Transformer_Variants" class="btn-type-1">Code</a>
          </div>

        </li>
      </ul>
    </div>

  </div>


  <div class="parallel">
    <div class='parallel-item'>
      <h3>Transformer&hairsp; Fusion</h3>
      <ul>
        <li>
          <p>사전 학습 인코더 모델을 트랜스포머 Seq2Seq 모델 구조에서 활용하기 위한 두 가지 모델 설계 방법론을 직접 구현하고, 각 방법론에 따른 자연어 생성 능력 비교 검증
          </p>
          <div class="btn-list">
            <a href="{{ '/transformer_fusion' | relative_url }}" class="btn-type-1">Detail</a>
            <a href="https://github.com/moon23k/Transformer_Fusion" class="btn-type-1">Code</a>
          </div>

        </li>
      </ul>
    </div>

  </div>
</div>



<!-- Training Strategy -->
<div class="container">
  
  <div class="full-width">
    <h3>&hairsp; Training&hairsp; Strategy</h3>
    <p>
      &nbsp; 모델 학습은 모델 디자인과 더불어, 머신러닝 엔지니어링의 주요 요소입니다. 동일한 모델이라도, 학습 방식에 따라 성능은 크게 달라질 수 있습니다. 특히나 자연어 생성과 같이 고난이도 과제의 경우, 일반적인 학습 방식만으로 원하는 결과를 얻기 어렵습니다. 아래 프로젝트에서는 자연어 생성 능력 향상을 위한 고도화 된 학습 방법론을 제시하고, 그 효용성을 검증합니다.
    </p>
  </div>

  <div class="parallel">
    <div class='parallel-item'>
      <h3>Customized&hairsp; Pre-Training</h3>
      <ul>
        <li>
          <p>지나치게 크고 무거운 대규모 사전학습 모델을 사용하는대신, 목표로하는 Down Stream Task에 집중한 맞춤형 사전 학습 방법론의 효용성 검증
          </p>
          <div class="btn-list">
            <a href="{{ '/cpt_train' | relative_url }}" class="btn-type-1">Detail</a>
            <a href="https://github.com/moon23k/CPT" class="btn-type-1">Code</a>
          </div>

        </li>
      </ul>
    </div>


    <div class='parallel-item'>
      <h3>G&hairsp; I&hairsp; F&hairsp; T</h3>
      <ul>
        <li>
          <p><b>Generation Improving Fine-Tuning, GIFT</b> 프로젝트에서는, 자연어 생성 능력 향상을 위한 네 가지 미세조정 방법론을 제시하고, 각 방법론의 효용성을 검증
          </p>
          <div class="btn-list">
            <a href="{{ '/gift' | relative_url }}" class="btn-type-1">Detail</a>
            <a href="https://github.com/moon23k/GIFT" class="btn-type-1">Code</a>
          </div>

        </li>
      </ul>
    </div>

  </div>


  <div class="parallel">
    <div class='parallel-item' style="width: 47%">
      <h3>IntelliGEN</h3>
      <ul>
        <li>
          <p>자연어 생성 능력 향상을 위해 Standard Training, <b>Generative &sdot; SeqGAN Fine tuning</b>, 총 세 단계에 걸친 똑똑한 생성적 학습 방법론을 제시하고, 그 효용성 검증
          </p>
          <div class="btn-list">
            <a href="{{ '/slow_gan' | relative_url }}" class="btn-type-1">Detail</a>
            <a href="https://github.com/moon23k/intelligen" class="btn-type-1">Code</a>
          </div>

        </li>
      </ul>
    </div>

  </div>
</div>




<!-- Task Specific Experiment -->
<div class="container">
  
  <div class="full-width">
    <h3>&hairsp; Task&hairsp; Specific</h3>
    <p>
      &nbsp; 위의 <b>Model Architecture</b> 및 <b>Training Strategy</b>에서는 자연어 생성이라는 전역적인 과제 해결을 위한 모델링 및 학습 방법론에 대해 살펴봤습니다. 이번 장에서는 <b>기계 번역, 대화 생성, 문서 요약</b>이라는 자연어 생성의 세부 과제를 효과적으로 해결하기 위한 과제 집중적 연구 프로젝트를 제시합니다. 
    </p>
  </div>

  <div class="parallel">
    <div class='parallel-item'>
      <h3>&hairsp;Multi-Lingual Translation</h3>
      <ul>
        <li>
          <p>&nbsp; 단일 언어 기계 번역에서 한 걸음 나아가, 다언어 기계 번역 성능 향상을 위한 트랜스포머 기반 Foundation Modeling 연구를 진행하고, 각 방법론 별 성능 변화를 검증
          </p>
          <div class="btn-list">
            <a href="{{ '/multomodal_translation' | relative_url }}" class="btn-type-1">Detail</a>
            <a href="https://github.com/moon23k/MultiLingual_Translation" class="btn-type-1">Code</a>
          </div>

        </li>
      </ul>
    </div>

    <div class='parallel-item'>
      <h3>Multi-Turn Dialogue</h3>
      <ul>
        <li>
          <p>&nbsp; 대화의 흐름을 이해하며 자연스럽고, 일관된 대화가 가능한 멀티턴 대화 모델 구축을 위한 대화 기록 활용 방법론을 제시하고, 그 효용성을 검증
          </p>
          <div class="btn-list">
            <a href="{{ '/multiturn_dialogue' | relative_url }}" class="btn-type-1">Detail</a>
            <a href="https://github.com/moon23k/intelligen" class="btn-type-1">Code</a>
          </div>

        </li>
      </ul>
    </div>
  </div>



  <div class="parallel">
    <div class='parallel-item' style="width: 47%">
      <h3>Efficient&hairsp; Summarization</h3>
      <ul>
        <li>
          <p>&nbsp; Efficient Attention, Block Sparse Attention을 활용함으로써, 긴 시퀀스를 다루는 문서 요약 과제에서의 효율성을 제고시키고, 성능과의 Cross Over 검토
          </p>
          <div class="btn-list">
            <a href="{{ '/efficient_summarization' | relative_url }}" class="btn-type-1">Detail</a>
            <a href="https://github.com/moon23k/CPT" class="btn-type-1">Code</a>
          </div>

        </li>
      </ul>
    </div>

  </div>
</div>





<!-- LLM Framework -->
<div class="container">
  
  <div class="full-width">
    <h3>&hairsp; LLM&hairsp; Framework</h3>
    <p>&nbsp; <b>Large Language Model, LLM</b>은 다양한 분야에서 뛰어난 효용성을 보여주고 있습니다. LLM을 있는 그대로 사용하는 것도 충분히 좋은 성능을 보이지만, 이를 더욱 효과적으로 사용할 수 있는 방법론이 존재합니다. 아래의 프로젝트에서는 특정 목적에 맞게 LLM을 효과적으로 사용하기 위한 프레임워크를 제시합니다. 
    </p>
  </div>

  <div class="parallel">
    <div class='parallel-item'>
      <h3>Context&hairsp; Framework</h3>
      <ul>
        <li>
          <p>&nbsp; 번역 과제에서 원문의 문맥을 파악하고, 문맥의 정보를 LLM에 전달함으로써 고품질의 번역을 가능토록 하는 프레임워크를 제시하고, 효용성 검증 
          </p>
          <div class="btn-list">
            <a href="{{ '/context_framework' | relative_url }}" class="btn-type-1">Detail</a>
            <a href="https://github.com/moon23k/context_framework" class="btn-type-1">Code</a>
          </div>

        </li>
      </ul>
    </div>

    <div class='parallel-item'>
      <h3>Character&hairsp; Framework</h3>
      <ul>
        <li>
          <p>&nbsp; LLM으로 하여금 사람처럼 개성있는 대화 스타일 학습시킴으로써, 흥미로운 대화 서비스를 제공하기 위한 프레임워크를 제시하고, 그 효용성을 검증
          </p>
          <div class="btn-list">
            <a href="{{ '/character_framework' | relative_url }}" class="btn-type-1">Detail</a>
            <a href="https://github.com/moon23k/char_framework" class="btn-type-1">Code</a>
          </div>
        </li>
      </ul>
    </div>
  </div>


  <div class="parallel">
    <div class='parallel-item' style="width: 47%">
      <h3>Trustworthy&hairsp; Framework</h3>
      <ul>
        <li>
          <p>혐오 발언, 정치 사회적 민감 발언 등 서비스 사용자로 하여금 불편함을 자아낼수 있는 시퀀스의 반환으로부터 소비자를 보호할수 있는 프레임워크를 제시하고, 그 효용성을 검증 
          </p>
          <div class="btn-list">
            <a href="{{ '/trust_framework' | relative_url }}" class="btn-type-1">Detail</a>
            <a href="https://github.com/moon23k/trust_framework" class="btn-type-1">Code</a>
          </div>

        </li>
      </ul>
    </div>
  </div>
</div>




<script src="./assets/js/script.js"></script>