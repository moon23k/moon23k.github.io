---
layout: default
---


<!-- Information -->
<div class="info">
  <h2 style="color: #303030; letter-spacing: 2px;">Hi, there!</h2>
  <p>
    &nbsp; 인공지능 기술을 활용한 현실 문제의 해결을 위해 항상 연구하는 머신러닝 엔지니어, 송문기입니다.
    다양한 인공지능 활용 분야들 중에서 자연어 처리에 초점을 맞추고 있으며, 
    사람과 자연스럽게 의사소통 가능한 인공지능 모델 개발을 목표로 삼고 있습니다. <br>
    목표 달성을 위해 다양한 연구 주제를 직접 선정하고 구현하며, 끊임없이 한 발짝씩 목표에 다가가는 중입니다. 이 페이지에서는 목표 달성을 위한 연구 프로젝트들을 주제별로 아래와 같이 정리해두었으며, 각 프로젝트 마다의 상세 및 코드 구현을 확인하실 수 있습니다.
  </p>
  <div class="btn-block">
    <a href="{{ '/resume' | relative_url }}" class="btn-type-1">Resume</a>
    <a href="https://github.com/moon23k" class="btn-type-1">Github</a>
    <a href="https://www.notion.so/Hello-I-m-Moon-e1ecc2e40b32405e997713cfb44e4f3c?pvs=4" class="btn-type-1">Study Log</a>
  </div>
</div>


<!-- Model Structure -->
<div class="container">
  
  <div class="full-width">
    <h3>&hairsp; Model&hairsp; Structure</h3>
    <p>&nbsp; 머신러닝 엔지니어링에서 모델의 구조적 디자인은 성능에 결정적인 영향을 미치는 주요 요소입니다. 특히 인코더와 디코더를 모두 필요로 하는 Sequence to Sequence 구조에서, 모델 디자인의 중요성은 더더욱 강조됩니다. 이 장에서는 가장 기초적인 모델 구조를 통한 자연어 생성 모델의 성능 기준 확립과, 성능 개선을 위한 다양한 모델 디자인에 집중한 프로젝트를 소개합니다. 모든 프로젝트에서는 데이터 및 학습 방식을 고정시킨채, 모델 디자인적인 변화에 따른 성능변화를 파악하는데 집중합니다.
    </p>
  </div>

  <div class="parallel">

    <div class='parallel-item'>
      <h3>The Baselines</h3>
      <ul>
        <li>
          <p>자연어 생성을 위한 가장 기본적인 모델 구조의 자연어 능력을 직접 확인함으로써, 추후 연구 성능 평가의 BaseLine으로 삼기 위한 기준 연구. 모델 구조로는 RNN Seq2Seq, RNN Seq2Seq with Attention, Transformer을 사용.
          </p>
          <div class="btn-list">
            <a href="{{ '/baselines' | relative_url }}" class="btn-type-1">Detail</a>
            <a href="https://github.com/moon23k/NLG_Baselines" class="btn-type-1">Code</a>
          </div>

        </li>
      </ul>
    </div>

    <div class='parallel-item'>
      <h3>Transformer&hairsp; Balance</h3>
      <ul>
        <li>
          <p>표준 트랜스포머의 균형잡힌 모델 디자인을 의도적으로 탈피해, <b>깊이 &middot; 너비 &middot; 다양성</b> 측면에서 다양한 불균형성이 야기하는 성능 변화를 탐색하고, 과제 별 특성에 따른 균형 조정 기준을 설립 
          </p>
          <div class="btn-list">
            <a href="{{ '/transformer_balance' | relative_url }}" class="btn-type-1">Detail</a>
            <a href="https://github.com/moon23k/Transformer_Balance" class="btn-type-1">Code</a>
          </div>

        </li>
      </ul>
    </div>

  </div>


  <div class="parallel">
    <div class='parallel-item'>
      <h3>Transformer&hairsp; Variants</h3>
      <ul>
        <li>
          <p><b>Recurrent Transformer, Evolved Transformer</b>라는 두 가지 트랜스포머 변형 모델을 직접 구현하고, 표준 트랜스포머와 자연어 생성 능력 비교 검증
          </p>
          <div class="btn-list">
            <a href="{{ '/transformer_variants' | relative_url }}" class="btn-type-1">Detail</a>
            <a href="https://github.com/moon23k/Transformer_Variants" class="btn-type-1">Code</a>
          </div>

        </li>
      </ul>
    </div>

    <div class='parallel-item'>
      <h3>Transformer&hairsp; Fusion</h3>
      <ul>
        <li>
          <p>사전 학습 인코더 모델을 트랜스포머 Seq2Seq 모델 구조에서 활용하기 위한 두 가지 모델 설계 방법론을 직접 구현하고, 각 방법론에 따른 자연어 생성 능력 비교 검증
          </p>
          <div class="btn-list">
            <a href="{{ '/transformer_fusion' | relative_url }}" class="btn-type-1">Detail</a>
            <a href="https://github.com/moon23k/Transformer_Fusion" class="btn-type-1">Code</a>
          </div>

        </li>
      </ul>
    </div>
  </div>

</div>



<!-- Training Strategy -->
<div class="container">
  
  <div class="full-width">
    <h3>&hairsp; Training&hairsp; Strategy</h3>
    <p>
      &nbsp; 앞선 프로젝트들은 모델 디자인을 집중적으로 탐구했습니다. 하지만 동일한 디자인의 딥러닝 모델을 사용할지라도, 학습 방식에 따라 모델의 성능을 제대로 이끌어 낼수도, 그렇지 못할 수도 있습니다. 이번 장에서는 다양한 딥러닝 모델의 학습 방법론을 직접 구현해보고, 각 방법론 마다의 실효성을 비교 검증합니다.
    </p>
  </div>

  <div class="parallel">
    <div class='parallel-item'>
      <h3>G&hairsp; I&hairsp; F&hairsp; T</h3>
      <ul>
        <li>
          <p><b>Generation Improving Fine-Tuning, GIFT</b> 프로젝트에서는 네 가지 미세조정 방법론 적용에 따른 자연어 생성 모델의 생성 능력 향상 여부 비교 분석
          </p>
          <div class="btn-list">
            <a href="{{ '/gift' | relative_url }}" class="btn-type-1">Detail</a>
            <a href="https://github.com/moon23k/GIFT" class="btn-type-1">Code</a>
          </div>

        </li>
      </ul>
    </div>

    <div class='parallel-item'>
      <h3>IntelliGEN</h3>
      <ul>
        <li>
          <p>자연어 생성 능력 향상을 위해 Standard Training, Generative Training, 그리고 Slow SeqGAN 세 단계에 걸친 똑똑한 생성적 학습 방법론을 제시하고, 그 효용성을 검증
          </p>
          <div class="btn-list">
            <a href="{{ '/slow_gan' | relative_url }}" class="btn-type-1">Detail</a>
            <a href="https://github.com/moon23k/intelligen" class="btn-type-1">Code</a>
          </div>

        </li>
      </ul>
    </div>

  </div>


  <div class="parallel">
    <div class='parallel-item'>
      <h3>Customized&hairsp; Pretraining</h3>
      <ul>
        <li>
          <p>자나치게 크고 무거운 대규모 사전학습 모델을 사용하는대신, 목표로하는 Down Stream Task에만 초점을 맞춘 맞춤형 사전학습의 효용성 검증
          </p>
          <div class="btn-list">
            <a href="{{ '/cpt_train' | relative_url }}" class="btn-type-1">Detail</a>
            <a href="https://github.com/moon23k/CPT" class="btn-type-1">Code</a>
          </div>

        </li>
      </ul>
    </div>

    <div class='parallel-item'>
      <h3>L&hairsp; E&hairsp; F&hairsp; T</h3>
      <ul>
        <li>
          <p><b>Llm Efficient Fine-Tuning, LEFT</b> 프로젝트에서는 대규모 사전학습 언어 모델의 효율적인 미세조정을 위한 다양한 방법론의 통합적 고찰
          </p>
          <div class="btn-list">
            <a href="{{ '/eff_train' | relative_url }}" class="btn-type-1">Detail</a>
            <a href="https://github.com/moon23k/left" class="btn-type-1">Code</a>
          </div>

        </li>
      </ul>
    </div>
  </div>


</div>




<!-- Task Specific Projects -->
<div class="container">
  
  <div class="full-width">
    <h3>&hairsp; Task&hairsp; Specific&hairsp; Projects</h3>
    <p>아래의 프로젝트들은 기계번역, 대화생성, 문서요약이라는 세 가지 대표적 자연어 생성 과제 자체에 초점을 맞추며, 각 과제를 더욱 잘 해결하기 위한 심층 연구를 다룹니다. 아래의 프로젝트들은 새롭게 중요성이 대두되는 과제에 대한 분석 실험과 기존 모델들의 아쉬운 점을 보완하기 위한 연구, 그리고 성능 뿐만 아니라 효율성을 제고 시키기 위한 연구 등 다양한 주제를 포함하고 있습니다.
    </p>
  </div>

  <div class="parallel">
    <div class='parallel-item'>
      <h3>Multi-Lingual&hairsp; Translation</h3>
      <ul>
        <li>
          <p> &nbsp; 
            다언어-단일언어, 단일언어-다언어, 다언어-다언어 번역 모델을 학습 및 평가하며, 세 가지 단언어 번역에 대한 학습 결과를 직접 확인 및 개선 여부 파악
          </p>
          <div class="btn-list">
            <a href="{{ '/translation_multilingual' | relative_url }}" class="btn-type-1">Detail</a>
            <a href="https://github.com/moon23k/NMT_MultiLingual" class="btn-type-1">Code</a>
          </div>

        </li>
      </ul>
    </div>

    <div class='parallel-item'>
      <h3>Code&hairsp; Translation</h3>
      <ul>
        <li>
          <p>사람의 자연어와 컴퓨터 프로그래밍 언어 간의 번역을 위한 일련의 실험 및 결과를 통한 인사이트를 공유
          </p>
          <div class="btn-list">
            <a href="{{ '/translation_code' | relative_url }}" class="btn-type-1">Detail</a>
            <a href="https://github.com/moon23k/NMT_Code" class="btn-type-1">Code</a>
          </div>
        </li>
      </ul>
    </div>
  </div>


  <div class="parallel">
    <div class='parallel-item'>
      <h3 style="letter-spacing: 1.2px;">Characteristic&hairsp; Dialogue</h3>
      <ul>
        <li>
          <p>SeqGAN의 기법을 활용해, Pretrained GPT 모델에게 특정 개성을 부여하고, 개성의 학습을 검증
          </p>
          <div class="btn-list">
            <a href="{{ '/dialogue_characteristic' | relative_url }}" class="btn-type-1">Detail</a>
            <a href="https://github.com/moon23k/Dialog_Char" class="btn-type-1">Code</a>
          </div>

        </li>
      </ul>
    </div>

    <div class='parallel-item'>
      <h3 style="letter-spacing: 1.2px;">Diverse&hairsp; Dialogue</h3>
      <ul>
        <li>
          <p>대화 다양성을 증진시키기 위한 SemEnt 기법을 직접 구현하고, 실제 대화의 다양성 개선 여부를 검증
          </p>
          <div class="btn-list">
            <a href="{{ '/dialogue_diverse' | relative_url }}" class="btn-type-1">Detail</a>
            <a href="https://github.com/moon23k/Dialog_SemEnt" class="btn-type-1">Code</a>
          </div>
    </div>
  </div>

  <div class="parallel">
    <div class='parallel-item'>
      <h3>Hierarchical&hairsp; Summarization</h3>
      <ul>
        <li>
          <p>긴 입력시퀀스를 잘 이해할 수 있도록 계층적 인코더 모델을 구현하고, 기존 방식의 모델과 요약 성능을 비교
          </p>
          <div class="btn-list">
            <a href="{{ '/summarization_hierarchical' | relative_url }}" class="btn-type-1">Detail</a>
            <a href="https://github.com/moon23k/Sum_HierEncoder" class="btn-type-1">Code</a>
          </div>

        </li>
      </ul>
    </div>

    <div class='parallel-item'>
      <h3 style="letter-spacing: 1.2px;">Sparse&hairsp; Summarization</h3>
      <ul>
        <li>
          <p>Full Attention 연산의 복잡도를 개선시킨 Sparse Attention 기반의 모델을 구현하고, 기존 방식의 모델과 요약 성능을 비교 
          </p>
          <div class="btn-list">
            <a href="{{ '/summarization_sparse' | relative_url }}" class="btn-type-1">Detail</a>
            <a href="https://github.com/moon23k/Sum_Sparse" class="btn-type-1">Code</a>
          </div>

        </li>
      </ul>
    </div>

  </div>
</div>



<script src="./assets/js/script.js"></script>