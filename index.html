---
layout: default
---

<!-- Information -->
<div class="info">
  <h2 style="color: #303030;">Hi there!</h2>
  <p>
    &nbsp; 안녕하세요 :) &nbsp;   
    인공지능에 매료되어, 인공지능 기술을 통한 현실의 문제 해결에 집중하고 있는  
    Machine Learning Engineer, Moon입니다.
    다양한 인공지능 활용 분야들 중에서도 특히, 자연어 처리에 초점을 맞추고 있습니다. 
    사람과 자연스럽게 의사소통 가능한 인공지능 모델 개발을 목표로 삼고 있습니다. 
    목표 달성을 위해 다양한 연구 주제를 직접 선정하고 구현하며, 
    끊임없이 한 발짝씩 목표에 다가가는 중입니다.  
  </p>
  <div class="btn-block">
    <a href="{{ '/resume' | relative_url }}" class="btn-type-1">Resume</a>
    <a href="https://github.com/moon23k" class="btn-type-1">Github</a>
    <a href="https://shy-vole-f74.notion.site/Hello-I-m-Moon-e1ecc2e40b32405e997713cfb44e4f3c?pvs=4" class="btn-type-1">Notion</a>
  </div>
</div>


<!-- First Research -->
<div class="container">
  
  <div class="full-width">
    <h3>Model Structure</h3>
    <p>
      &nbsp; Machine Learning Engineering에서 모델의 구조적 디자인은 성능을 결정짓는 핵심 요소입니다. 특히 자연어 생성과 같이 Encoder와 Decoder가 모두 필요한 경우, 모델 디자인의 중요성은 더욱 커집니다. 학습 방법 및 데이터처리 등 다른 변인을 고정한 채, 모델 구조에만 초점을 맞춘 다양한 실험을 통해 각 모델 구조별 특성을 파악하고, 인사이트를 공유합니다.
    </p>
  </div>

  <div class="parallel">
    <div class='parallel-item'>
      <h3>RNN Sequence to Sequence</h3>
      <ul>
        <li>
          <p>
            RNN, LSTM, GRU 세가지 네트워크를 활용해 Sequence to Sequence 모델 구조를 구현하고, 세 가지 NLG Task에서의 실효성 검증
          </p>
          <div class="btn-list">
            <a href="{{ '/rnn_seq2seq' | relative_url }}" class="btn-type-1">Detail</a>
            <a href="#" class="btn-type-1">Code</a>
          </div>

        </li>
      </ul>
    </div>

    <div class='parallel-item'>
      <h3>RNN Sequence to Sequence with Attention</h3>
      <ul>
        <li>
          <p>
            GRU Sequence to Sequence 모델 구조에서 세가지 Attention Mechanism을 구현하고, 세 가지 NLG Task에서의 실효성 검증
          </p>
          <div class="btn-list">
            <a href="{{ '/rnn_seq2seq_attn' | relative_url }}" class="btn-type-1">Detail</a>
            <a href="#" class="btn-type-1">Code</a>
          </div>

        </li>
      </ul>
    </div>

  </div>

  <div class="parallel">
    <div class='parallel-item'>
      <h3>Transformer</h3>
      <ul>
        <li>
          <p>
            Attention Mechanism만으로 효율성과 성능을 획기적으로 상승시킨 Transformer를 구현하고, 세 가지 NLG Task에서의 실효성 검증
          </p>
          <div class="btn-list">
            <a href="{{ '/transformer' | relative_url }}" class="btn-type-1">Detail</a>
            <a href="#" class="btn-type-1">Code</a>
          </div>

        </li>
      </ul>
    </div>

    <div class='parallel-item'>
      <h3>Transformer Variants</h3>
      <ul>
        <li>
          <p>
            Standard Transformer, Recurrent Transformer, Evolved Transformer 모델을 구현하고 , 세 가지 NLG Task에서의 실효성 검증
          </p>
          <div class="btn-list">
            <a href="{{ '/transformer_variants' | relative_url }}" class="btn-type-1">Detail</a>
            <a href="#" class="btn-type-1">Code</a>
          </div>

        </li>
      </ul>
    </div>

  </div>

  <div class="parallel">
    <div class='parallel-item'>
      <h3>Encoder & Decoder Balance</h3>
      <ul>
        <li>
          <p>
            Sequence to Sequence의 기본 골조인 Encoder와 Decoder의 디자인 방식에 따른 차이를 알아보고, 세 가지 NLG Task에서의 실효성 검증
          </p>
          <div class="btn-list">
            <a href="{{ '/balance' | relative_url }}" class="btn-type-1">Detail</a>
            <a href="#" class="btn-type-1">Code</a>
          </div>

        </li>
      </ul>
    </div>

    <div class='parallel-item'>
      <h3>PLM Fusion</h3>
      <ul>
        <li>
          <p>
            PLM Encoder를 Sequence to Sequence 구조에서 효과적으로 사용할 수 있는 방법론을 구현하고, 세 가지 NLG Task에서의 실효성 검증
          </p>
          <div class="btn-list">
            <a href="{{ '/plm_fusion' | relative_url }}" class="btn-type-1">Detail</a>
            <a href="#" class="btn-type-1">Code</a>
          </div>

        </li>
      </ul>
    </div>

  </div>

</div>



<!-- Second Research -->
<div class="container">
  
  <div class="full-width">
    <h3>Efficiency</h3>
    <p>
      &nbsp; 다량의 파라미터로 이루어진 대규모 딥러닝 모델일수록 좋은 성능을 보여주는 경향성이 짙습니다. 
      하지만 대규모 모델의 경우, 일반적인 컴퓨팅 환경에서 활용하기 어렵다는 한계점이 있습니다. 
      한정적인 컴퓨팅 자원에서도 좋은 모델을 사용하기 위해 효율성에 초점을 맞춘 프로젝트를 진행하고, 인사이트를 공유합니다.
    </p>
  </div>

  <div class="parallel">
    <div class='parallel-item'>
      <h3>Efficient Training</h3>
      <ul>
        <li>
          <p>
            RNN, LSTM, GRU 세가지 네트워크를 활용해 Sequence to Sequence 모델 구조를 구현하고, 세 가지 NLG Task에서의 실효성 검증
          </p>
          <div class="btn-list">
            <a href="{{ '/eff_train' | relative_url }}" class="btn-type-1">Detail</a>
            <a href="#" class="btn-type-1">Code</a>
          </div>

        </li>
      </ul>
    </div>

    <div class='parallel-item'>
      <h3>Efficient PreTrained Language Models</h3>
      <ul>
        <li>
          <p>
            RNN, LSTM, GRU 세가지 네트워크를 활용해 Sequence to Sequence 모델 구조를 구현하고, 세 가지 NLG Task에서의 실효성 검증
          </p>
          <div class="btn-list">
            <a href="{{ '/eff_model' | relative_url }}" class="btn-type-1">Detail</a>
            <a href="#" class="btn-type-1">Code</a>
          </div>

        </li>
      </ul>
    </div>

  </div>

  <div class="parallel">
    <div class='parallel-item' style="width: 47%">
      <h3>Parameter Efficient Fine-Tuning</h3>
      <ul>
        <li>
          <p>
            RNN, LSTM, GRU 세가지 네트워크를 활용해 Sequence to Sequence 모델 구조를 구현하고, 세 가지 NLG Task에서의 실효성 검증
          </p>
          <div class="btn-list">
            <a href="{{ '/peft' | relative_url }}" class="btn-type-1">Detail</a>
            <a href="#" class="btn-type-1">Code</a>
          </div>

        </li>
      </ul>
    </div>

  </div>


</div>


<!-- Third Research -->
<div class="container">
  
  <div class="full-width">
    <h3>Training Strategy</h3>
    <p>
      &nbsp; 딥러닝 엔지니어링의 주요 Process중 하나인 학습과정에 초점을 맞추어, 다양한 학습방식을 통한 모델 개선 여부를 직접 실험해봅니다.
    </p>
  </div>

  <div class="parallel">
    <div class='parallel-item'>
      <h3>Auxiliary Training</h3>
      <ul>
        <li>
          <p>
            RNN, LSTM, GRU 세가지 네트워크를 활용해 Sequence to Sequence 모델 구조를 구현하고, 세 가지 NLG Task에서의 실효성 검증
          </p>
          <div class="btn-list">
            <a href="{{ '/aux_train' | relative_url }}" class="btn-type-1">Detail</a>
            <a href="#" class="btn-type-1">Code</a>
          </div>

        </li>
      </ul>
    </div>

    <div class='parallel-item'>
      <h3>Scheduled Sampling</h3>
      <ul>
        <li>
          <p>
            RNN, LSTM, GRU 세가지 네트워크를 활용해 Sequence to Sequence 모델 구조를 구현하고, 세 가지 NLG Task에서의 실효성 검증
          </p>
          <div class="btn-list">
            <a href="{{ '/scheduled_sampling' | relative_url }}" class="btn-type-1">Detail</a>
            <a href="#" class="btn-type-1">Code</a>
          </div>

        </li>
      </ul>
    </div>

  </div>

  <div class="parallel">
    <div class='parallel-item'>
      <h3>Generative Training</h3>
      <ul>
        <li>
          <p>
            RNN, LSTM, GRU 세가지 네트워크를 활용해 Sequence to Sequence 모델 구조를 구현하고, 세 가지 NLG Task에서의 실효성 검증
          </p>
          <div class="btn-list">
            <a href="{{ '/gen_train' | relative_url }}" class="btn-type-1">Detail</a>
            <a href="#" class="btn-type-1">Code</a>
          </div>

        </li>
      </ul>
    </div>

    <div class='parallel-item'>
      <h3>SeqGAN Training</h3>
      <ul>
        <li>
          <p>
            RNN, LSTM, GRU 세가지 네트워크를 활용해 Sequence to Sequence 모델 구조를 구현하고, 세 가지 NLG Task에서의 실효성 검증
          </p>
          <div class="btn-list">
            <a href="{{ '/gan_train' | relative_url }}" class="btn-type-1">Detail</a>
            <a href="#" class="btn-type-1">Code</a>
          </div>

        </li>
      </ul>
    </div>

  </div>

</div>



<!-- Fourth Research -->
<div class="container">
  
  <div class="full-width">
    <h3>Neural Machine Translation</h3>
    <p>
      &nbsp; Machine Learning Engineering에서 모델의 구조적 디자인은 성능을 결정짓는 핵심 요소입니다. 특히 자연어 생성과 같이 Encoder와 Decoder가 모두 필요한 경우, 모델 디자인의 중요성은 더욱 커집니다. 학습 방법 및 데이터처리 등 다른 변인을 고정한 채, 모델 구조에만 초점을 맞춘 다양한 실험을 통해 각 모델 구조별 특성을 파악하고, 인사이트를 공유합니다.
    </p>
  </div>

  <div class="parallel">
    <div class='parallel-item'>
      <h3>Back Translation</h3>
      <ul>
        <li>
          <p>
            데이터의 부족함을 극복하기 위한 기법 중 하나인 Back Translation 방식을 구현하고, NMT Task에서의 실효성 검증
          </p>
          <div class="btn-list">
            <a href="{{ '/back_translation' | relative_url }}" class="btn-type-1">Detail</a>
            <a href="#" class="btn-type-1">Code</a>
          </div>

        </li>
      </ul>
    </div>

    <div class='parallel-item'>
      <h3>Multi-Lingual Translation</h3>
      <ul>
        <li>
          <p>
            여러가지 언어를 동시에 다룰수 있는 Multi Lingual Translation Model을 학습시키고, 각 언어 쌍으로만 학습한 모델과의 차이점을 확인해봅니다
          </p>
          <div class="btn-list">
            <a href="{{ '/multilinual' | relative_url }}" class="btn-type-1">Detail</a>
            <a href="#" class="btn-type-1">Code</a>
          </div>

        </li>
      </ul>
    </div>

  </div>

  <div class="parallel">
    <div class='parallel-item' style="width: 47%">
      <h3>NMT Blend</h3>
      <ul>
        <li>
          <p>
            효과적인 NMT Modeling을 위해 앞선 연구의 결과들을 적절히 Blending해보며, 성능을 비교합니다
          </p>
          <div class="btn-list">
            <a href="{{ '/nmt_blend' | relative_url }}" class="btn-type-1">Detail</a>
            <a href="#" class="btn-type-1">Code</a>
          </div>

        </li>
      </ul>
    </div>

  </div>

</div>


<!-- Fifth Research -->
<div class="container">
  
  <div class="full-width">
    <h3>Dialogue Generation</h3>
    <p>
      &nbsp; Machine Learning Engineering에서 모델의 구조적 디자인은 성능을 결정짓는 핵심 요소입니다. 특히 자연어 생성과 같이 Encoder와 Decoder가 모두 필요한 경우, 모델 디자인의 중요성은 더욱 커집니다. 학습 방법 및 데이터처리 등 다른 변인을 고정한 채, 모델 구조에만 초점을 맞춘 다양한 실험을 통해 각 모델 구조별 특성을 파악하고, 인사이트를 공유합니다.
    </p>
  </div>

  <div class="parallel">
    <div class='parallel-item'>
      <h3>Characteristic Dialogue Generation</h3>
      <ul>
        <li>
          <p>
            SeqGAN의 기법을 활용해, PreTrained GPT 모델에게 특정 개성을 부여해봅니다
          </p>
          <div class="btn-list">
            <a href="{{ '/character' | relative_url }}" class="btn-type-1">Detail</a>
            <a href="#" class="btn-type-1">Code</a>
          </div>

        </li>
      </ul>
    </div>

    <div class='parallel-item'>
      <h3>SimEnt Utilization</h3>
      <ul>
        <li>
          <p>
            대화 다양성을 증진시키기 위한 SimEnt 기법을 직접 구현하고, 실제 대화의 다양성 개선 여부를 확인해봅니다
          </p>
          <div class="btn-list">
            <a href="{{ '/siment' | relative_url }}" class="btn-type-1">Detail</a>
            <a href="#" class="btn-type-1">Code</a>
          </div>

        </li>
      </ul>
    </div>

  </div>

  <div class="parallel">
    <div class='parallel-item'>
      <h3>Multi-Turn Dialogue Generation</h3>
      <ul>
        <li>
          <p>
            Encoder구조를 개선시켜 Multi-Turn Dialogue Generation Model 구현하고, 실제 성능을 평가해봅니다 
          </p>
          <div class="btn-list">
            <a href="{{ '/multiturn' | relative_url }}" class="btn-type-1">Detail</a>
            <a href="#" class="btn-type-1">Code</a>
          </div>

        </li>
      </ul>
    </div>

    <div class='parallel-item'>
      <h3>Dialogue Generation Blend</h3>
      <ul>
        <li>
          <p>
            앞선 연구결과를 바탕으로 최적의 Dialogue Generation모델을 구현하고, 성능을 확인해봅니다.
          </p>
          <div class="btn-list">
            <a href="{{ '/dialog_blend' | relative_url }}" class="btn-type-1">Detail</a>
            <a href="#" class="btn-type-1">Code</a>
          </div>

        </li>
      </ul>
    </div>

  </div>

</div>


<!-- Sixth Research -->
<div class="container">
  
  <div class="full-width">
    <h3>Abstractive Text Summarization</h3>
    <p>
      &nbsp; Machine Learning Engineering에서 모델의 구조적 디자인은 성능을 결정짓는 핵심 요소입니다. 특히 자연어 생성과 같이 Encoder와 Decoder가 모두 필요한 경우, 모델 디자인의 중요성은 더욱 커집니다. 학습 방법 및 데이터처리 등 다른 변인을 고정한 채, 모델 구조에만 초점을 맞춘 다양한 실험을 통해 각 모델 구조별 특성을 파악하고, 인사이트를 공유합니다.
    </p>
  </div>

  <div class="parallel">
    <div class='parallel-item'>
      <h3>Hierarchical Encoder</h3>
      <ul>
        <li>
          <p>
            긴 입력시퀀스를 잘 이해할 수 있도록 Hierarchical Encoder를 구현하고, 성능을 평가합니다
          </p>
          <div class="btn-list">
            <a href="{{ '/hierarchical' | relative_url }}" class="btn-type-1">Detail</a>
            <a href="#" class="btn-type-1">Code</a>
          </div>

        </li>
      </ul>
    </div>

    <div class='parallel-item'>
      <h3>Sparse Attention</h3>
      <ul>
        <li>
          <p>
            Full Attention 연산의 복잡도를 개선시킨 Sparse Attention Based Model을 구현하고, 성능을 비교해봅니다 
          </p>
          <div class="btn-list">
            <a href="{{ '/sparse_attention' | relative_url }}" class="btn-type-1">Detail</a>
            <a href="#" class="btn-type-1">Code</a>
          </div>

        </li>
      </ul>
    </div>

  </div>

  <div class="parallel">
    <div class='parallel-item' style="width: 47%">
      <h3>Summarization Blend</h3>
      <ul>
        <li>
          <p>
            앞선 연구들의 결과를 활용해 개선된 Summarization Model을 구현하고, 성능을 확인합니다
          </p>
          <div class="btn-list">
            <a href="{{ '/sum_blend' | relative_url }}" class="btn-type-1">Detail</a>
            <a href="#" class="btn-type-1">Code</a>
          </div>

        </li>
      </ul>
    </div>

  </div>

</div>


<script src="./assets/js/script.js"></script>