---
layout: default
permalink: /baselines
---


<div id="detail">
  <h1 class="title">The&hairsp; Baselines</h1>  
  <div class="post">

    <hr>
    <div class="callout">
      <p class="highlighted-text"><span>Project Objective</span></p>
      <div class="small-spacer"></div>
      <p>&nbsp; 이 프로젝트는 세 가지 기초 자연어 생성 딥러닝 모델 아키텍처인 <b>RNN Seq2Seq</b>, <b>RNN Seq2Seq with Attention</b>, 그리고 <b>Transformer</b>의 자연어 생성 능력을 <b>기계 번역</b>, <b>대화 생성</b>, <b>문서 요약</b>이라는 세 가지 주요 과제에서 면밀하게 비교 분석합니다. 이 프로젝트를 통해 순환신경망, 어텐션, 트랜스포머로 이어지는 딥러닝 모델 아키텍처의 발전 과정이 자연어 생성 능력에 미치는 영향력을 확인하고, 추후 고도화된 모델 및 학습 방법론에 따른 성능 개선 여부를 판단하기 위한 기준점을 확립합니다. 전체 실험의 구현 코드는 <a class="linked-text" href="https://github.com/moon23k/Baselines.git">https://github.com/moon23k/Baselines.git</a>에서 확인할 수 있습니다
      </p>
    </div>

    <div class="tblContents">
      <ul>

        <li>
          <a href="#1">1. &hairsp; Introduction</a>
        </li>

        <li>
          <a href="#2">2. &hairsp; Background</a>
        </li>

        <li>
          <a href="#3">3. &hairsp; Architecture</a>
        </li>

        <li>
          <a href="#4">4. &hairsp; Experimental Setup</a>
        </li>

        <li>
          <a href="#5">5. &hairsp; Results</a>
        </li>

        <li>
          <a href="#6">6. &hairsp; Conclusion</a>
        </li>

        <li>
          <a href="#7">7. &hairsp; Reference</a>
        </li>

      </ul>
    </div>
    <hr>


<!-- 1. Introduction -->    
    <h2 id="1">&hairsp; 1. &hairsp; Introduction</h2>
    <div class="chapter-box">

      <div class="list-container">
        <ul>
          <li>배경</li>
          <p>자연어 생성을 위한 가장 기초적인 모델 구조인 RNN Seq2Seq, RNN Seq2Seq with Attention, Transformer를 기반으로 한 다양한 연구들이 존재하지만, 세 가지 모델의 자연어 생성 능력 발전을 다양한 자연어 생성 과제에서 비교 분석한 연구는 상대적으로 부족합니다. 특히 일반적인 컴퓨팅 환경과, 접근 가능한 양의 데이터를 기반으로 한 비교 분석은 더더욱 부족합니다. 이 프로젝트에서는 이러한 문제점을 직접 모델을 구현하고, 실험하고, 정리하며 해결합니다.
          </p>

          <div class="spacer"></div>
          <li>목적</li>
          <p>RNN Seq2Seq, RNN Seq2Seq with Attention, Transformer 모델의 자연어 생성 능력을 기반으로 한 장단점 및 특성을 파악할 수 있습니다.
          </p>

          <div class="spacer"></div>
          <li>기여 및 기대 효과</li>
          <p>RNN Seq2Seq, RNN Seq2Seq with Attention, Transformer 모델의 자연어 생성 능력을 기반으로 한 장단점 및 특성을 파악할 수 있습니다.
          </p>

        </ul>
      </div>
    </div>



<!-- 2. Background -->    
    <h2 id="2">&hairsp; 2. &hairsp; Background</h2>
    <div class="chapter-box">
      <div class="list-container">
        <ul>

          <li>Natural Language Generation Deep Learning Model
            <ul>
              <li>자연어 생성 기술의 중요성: 자연어 처리 분야에서 자연어 생성 기술은 매우 중요한 역할을 합니다. 이는 챗봇, 기계 번역, 자동 요약, 문서 생성 등 다양한 응용 분야에 활용됩니다.</li>
              <li>업무 및 학문적인 영역에서의 활용: 자연어 생성 기술은 비즈니스 커뮤니케이션, 교육, 의료, 기계 학습 등의 분야에서 다양한 영역에 적용되고 있습니다.</li>
              <li>기존 순환 신경망 대비 훨씬 높은 성능과 학습 효율성을 제공하며, 대규모 데이터셋에서 활용되는 강력한 모델</li>
            </ul>
          </li>

          <div class="spacer"></div>
          <li>Need for Benchmarks
            <ul>
              <li>대부분의 연구 논문에서는 가능한 큰 자원을 사용한, 최선의 결과를 보여주곤합니다. 하지만 일반적인 이용자의 경우, 한정된 자원에서 다양한 과제에 활용하길 원합니다.</li>
              <li>성능 기준점의 필요성: 자연어 생성 모델의 성능을 평가하기 위해서는 표준적인 벤치마크 및 성능 지표가 필요합니다. 이를 통해 다양한 모델을 비교하고, 연구의 진전을 추적할 수 있습니다.</li>
              <li>소규모 데이터셋과, 일반적인 컴퓨팅 환경에서 자연어 생성을 위한 기본적 성능 기준표의 부재가 문제가 됨.</li>
            </ul>
          </li>

        </ul>
      </div>
    </div>




<!-- Architecture -->    
    <h2 id="3">&hairsp; 3. &hairsp; Architecture</h2>

    <div class="chapter-box">

      <div class="list-container">
        <ul>
          <li>RNN Sequence to Sequence</li>
          <p> &nbsp; 가장 기본적인 구현 모델로, 코드 구현 역시 연산 과정을 직관적으로 반영하고 있는 방식으로 구현된 모델입니다. MultiHeadAttention을 포함한 대부분의 Transformer 컴포넌트를 구현한 모델입니다.
          </p>


          <div class="spacer"></div>
          <li>RNN Seq2Seq with Attention</li>
          <p> &nbsp; Pytorch 라이브러리에서 제공하는 내부 클래스를 활용해 구현한 모델입니다. Scratch Model과 유사한 모델 구조를 설정하기 위해 nn.Transformer 클래스를 그대로 사용하지 않고, nn.TransformerEncoderLayer와 nn.TransformerDecoderLayer만을 활용해 Scratch Model과 동일한 연산과정을 구현했습니다. 
          </p>

          <div class="spacer"></div>
          <li>Transformer</li>
          <p> &nbsp; 앞서 소개한 Scratch Model과 Torch Model의 성능 차이를 주요하게 견인하는 MultiHeadAttention에 집중하여 Scratch Model의 틀안에서 MultiHeadAttention 클래스 만을 Pytorch에서 제공하는 클래스를 사용한 모델입니다. 이와 같이 필요한 부분만을 상속받으며, 
          </p>


        </div>
    </div>




<!-- 4. Experimental Setup -->
    <h2 id="4">&hairsp; 4. &hairsp; Experimental Setup</h2>
    <div class="chapter-box">

      <div class="list-container">
        <ul>
          <li>Data Setup
            <div class="small-spacer"></div>
            <div class="dual-container">
              <ul>
                <li><b>Translation Task</b>: &nbsp; WMT'14 En-De</li>
                <li><b>Dialogue Task</b>: &nbsp; Daily Dialogue</li>
                <li><b>Summarization Task</b>: &nbsp; CNN Daily</li>
                <li><b>Tokenizer</b>: &nbsp; BPE Tokenizer</li>
              </ul>
              <ul>
                <li><b>Train Data Volumn</b>: &nbsp; 50,000</li>
                <li><b>Valid Data Volumn</b>: &nbsp; 5,000</li>
                <li><b>Test Data Volumn</b>: &nbsp; 100</li>
                <li><b>Vocab Size</b>: &nbsp; 15,000</li>                
              </ul>
            </div>
          </li>

          <div class="spacer"></div>
          <li>Training Setup
            <div class="small-spacer"></div>
            <div class="dual-container">
              <ul>
                <li><b>Num Epochs</b>: &nbsp; 10</li>
                <li><b>Batch Size</b>: &nbsp; 32</li>
                <li><b>Learning Rate</b>: &nbsp; 5e-4</li>
                <li><b>LR Scheduler</b>: &nbsp; ReduceLROnPlateau</li>
              </ul>
              <ul>
                <li><b>Optimizer</b>: &nbsp; AdamW</li>
                <li><b>Apply Mixed Precision</b>: &nbsp; True</li>
                <li><b>Gradient Accumulation Steps</b>: &nbsp; 4
                <li><b>Early Stop Patience</b>: &nbsp; 3</li>                
              </ul>
            </div>
          </li>

          <div class="spacer"></div>
          <li>RNN Sequence to Seqeunce Model Setup
            <div class="small-spacer"></div>
            <div class="dual-container">
              <ul>
                <li><b>Input Dim</b>: &nbsp; 15,000</li>
                <li><b>Output Dim</b>: &nbsp; 15,000</li>
                <li><b>Hidden Dim</b>: &nbsp; 256</li>
                <li><b>RNN Cell Type</b>: &nbsp; GRU</li>
              </ul>
              <ul>
                <li><b>N Layers</b>: &nbsp; 3</li>
                <li><b>N Directions</b>: &nbsp; 8</li>
                <li><b>Dropout Ratio</b>: &nbsp; 0.1</li>
                <li><b>Model Params</b>: &nbsp; 15,488,664</li>              
              </ul>
            </div>
          </li>

          <div class="spacer"></div>
          <li>RNN Sequence to Seqeunce Model Setup
            <div class="small-spacer"></div>
            <div class="dual-container">
              <ul>
                <li><b>Input Dim</b>: &nbsp; 15,000</li>
                <li><b>Output Dim</b>: &nbsp; 15,000</li>
                <li><b>Hidden Dim</b>: &nbsp; 256</li>
                <li><b>RNN Cell Type</b>: &nbsp; GRU</li>
              </ul>
              <ul>
                <li><b>N Layers</b>: &nbsp; 3</li>
                <li><b>N Directions</b>: &nbsp; 8</li>
                <li><b>Attention Type</b>: &nbsp; Bahdanau</li>
                <li><b>Model Params</b>: &nbsp; 15,488,664</li>              
              </ul>
            </div>
          </li>

          <div class="spacer"></div>
          <li>Transformer Model Setup
            <div class="small-spacer"></div>
            <div class="dual-container">
              <ul>
                <li><b>Input Dim</b>: &nbsp; 15,000</li>
                <li><b>Output Dim</b>: &nbsp; 15,000</li>
                <li><b>Hidden Dim</b>: &nbsp; 256</li>
                <li><b>PFF Dim</b>: &nbsp; 512</li>
              </ul>
              <ul>
                <li><b>N Layers</b>: &nbsp; 3</li>
                <li><b>N Heads</b>: &nbsp; 8</li>
                <li><b>Norm First</b>: &nbsp; True</li>
                <li><b>Model Params</b>: &nbsp; 15,488,664</li>              
              </ul>
            </div>
          </li>

        </ul>
      </div>
    </div>



<!-- Result --> 
    <h2 id="5">&hairsp; 5. &hairsp; Result</h2>
    <div class="chapter-box">

      <div class="list-container">
        <ul>
          <li>Machine Translation</li>

            <table class="result-table">
              <thead>
                <tr>
                  <th>Model Type</th>
                  <th>Eval Score</th>
                  <th>Epoch Time</th>
                  <th>Avg GPU</th>
                  <th>Max GPU</th>
                </tr>
              </thead>
              <tbody>
                <tr>
                  <td>Scratch Model</td>
                  <td>11.51</td>
                  <td>53s</td>
                  <td>0.20 GB</td>
                  <td>1.05 GB</td>
                </tr>
                <tr>
                  <td>Torch Model</td>
                  <td>13.72</td>
                  <td>45s</td>
                  <td>0.20GB</td>
                  <td>0.95GB</td>
                </tr>
                <tr>
                  <td>Hybrid Model</td>
                  <td>13.81</td>
                  <td>45s</td>
                  <td>0.20GB</td>
                  <td>0.95GB</td>
                </tr>


              </tbody>
            </table>

          <div class="spacer"></div>
          <li>Dialogue Generation</li>

            <table class="result-table">
              <thead>
                <tr>
                  <th>Model Type</th>
                  <th>Eval Score</th>
                  <th>Epoch Time</th>
                  <th>Avg GPU</th>
                  <th>Max GPU</th>
                </tr>
              </thead>
              <tbody>
                <tr>
                  <td>Scratch Model</td>
                  <td>3.40</td>
                  <td>49s</td>
                  <td>0.20 GB</td>
                  <td>0.90 GB</td>
                </tr>
                <tr>
                  <td>Torch Model</td>
                  <td>1.34</td>
                  <td>42s</td>
                  <td>0.20GB</td>
                  <td>0.85GB</td>
                </tr>
                <tr>
                  <td>Hybrid Model</td>
                  <td>1.58</td>
                  <td>45s</td>
                  <td>0.20GB</td>
                  <td>0.84GB</td>
                </tr>                

              </tbody>
            </table>

          <div class="spacer"></div>
          <li>Text Summarization</li>

            <table class="result-table">
              <thead>
                <tr>
                  <th>Model Type</th>
                  <th>Eval Score</th>
                  <th>Epoch Time</th>
                  <th>Avg GPU</th>
                  <th>Max GPU</th>
                </tr>
              </thead>
              
              <tbody>
                <tr>
                  <td>Scratch Model</td>
                  <td>7.10</td>
                  <td>5m 41s</td>
                  <td>0.21 GB</td>
                  <td>5.67 GB</td>
                </tr>

                <tr>
                  <td>Torch Model</td>
                  <td>7.68</td>
                  <td>2m 41s</td>
                  <td>0.21GB</td>
                  <td>2.89GB</td>
                </tr>

                <tr>
                  <td>Hybrid Model</td>
                  <td>7.40</td>
                  <td>2m 42s</td>
                  <td>0.21GB</td>
                  <td>2.90GB</td>
                </tr>

              </tbody>
            </table>

          <div class="spacer"></div>  
          <li>Result Analysis</li>
            <p> &nbsp; 위의 결과 표를 통해 기계번역, 대화생성, 문서요약 모든 자연어 과제에서 Torch Model이 Scratch Model보다 성능적으로도, 효율성에서도 우수함을 확인할 수 있었습니다. 모델의 파라미터 및 연산과정은 동일하지만, Multi Head Attention 연산과정의 고도화 부족이 이러한 차이를 유발한 것으로 판단됩니다. 실제   
            </p>

        </ul>
      </div>
    </div>



    <h2 id="6">&hairsp; 6. &hairsp; Conclusion</h2>
    <div class="chapter-box">

      <div class="list-container">
        <ul>
          <li>직접 코드 구현을 통한 Transformer 모델에 대한 깊은 이해</li>
            <p>모델 구현을 통해 논문에서 글로 서술된 Transformer를 코드적으로 이해하게 되었습니다. 
                특히 모델의 구현을 위한 다양한 코드 구현을 했지만, 간단한 부분에서의 다른 구현이 성능의 하락을 야기하기도 했습니다.
                가장 대표적으로는 코드적으로 깔끔해 보이기는 했지만, 안좋은 성능을 보였던 아래와 같은 경우도 존재했습니다.
                결과적으로 가장 직관적으로 구현한 코드가 좋은 성능을 보였습니다. <br>
                추가적으로 논문에서 소개한 Transformer Base 모델과 동일한 하이퍼 파라미터를 적용하면, 데이터량 대비 과도하게 큰 모델이 되어 학습이 정상적으로 진행되지 않음을 실험적으로 확인.
                학습 요건에 맞는 모델링 구성이 중요함을 알게됨.
            </p>
        

          <div class="spacer"></div>
          <li>Transformer의 자연어 생성 능력 측정</li>
          <p>&nbsp; 5만개라는 소규모 데이터로 학습했음에도 불구하고, Transformer는 기계번역, 대화생성, 문서요약이라는 어려운 과제에서 000이라는 성능을 기록했습니다.
            Transformer 이전 가장 많이 활용되던 RNN Sequence to Sequence with Attention 모델과 비교하면 00%의 성능 개선, 00%의 효율성 개선이라는 뛰어난 기술적 진보를 만들어냈습니다.
          </p>

          <div class="spacer"></div>
          <li>직접 구현한 Transformer와 Pytorch에서 제공하는 Transformer모델의 성능 비교</li>
          <p>Pytorch에서 제공하는 Transformer모델이 직접 구현한 Transformer 모델보다 성능 및 효율성 측면에서 모두 뛰어남을 보였습니다.
                모델 파라미터 개수, 연산과정에서는 차이가 없으나, 이러한 차이를 만들어낸 가장 큰 원인은 Multi Head Attention클래스의 고도화 였습니다.
                직접 구현한 모델에 Multi Head Attention클래스만 이식해서 실험해본 결과, Pytorch 모델과 동일한 성능을 보임.
          </p>
        </ul>
      </div>
    </div>


    <h2 id="7">&hairsp; 7. &hairsp; Reference</h2>
    <div class="chapter-box">
      <a class="reference" href="https://arxiv.org/abs/1706.03762">&nbsp; Attention Is All You Need</a> <br>         
      <a class="reference" href="https://arxiv.org/abs/1706.03762">&nbsp; Attention Is All You Need</a> <br> 
      <a class="reference" href="https://arxiv.org/abs/1706.03762">&nbsp; Attention Is All You Need</a>
    </div>

  </div>
</div>


<div class="pagination">
  <a href="{{ '/summarization_sparse' | relative_url }}" class="btn-prev"><span>Sparse Summarization</span></a>
  <a href="{{ '/transformer_balance' | relative_url }}" class="btn-next"><span>Transformer Balance</span></a>
</div>