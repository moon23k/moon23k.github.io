---
layout: default
permalink: /eff_model
---


<div id="detail">
  <h1 class="title">Efficient PLM Comparision</h1>  
  <div class="post">

    <hr>
    <div class="callout">
      <p class="highlighted-text"><span>Project Objective</span></p>
      <div class="small-spacer"></div>
      <p>&nbsp; 이 프로젝트에서는 효율적인 사전학습 모델들의 효율성과 성능을 비교합니다. 총 두가지의 모델군으로 나누어 비교를 진행합니다.
        첫번째 모델군에는 파라미터의 수를 줄여 경량화 함으로써 효율성을 제고시킨 모델들을. 두번째 모델군은 어텐션 연산과정에서 효율성을 증대시킨 모델들.
        모든 모델군의 Baseline 모델로는 BERT를 설정
        첫번째 모델군에
      </p>
    </div>

    <div class="tblContents">
      <ul>

        <li>
          <a href="#1">1. &hairsp; Introduction</a>
        </li>

        <li>
          <a href="#2">2. &hairsp; Parameter-wise Efficient Models</a>
        </li>

        <li>
          <a href="#3">3. &hairsp; Attention-wise Efficient Models</a>
        </li>

        <li>
          <a href="#4">4. &hairsp; Experimental Setup</a>
        </li>

        <li>
          <a href="#5">5. &hairsp; Results</a>
        </li>

        <li>
          <a href="#6">6. &hairsp; Conclusion</a>
        </li>

        <li>
          <a href="#7">7. &hairsp; Reference</a>
        </li>

      </ul>
    </div>
    <hr>



<!-- 1. Introduction -->    
    <h2 id="1">&hairsp; 1. &hairsp; Introduction</h2>
    <div class="chapter-box">

      <div class="list-container">
        <ul>
          <li>Description</li>
            <p> &nbsp; 
              사전학습된 모델이 좋은 성능을 보인다는 것은 다양한 방면에 걸쳐 입증되어 왔습니다. 일반적으로 더 큰 모델일수록 더 좋은 성능을 보이고는 있지만, 그 만큼 많은 컴퓨팅자원을 필요로합니다. 때문에 모델의 성능을 최대한 유지하면서도 모델의 효율성을 제고시키기 위한 연구들이 진행으며, 대표적 연구 방향은 두 가지로 나누어 볼 수 있습니다. 
              첫번째는 모델의 파라미터 개수를 감소시키는 것과, 두번째는 Transformer의 핵심 연산인 Attention의 \( O^2 \) 복잡성을 개선시키는 연구입니다.

              이렇게 효율성 증진에 대한 연구는 활발히 진행 중이지만, 다양한 효율성 증대 모델들을 동일한 조건에서 비교 검증하는 연구는 부재합니다.
              이 프로젝트에서는 이런 문제점을 직접 해결하고, 효율성의 측면에서 다양한 사전학습 모델들을 검증해봅니다. 

              <br>

              비교를 위한 모델들을 우선 두 가지 방향성에 따라 나누어 비교합니다. 첫번째 모델 군은 파리미터를 감소시킨 모델들을 일컫는 Parameter-wise Efficient Models, 그리고 두번째 모델군은 Attention 연산의 효율성을 증대시킨 Attention-wise Efficient Models이라고 명명하고, 각 연구에 부합하는 모델들을 비교합니다.  
              
              Parameter-wise Efficient Models에는 Distil BERT, AlBERT, Mobile BERT를, Attention-wise Efficient Models에는 Longformer, BigBird를 선정했습니다.
              모든 모델군에서의 Baseline 모델로는 BERT를 사용합니다.
            </p>

          <div class="spacer"></div>
          <li>Objective
            <ul>
              <li style="font-weight: normal;">효율성 증진과 크로스 오버로 발생가능한 성능 하락 사이 </li>
              <li style="font-weight: normal;">다양한 구현 방식에 따른 Transformer의 성능 차이를 직접 확인</li>
              <li style="font-weight: normal;">Transformer의 자연어 생성 능력 측정을 통해 Transformer BaseLine 확립</li>
              <li style="font-weight: normal;">직접 구현한 Transformer와 Pytorch에서 제공하는 Transformer모델의 성능 비교</li>
            </ul>
          </li>

        </ul>
      </div>

    </div>



<!-- 2. Parameter-wise Efficient Models -->    
    <h2 id="2">&hairsp; 2. &hairsp; Parameter-wise Efficient Models</h2>
    <div class="chapter-box">      
      <div class="list-container">
        <ul>
          <li>Distill BERT
            <ul>
              
              <li>Knowledge Distillation 기법을 활용해 BERT를 경량화 시킨 모델</li>
              
              <li><b>Forget Gate (0~1)</b>: &nbsp; \( f_t = \theta (W_f x_t + U_f h_{t-1} + b_f) \) <br>과거 정보를 기억할지 아닐지 여부를 결정하는 Gate. 1에 가까울수록 과거 정보를 강하게 유지</li>
              

              <li>연산 과정이 복잡해, 연산 속도가 상대적으로 느림</li>
            </ul>

          </li>


          <div class="spacer"></div>
          <li>AlBERT
            <ul>

              <li>GRU는 LSTM의 취지와 동일하게 네트워크의 장기기억력을 향상시키기 위한 네트워크이지만, LSTM보다 간단한 구조를 지니고 있으며, 파라미터의 수 또한 적어 LSTM보다 학습속도가 빠름</li>
              
              <li><b>Update Gate(0~1)</b>: &nbsp; \( z_t = \theta (W_z x_t + U_z h_{t-1} + b_z) \) <br> LSTM Cell의 Forget, Input Gate와 유사한 역할을 하며, 과거와 현재의 정보를 각각 얼마나 반영할지에 대한 비율값을 표현</li>


              <li>연산의 복잡성 및 속도는 RNN과 LSTM 중간정도에 위치</li>
            </ul>
          </li>

          <div class="spacer"></div>
          <li>Mobile BERT
            <ul>

              <li>모바일 디바이스에서도 작동가능한 뛰어난 사전학습 딥러닝 모델을 목표로 BERT를 경량화 시킨 모델</li>
              
              <li><b>Update Gate(0~1)</b>: &nbsp; \( z_t = \theta (W_z x_t + U_z h_{t-1} + b_z) \) <br> LSTM Cell의 Forget, Input Gate와 유사한 역할을 하며, 과거와 현재의 정보를 각각 얼마나 반영할지에 대한 비율값을 표현</li>


              <li>연산의 복잡성 및 속도는 RNN과 LSTM 중간정도에 위치</li>
            </ul>
          </li>


          <div class="spacer"></div>
          <li>Comparision Table</li>
          <table class="result-table">
            <thead>
              <tr>
                <th>Model</th>
                <th>Params</th>
                <th>Light Weightening Strategy</th>
              </tr>
            </thead>
            <tbody>
              <tr>
                <td><b>BERT</b></td>
                <td>Low</td>
                <td>High</td>
              </tr>

              <tr>
                <td><b>Distil BERT</b></td>
                <td>Low</td>
                <td>High</td>
              </tr>

              <tr>
                <td><b>AlBERT</b></td>
                <td>bert-base-uncased</td>
                <td>High</td>
              </tr>

              <tr>
                <td><b>Mobile BERT</b></td>
                <td>Low</td>
                <td>High</td>
              </tr>

            </tbody>
          </table>

        </ul>
      </div>
    </div>         



<!-- 3. Attention-wise Efficient Models -->    
    <h2 id="3">&hairsp; 3. &hairsp; Attention-wise Efficient Models</h2>
    <div class="chapter-box">      
      <div class="list-container">
        <ul>
          <li>Longformer
            <ul>
              
              <li>LSTM은 장기기억력 보존을 목적으로 개선된 순환 신경망 구조로, 3개의 게이트와 2개의 State를 사용.</li>
              
              <li><b>Forget Gate (0~1)</b>: &nbsp; \( f_t = \theta (W_f x_t + U_f h_{t-1} + b_f) \) <br>과거 정보를 기억할지 아닐지 여부를 결정하는 Gate. 1에 가까울수록 과거 정보를 강하게 유지</li>

            </ul>

          </li>


          <div class="spacer"></div>
          <li>Bigbird
            <ul>

              <li>GRU는 LSTM의 취지와 동일하게 네트워크의 장기기억력을 향상시키기 위한 네트워크이지만, LSTM보다 간단한 구조를 지니고 있으며, 파라미터의 수 또한 적어 LSTM보다 학습속도가 빠름</li>
              
              <li><b>Update Gate(0~1)</b>: &nbsp; \( z_t = \theta (W_z x_t + U_z h_{t-1} + b_z) \) <br> LSTM Cell의 Forget, Input Gate와 유사한 역할을 하며, 과거와 현재의 정보를 각각 얼마나 반영할지에 대한 비율값을 표현</li>

            </ul>
          </li>



          <div class="spacer"></div>
          <li>Comparision Table</li>

          <table class="result-table">
            <thead>
              <tr>
                <th>Model</th>
                <th>Params</th>
                <th>Attention</th>
              </tr>
            </thead>
            <tbody>
              <tr>
                <td><b>BERT</b></td>
                <td>Low</td>
                <td>High</td>
              </tr>

              <tr>
                <td><b>Longformer</b></td>
                <td>Low</td>
                <td>High</td>
              </tr>

              <tr>
                <td><b>Bigbird</b></td>
                <td>bert-base-uncased</td>
                <td>High</td>
              </tr>


            </tbody>
          </table>

        </ul>
      </div>
    </div>         




<!-- 4. Experimental Setup -->
    <h2 id="4">&hairsp; 4. Experimental Setup</h2>
    <div class="chapter-box">

      <div class="list-container">
        <ul>
          <li>Data Setup
            <div class="dual-container">
              <ul>
                <li><b>Datasets</b>: &nbsp; IMDB, AG News</li>
                <li>Param-wise Efficient Model의 성능평가에서는 IMDB를, Attention-wise Efficient Model의 성능쳥가에서는 AG News를 사용</li>
                <li>모든 레이블 개수 동일하게 분배</li>
              </ul>
              <ul>
                <li><b>Train Data Volumn</b>: &nbsp; 50,000</li>
                <li><b>Valid Data Volumn</b>: &nbsp; 5,000</li>
                <li><b>Test Data Volumn</b>: &nbsp; 100</li>                
              </ul>
            </div>
          </li>


          <div class="half-spacer"></div>
          <li>Training Setup
            <div class="dual-container">
              <ul>
                <li><b>Num Epochs</b>: &nbsp; 10</li>
                <li><b>Batch Size</b>: &nbsp; 32</li>
                <li><b>Learning Rate</b>: &nbsp; 5e-4</li>
                <li><b>LR Scheduler</b>: &nbsp; pleature</li>
              </ul>
              <ul>
                <li><b>Hardware</b>: &nbsp; T4</li>
                <li><b>Optimizer</b>: &nbsp; AdamW</li>
                <li><b>Gradient Accumulation Steps</b>: &nbsp; 4
                <li><b>Teacher Forcing Ratio</b>: &nbsp; 0.5</li>                
              </ul>
            </div>
          </li>
        </ul>
      </div>
    </div>


<!-- 5. Result -->
    <h2 id="5">&hairsp; 5. &hairsp; Result</h2>
    <div class="chapter-box">

      <div class="list-container">
        <ul>
          <li>IMDB</li>

            <table class="result-table">
              <thead>
                <tr>
                  <th>Model Type</th>
                  <th>Eval Score</th>
                  <th>Epoch Time</th>
                  <th>Avg GPU</th>
                  <th>Max GPU</th>
                </tr>
              </thead>
              <tbody>
                <tr>
                  <td>BERT</td>
                  <td>2.12</td>
                  <td>3m 58s</td>
                  <td>0.22GB</td>
                  <td>0.82GB</td>
                </tr>
                <tr>
                  <td>Ditill BERT</td>
                  <td>8.35</td>
                  <td>4m 16s</td>
                  <td>0.27GB</td>
                  <td>1.30GB</td>
                </tr>
                <tr>
                  <td>AlBERT</td>
                  <td>9.75</td>
                  <td>4m 6s</td>
                  <td>0.26GB</td>
                  <td>1.16GB</td>
                </tr>
                <tr>
                  <td>Mobile BERT</td>
                  <td>9.75</td>
                  <td>4m 6s</td>
                  <td>0.26GB</td>
                  <td>1.16GB</td>
                </tr>                

              </tbody>
            </table>
          
          <div class="half-spacer"></div>
          <li>AG News</li>
            <table class="result-table">
              <thead>
                <tr>
                  <th>Model Type</th>
                  <th>Eval Score</th>
                  <th>Epoch Time</th>
                  <th>Avg GPU</th>
                  <th>Max GPU</th>
                </tr>
              </thead>
              <tbody>
                <tr>
                  <td>BERT</td>
                  <td>0.10</td>
                  <td>3m 40s</td>
                  <td>0.22GB</td>
                  <td>0.79GB</td>
                </tr>
                <tr>
                  <td>Longformer</td>
                  <td>0.37</td>
                  <td>4m 5s</td>
                  <td>0.27GB</td>
                  <td>1.23GB</td>
                </tr>
                <tr>
                  <td>Bigbird</td>
                  <td>2.16</td>
                  <td>3m 58s</td>
                  <td>0.26GB</td>
                  <td>1.11GB</td>
                </tr>

              </tbody>
            </table>


          <div class="spacer"></div>  
          <li>Result Analysis</li>
            <p> &nbsp; 결과를 확인해보면, 모든 과제에서 GRU가 가장 좋은 성능을 보입니다. RNN은 다양한 정보를 함유하기에 연산 과정이 지나치게 단순하고, LSTM은 연산 과정이 복잡하기에 그만큼 학습 과정에서 영향받는 요소가 많아 학습 수렴이 어렵습니다. 때문에 모든 학습 결과에서는 GATE를 사용하면서도 단순화된 연산으로 학습의 이점이 큰 GRU가 가장 뛰어난 성능을 보임을 알 수 있습니다. 
            뿐만 아니라, 학습 데이터가 3만개에 불과하다는 점에서 GRU의 성능이 좋게 나온것에 큰 영향을 미친것으로 보입니다.
            학습 데이터를 더 크게 설정한 경우에는, LSTM의 성능이 향상될 것으로 예상됩니다.  
            </p>
        </ul>
      </div>
    </div>



<!-- 6. Conclusion -->
    <h2 id="6">&hairsp; 6. &hairsp; Conclusion</h2>
    <div class="chapter-box">

      <div class="list-container">
        <ul>
          <li>RNN, LSTM, GRU 을 활용한 seq2seq 모델링 방식의 이해</li>
            <p>&nbsp; 시퀀스의 벡터 변환을 위한 Embedding Layer와 실제 연산을 위한 순환 신경망 레이어만으로 구성된 Encoder, Decoder 구조 구현. 그리고 Teacher Forcing 기능을 포함하는 디코딩의 구현을 통해 RNN Sequence to Sequnce Modeling에 대한 코드적 이해도를 증진시킬 수 있었습니다.
            </p>

          <div class="half-spacer"></div>
          <li>세 가지 순환 신경망의 성능 비교 검증</li>
            <p>적은 학습 데이터 바탕의 자연어 생성 과제에서는 GRU가 RNN 및 LSTM보다 좋은 성능을 보입니다. 
            </p>
            
          <div class="half-spacer"></div>
          <li>최소 기준치 확립</li>
            <p>앞서 언급했듯이 순환 신경망을 사용한 Seq2Seq모델은 앞으로 다루게 될 다양한 Seq2Seq 모델들의 시작점입니다. 고도화 된 모델일지라도 하이퍼 파라미터 설정이나, 훈련 방식 등의 이유에서 잘못된 성능이 나올수도 있습니다. 이때, 최소한의 동작은 하는 구나 하는 판단의 근거로 다음과 같은 스코어를 활용할 수 있습니다. <br> 
            기계번역 BLEU Score: 00.00, 대화 생성 Rouge Score: 00.00, 문서 요약 Rouge Score: 00.00 이하의 성능ㅇ
            </p>

        </ul>
      </div>
    </div>


<!-- 6. Reference -->
    <h2 id="7">&hairsp; 7. &hairsp; Reference</h2>
    <div class="chapter-box">
      <a class="reference" href="https://arxiv.org/abs/1810.04805">&nbsp; BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding</a> <br> 
      <a class="reference" href="https://arxiv.org/abs/1909.11942">&nbsp; ALBERT: A Lite BERT for Self-supervised Learning of Language Representations</a> <br>
      <a class="reference" href="https://arxiv.org/abs/1910.01108">&nbsp; DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter</a> <br> 
      <a class="reference" href="https://arxiv.org/abs/2004.02984">&nbsp; MobileBERT: a Compact Task-Agnostic BERT for Resource-Limited Devices</a> <br> 

      <a class="reference" href="https://arxiv.org/abs/2004.05150">&nbsp; Longformer: The Long-Document Transformer</a> <br> 
      <a class="reference" href="https://arxiv.org/abs/2007.14062">&nbsp; Big Bird: Transformers for Longer Sequences</a>



    </div>


  </div>
</div>


<div class="pagination">
  <a href="{{ '/eff_train' | relative_url }}" class="btn-prev"><span>Efficient Training</span></a>
  <a href="{{ '/peft' | relative_url }}" class="btn-next"><span>PEFT</span></a>
</div>