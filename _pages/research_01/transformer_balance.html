---
layout: default
permalink: /transformer_balance
---


<div id="detail">
  <h1 class="title">Transformer Balance Research for NLG Tasks</h1>  
  <div class="post">

    <hr>
    <div class="callout">
      <p class="highlighted-text"><span>Project Overview</span></p>
      <div class="small-spacer"></div>
      <p>&nbsp; 이 프로젝트에서는 Transformer Seq2Seq 모델의 Encoder와 Decoder 사이 균형성에 따른 성능 변화를 다각적으로 검증합니다.

        균형성의 변인으로는 주요 연산 레이어 차원, 인코더 및 디코더 레이어의 개수, 어텐션 헤드의 개수를 설정했으며, 각 변인을 적용한 모델을 Wide, Deep, Diverse 모델이라고 명명하고, 변인이 적용된 모델간 성능을 비교합니다.

        성능의 측정에는 인코딩과 디코딩 중요도가 상이한 세 가지 자연어 생성 과제를 선택했으며, ~~ 하다는 결과를 확인했습니다.

      </p>
    </div>

    <div class="tblContents">
      <ul>

        <li>
          <a href="#1">1. &hairsp; Introduction</a>
        </li>

        <li>
          <a href="#2">2. &hairsp; Background</a>
        </li>

        <li>
          <a href="#3">3. &hairsp; Architecture</a>
        </li>

        <li>
          <a href="#4">4. &hairsp; Experimental Setup</a>
        </li>

        <li>
          <a href="#5">5. &hairsp; Results</a>
        </li>

        <li>
          <a href="#6">6. &hairsp; Conclusion</a>
        </li>

        <li>
          <a href="#7">7. &hairsp; Reference</a>
        </li>

      </ul>
    </div>
    <hr>




<!-- 1. Introduction -->    
    <h2 id="1">&hairsp; 1. &hairsp; Introduction</h2>
    <div class="chapter-box">

      <div class="list-container">
        <ul>
          <li>Description</li>
            <p> &nbsp; Transformer는 다양한 딥러닝 처리 과제에서 널리 사용되는 뛰어난 모델입니다. 일반적으로 Transformer Seq2Seq 모델의 활용 시, Encoder와 Decoder의 설정을 대부분 일치시킵니다. 
            일치 시키는 가장 주된 이유는 모델의 안정성을 위함입니다. 서로 다른 설정값에서 모델의 학습이 더 어려워지며, 결과적으로 성능 하락을 야기하기 쉽습니다.
            하지만 해결하려는 과제 마다의 특성을 면밀히 살펴보면, 특정 과제는 Encoder가 Decoder보다 중요하게 작용해야하는 과제가 존재하기도, 반대로 Decoder가 Encoder보다 중요하게 역할을 해줘야 하는 경우 역시도 존재합니다.
            대대수의 경우에서, 해결하고자 하는 과제의 성격과는 무관하게 균형잡힌 Transformer 모델을 사용하기 때문에 Encoder와 Decoder 균형성에 대한 연구가 상대적으로 부족합니다.
            이 프로젝트에서는 앞서 언급한 연구 부족의 문제점을 직접 해결하고, 모델 디자인에 있어 균형성에 대한 중요성, 혹은 의도적인 불균형성의 효과 등을 실험적으로 확인합니다.

            실험의 주요 변인은 모델 구조의 균형성이며, Transformer모델을 의도에 따라 불균형하게 설정해, 총 00개의 모델을 구성하고, 이를 기계번역, 대화생성, 문서요약이라는 세가지 자연어 생성과제에서의 성능을 비교 분석해봅니다.
            </p>


          <div class="spacer"></div>
          <li>Hypothesis
            <div class="small-spacer"></div>
            <ul>
              <li style="font-weight: normal;">기계번역에서는 Balance가 유지되는 모델의 성능이 좋을 것이다</li>
              <li style="font-weight: normal;">대화생성에서는 디코더에 밸런스가 쏠린 모델 구조가 더 좋은 성능을 보일 것이다</li>
              <li style="font-weight: normal;">문서요약에서는 인코더에 밸러스가 더 쏠린 모델 구조가 좋은 성능을 보일 것이다</li>
            </ul>
          </li>


          <div class="spacer"></div>
          <li>Objective
            <div class="small-spacer"></div>
            <ul>
              <li style="font-weight: normal;">Transformer Width 불균형에 따른 자연어 생성 능력 변화 확인</li>
              <li style="font-weight: normal;">Transformer Depth 불균형에 따른 자연어 생성 능력 변화 확인</li>
              <li style="font-weight: normal;">Transformer Diversity 불균형에 따른 자연어 생성 능력 변화 확인</li>
              <li style="font-weight: normal;">자연어 생성 세부 과제별 가설 검증</li>
            </ul>
          </li>

        </ul>
      </div>
    </div>




<!-- 2. Background -->
    <h2 id="2">&hairsp; 2. &hairsp; Background</h2>
    <div class="chapter-box">

      <div class="list-container">  
        <ul>
          <li>Transformer Balance
            <div class="small-spacer"></div>
            <p>Standard Transformer는 균형이 잡힌 모델구조를 사용하고 있습니다. 이는 안정성 증진과 다양한 과제에서 범용적으로 사용될 수 있는 설정값이라는 장점을 지닙니다. 
              하지만, 모델의 구성에 변화를 줌으로써 특정 기능을 더욱 활성화 시킬수도 있습니다. 이런 ...는 Width, Depth, Diverse라고 할 수 있으며, 자세한 내용은 아래와 같습니다.
            </p>
            <div class="small-spacer"></div>

            <ul>              
              <div class="small-spacer"></div>
              <li><b>Width</b></li>
              <p>&nbsp; 기계 번역은 특정 언어로 표현된 시퀀스를 입력값으로 받아, 동일한 의미의 다른 언어 시퀀스를 반환하는 자연어 생성 과제를 의미합니다.
              가령 "I am a student"라는 표현을 "나는 학생입니다" 로 변환하는것.
              이 과제에서는 입력 시퀀스와 출력 시퀀스 간 거의 1:1 대응관계에 있다고 볼 수 있습니다.
              덕분에 인코딩과 디코딩의 비중이 균형적이라고 볼 수 있습니다.
              </p>              

              <div class="small-spacer"></div>
              <div class="small-spacer"></div>
              <li><b>Depth</b></li>
              <p>&nbsp; 대화 생성은 입력 시퀀스의 의미를 파악해, 적절한 답변을 출력하는 자연어 생성 과제입니다.
               이때 "Hi"라는 입력값에 대해, 다양한 출력 시퀀스가 모두 맞는 경우가 될 수 있으며, 
               1:1 대응이라기 보단, 자연스럽고 다양한 시퀀스 생성이 더욱 중요한 과제라고 볼 수 있습니다.

               때문에 인코딩과 디코딩 부분에서 디코딩이 더욱 중요하게 작용하는 과제라고 볼 수 있습니다.
               대표적으로 대화 생성에 특화된 GPT 계열 모델들역시 Decoder를 적극활용하고 있다는 점도 강력한 근거로 작용할 수 있습니다. 
              </p>

              <div class="small-spacer"></div>
              <div class="small-spacer"></div>              
              <li><b>Diverse</b></li>
              <p>&nbsp; 문서 요약은 긴 입력시퀀스에서 중요한 내용만을 파악하고, 이를 다시 새롭게 요약된 시퀀스로 반환하는 자연어 생성 과제입니다.
              입력 시퀀스의 길이가 출력 시퀀스에 비해 매우 길다는 특징이 있습니다.
              때문에 입력 시퀀스에서 파악해야하는 정보의 양이 매우 많습니다.
              자연스럽게 이는 인코딩과 디코딩의 비중중에서 인코딩의 비중이 매우 크다는 것을 시사합니다.
              실제 문서요약의 성능 개선을 위해, 인코더 방면에서의 연구들이 더 활발하게 이루어지고 있습니다.
              </p>              
            </ul>

          </li>


          <div class="spacer"></div>
          <li>Task Balance
            <ul>
              <div class="small-spacer"></div>
              <li><b>Machine Translation</b></li>
              <p>&nbsp; 기계 번역은 특정 언어로 표현된 시퀀스를 입력값으로 받아, 동일한 의미의 다른 언어 시퀀스를 반환하는 자연어 생성 과제를 의미합니다.
              가령 "I am a student"라는 표현을 "나는 학생입니다" 로 변환하는것.
              이 과제에서는 입력 시퀀스와 출력 시퀀스 간 거의 1:1 대응관계에 있다고 볼 수 있습니다.
              덕분에 인코딩과 디코딩의 비중이 균형적이라고 볼 수 있습니다.
              </p>              

              <div class="half-spacer"></div>
              <li><b>Dialogue Generation</b></li>
              <p>&nbsp; 대화 생성은 입력 시퀀스의 의미를 파악해, 적절한 답변을 출력하는 자연어 생성 과제입니다.
               이때 "Hi"라는 입력값에 대해, 다양한 출력 시퀀스가 모두 맞는 경우가 될 수 있으며, 
               1:1 대응이라기 보단, 자연스럽고 다양한 시퀀스 생성이 더욱 중요한 과제라고 볼 수 있습니다.

               때문에 인코딩과 디코딩 부분에서 디코딩이 더욱 중요하게 작용하는 과제라고 볼 수 있습니다.
               대표적으로 대화 생성에 특화된 GPT 계열 모델들역시 Decoder를 적극활용하고 있다는 점도 강력한 근거로 작용할 수 있습니다. 
              </p>

              <div class="half-spacer"></div>              
              <li><b>Text Summarization</b></li>
              <p>&nbsp; 문서 요약은 긴 입력시퀀스에서 중요한 내용만을 파악하고, 이를 다시 새롭게 요약된 시퀀스로 반환하는 자연어 생성 과제입니다.
              입력 시퀀스의 길이가 출력 시퀀스에 비해 매우 길다는 특징이 있습니다.
              때문에 입력 시퀀스에서 파악해야하는 정보의 양이 매우 많습니다.
              자연스럽게 이는 인코딩과 디코딩의 비중중에서 인코딩의 비중이 매우 크다는 것을 시사합니다.
              실제 문서요약의 성능 개선을 위해, 인코더 방면에서의 연구들이 더 활발하게 이루어지고 있습니다.
              </p>              
            </ul>
          </li>

        </ul>
      </div>  
    </div>


    <h2 id="3">&hairsp; 3. &hairsp; Architecture</h3>
    <div class="chapter-box">

      <div class="list-container">
        <ul>
          <li>Balanced Model
            <ul>
              <div class="small-spacer"></div>
              <li><b>Baseline Model</b></li>
              <p>일반적인 트랜스포머의 설정값과 같이, 인코더와 디코더의 하이퍼 파리미터의 균형을 일치시킨 모델입니다. Depth, Width, Diversity 모두 동일하며, 전체 실험의 Baseline 모델로 작용합니다.
              </p>

              <div class="small-spacer"></div>
              <div class="small-spacer"></div>
              <li><b>Large Model</b></li>
              <p>Baseline 모델과 같이, 인코더와 디코더의 Depth, Width, Diversity 모두 동일합니다. 하지만 Depth, Width, Diversity 관련 설정값이 모두 2배인 모델입니다. 이 모델은 균형성이 아닌, 모델 사이즈가 성능에 미치는 영향력을 파악하기 위해 사용합니다.
              </p>
             
            </ul>
          </li>



          <div class="spacer"></div>          
          <li>Width Unbalanced Model
          <p>Width 측면에서 밸런스를 의도적으로 붕괴시킨 모델입니다. Encoder의 무게중심을 더 크게 준 모델을 Encoder_Width Model이라고, 디코더에 무게중심을 더 준 모델을 Decoder Width_Model이라고 명명하고, Back-bone 모델에서 공유되는 Hidden Dimension차원을 2배만큼 증가 시킴으로써, 인코더 혹은 디코더의 은닉차원에서 함유할수 있는 정보의 양을 늘린 모델입니다. <br>
            차원의 크기. 모든 차원의 크기를 동일하게 늘리는 것이 아니라, Encoder에서 혹은 Decoder에서 공통적으로 적용되는 Hidden Dimension과 PFF 차원의 사이즈를 조절합니다. 이 차원의 크기가 커질수록 담을수 있는 정보의 양도 많아집니다.
                  
            차원의 크기를 조절하는 Encoder_Wide Model과 Decoder Wide Model을 사용하는 경우, Encoder의 차원과 Decoder에서 사용하는 차원이 달라 발생하는 문제를 Connection Layer를 둠으로써 해결합니다.
          </p>
            <ul>
              <div class="small-spacer"></div>
              <li><b>Encoder Wide Model</b></li>
              <p>일반적인 트랜스포머의 설정값과 같이, 인코더와 디코더의 하이퍼 파리미터의 균형을 일치시킨 모델입니다. Depth, Width, Diversity 모두 동일하며, 전체 실험의 Baseline 모델로 작용합니다.
              </p>

              <div class="small-spacer"></div>
              <div class="small-spacer"></div>
              <li><b>Decoder Wide Model</b></li>
              <p>Baseline 모델과 같이, 인코더와 디코더의 Depth, Width, Diversity 모두 동일합니다. 하지만 Depth, Width, Diversity 관련 설정값이 모두 2배인 모델입니다. 이 모델은 균형성이 아닌, 모델 사이즈가 성능에 미치는 영향력을 파악하기 위해 사용합니다.
              </p>
             
            </ul>

          </li>

          <div class="spacer"></div>
          <li>Depth Unbalanced Model</li>
          <p>        
            Transformer의 인코더와 디코더는 각각 복수개의 인코더 레이어와 디코더 레이어로 구성됩니다. 이때 레이어의 개수에 따라 레이어가 많을수록 더 많은 연산 과정 속에서 주어진 시퀀스에 대한 보다 깊은 이해능력이 배양됩니다.
            Width 측면에서 밸런스를 의도적으로 붕괴시킨 모델입니다. Encoder의 무게중심을 더 크게 준 모델을 Encoder_Width Model이라고, 디코더에 무게중심을 더 준 모델을 Decoder Width_Model이라고 명명하고, Back-bone 모델에서 공유되는 Hidden Dimension차원을 2배만큼 증가 시킴으로써, 인코더 혹은 디코더의 은닉차원에서 함유할수 있는 정보의 양을 늘린 모델입니다. 
          </p>      


          <div class="spacer"></div>
          <li>Diversity Unbalanced Model</li>
          <p>Transformer의 핵심 연산 부라고 할 수 있는 Multi Head Attention 구현부에서 Attention Head는 몇개의 쪼개진 조각으로 주어진 벡터를 파악할 것인지를 결정하는 계수입니다. 이 Attention Head의 개수가 많을 수록, 다양한 관점에서 다각적인 의미 해석 및 추출이 가능하게 됩니다. <br>
            Width 측면에서 밸런스를 의도적으로 붕괴시킨 모델입니다. Encoder의 무게중심을 더 크게 준 모델을 Encoder_Width Model이라고, 디코더에 무게중심을 더 준 모델을 Decoder Width_Model이라고 명명하고, Back-bone 모델에서 공유되는 Hidden Dimension차원을 2배만큼 증가 시킴으로써, 인코더 혹은 디코더의 은닉차원에서 함유할수 있는 정보의 양을 늘린 모델입니다.        
          </p>

          <div class="spacer"></div>
          <li>Comparison Table</li> 
          <div class="small-spacer"></div>     
          <table class="result-table">
            <thead>
              <tr>
                <th>Model Type</th>
                <th>Dimension</th>
                <th>Connect Layer</th>
                <th>Num of Layers</th>
                <th>Num of Attn Heads</th>
              </tr>
            </thead>

            <tbody>
              <tr>
                <td><b>Base</b></td>
                <td>Dimension</td>
                <td>No</td>
                <td>N Layers</td>
                <td>N Heads</td>
              </tr>

              <tr>
                <td><b>Wide</b></td>
                <td>Dimension * 2</td>
                <td>Yes</td>
                <td>N Layers</td>
                <td>N Heads</td>
              </tr>

              <tr>
                <td><b>Deep</b></td>
                <td>Dimension</td>
                <td>No</td>
                <td>N Layers * 2</td>
                <td>N Heads</td>
              </tr>

              <tr>
                <td><b>Diverse</b></td>
                <td>Dimension</td>
                <td>No</td>
                <td>N Layers</td>
                <td>N Heads * 2</td>
              </tr>

              <tr>
                <td><b>Large</b></td>
                <td>Dimension * 2</td>
                <td>No</td>
                <td>N Layers * 2</td>
                <td>N Heads * 2</td>
              </tr>

            </tbody>
          </table>
      </div>
    </div>



<!-- 4. Experimental Setup -->
    <h2 id="4">4. Experimental Setup</h2>
    <div class="chapter-box">

      <div class="list-container">
        <ul>
          <li>Data Setup
            <div class="dual-container">
              <ul>
                <li><b>Translation Task</b>: &nbsp; WMT'14 En-De</li>
                <li><b>Dialogue Task</b>: &nbsp; Daily Dialogue</li>
                <li><b>Summarization Task</b>: &nbsp; CNN Daily</li>
                <li><b>Tokenizer</b>: &nbsp; Word-Level Tokenizer</li>
              </ul>
              <ul>
                <li><b>Train Data Volumn</b>: &nbsp; 50,000</li>
                <li><b>Valid Data Volumn</b>: &nbsp; 5,000</li>
                <li><b>Test Data Volumn</b>: &nbsp; 100</li>
                <li><b>Vocab Size</b>: &nbsp; 10,000</li>                
              </ul>
            </div>
          </li>

          <div class="spacer"></div>
          <li>Model Setup
            <div class="dual-container">
              <ul>
                <li><b>Input Dim</b>: &nbsp; 10,000</li>
                <li><b>Output Dim</b>: &nbsp; 10,000</li>
                <li><b>Embedding Dim</b>: &nbsp; 512</li>
              </ul>
              <ul>
                <li><b>Hidden Dim</b>: &nbsp; 512</li>
                <li><b>Model Params</b>: &nbsp; 000</li>
                <li><b>Model Size</b>: &nbsp; 000</li>                
              </ul>
            </div>
          </li>

          <div class="spacer"></div>
          <li>Training Setup
            <div class="dual-container">
              <ul>
                <li><b>Num Epochs</b>: &nbsp; 10</li>
                <li><b>Batch Size</b>: &nbsp; 32</li>
                <li><b>Learning Rate</b>: &nbsp; 5e-4</li>
                <li><b>LR Scheduler</b>: &nbsp; ReduceLROnPlateau</li>
              </ul>
              <ul>
                <li><b>Optimizer</b>: &nbsp; AdamW</li>
                <li><b>Gradient Accumulation Steps</b>: &nbsp; 4
                <li><b>Teacher Forcing Ratio</b>: &nbsp; 0.5</li>                
              </ul>
            </div>
          </li>
        </ul>
      </div>
    </div>



<!-- Result --> 
    <h2 id="5">5. &hairsp; Result</h2>

    <div class="chapter-box">

      <div class="list-container">
        <ul>
          <li>Machine Translation</li>

            <table class="result-table">
              <thead>
                <tr>
                  <th>Model Type</th>
                  <th>Eval Score</th>
                  <th>Epoch Time</th>
                  <th>Avg GPU</th>
                  <th>Max GPU</th>
                </tr>
              </thead>
              <tbody>
                <tr>
                  <td>Baseline Model</td>
                  <td>13.20</td>
                  <td>0m 44s</td>
                  <td>0.20 GB</td>
                  <td>0.95 GB</td>
                </tr>
                <tr>
                  <td>Encoder-Wide Model</td>
                  <td>14.20</td>
                  <td>0m 48s</td>
                  <td>0.26GB</td>
                  <td>1.12GB</td>
                </tr>
                <tr>
                  <td>Decoder-Wide Model</td>
                  <td>14.09</td>
                  <td>0m 51s</td>
                  <td>0.29GB</td>
                  <td>1.20GB</td>
                </tr>
                <tr>
                  <td>Encoder-Deep Model</td>
                  <td>12.61</td>
                  <td>0m 53s</td>
                  <td>0.22GB</td>
                  <td>1.06GB</td>
                </tr>
                <tr>
                  <td>Decoder-Deep Model</td>
                  <td>11.47</td>
                  <td>0m 59s</td>
                  <td>0.23GB</td>
                  <td>1.12GB</td>
                </tr>
                <tr>
                  <td>Encoder-Diverse Model</td>
                  <td>13.23</td>
                  <td>0m 40s</td>
                  <td>0.20GB</td>
                  <td>0.95GB</td>
                </tr>                
                <tr>
                  <td>Decoder-Diverse Model</td>
                  <td>12.96</td>
                  <td>0m 41s</td>
                  <td>0.20GB</td>
                  <td>0.95GB</td>
                </tr>
                <tr>
                  <td>Large Model</td>
                  <td>14.49</td>
                  <td>0m 41s</td>
                  <td>0.20GB</td>
                  <td>0.95GB</td>
                </tr>                

              </tbody>
            </table>

          <div class="spacer"></div>
          <li>Dialogue Generation</li>

            <table class="result-table">
              <thead>
                <tr>
                  <th>Model Type</th>
                  <th>Eval Score</th>
                  <th>Epoch Time</th>
                  <th>Avg GPU</th>
                  <th>Max GPU</th>
                </tr>
              </thead>
              <tbody>
                <tr>
                  <td>Baseline Model</td>
                  <td>2.84</td>
                  <td>0m 40s</td>
                  <td>0.20 GB</td>
                  <td>0.85 GB</td>
                </tr>
                <tr>
                  <td>Encoder-Wide Model</td>
                  <td>0.74</td>
                  <td>0m 44s</td>
                  <td>0.26GB</td>
                  <td>1.00GB</td>
                </tr>
                <tr>
                  <td>Decoder-Wide Model</td>
                  <td>1.91</td>
                  <td>0m 46s</td>
                  <td>0.29GB</td>
                  <td>1.07GB</td>
                </tr>
                <tr>
                  <td>Encoder-Deep Model</td>
                  <td>0.46</td>
                  <td>0m 51s</td>
                  <td>0.22GB</td>
                  <td>0.95GB</td>
                </tr>
                <tr>
                  <td>Decoder-Deep Model</td>
                  <td>0.00</td>
                  <td>1m 0s</td>
                  <td>0.23GB</td>
                  <td>1.00GB</td>
                </tr>
                <tr>
                  <td>Encoder-Diverse Model</td>
                  <td>2.72</td>
                  <td>0m 41s</td>
                  <td>0.20GB</td>
                  <td>0.85GB</td>
                </tr>                
                <tr>
                  <td>Decoder-Diverse Model</td>
                  <td>2.86</td>
                  <td>0m 42s</td>
                  <td>0.20GB</td>
                  <td>0.85GB</td>
                </tr>

              </tbody>
            </table>

          <div class="spacer"></div>
          <li>Text Summarization</li>

            <table class="result-table">
              <thead>
                <tr>
                  <th>Model Type</th>
                  <th>Eval Score</th>
                  <th>Epoch Time</th>
                  <th>Avg GPU</th>
                  <th>Max GPU</th>
                </tr>
              </thead>
              <tbody>
                <tr>
                  <td>Baseline Model</td>
                  <td>8.53</td>
                  <td>2m 40s</td>
                  <td>0.21 GB</td>
                  <td>2.90 GB</td>
                </tr>
                <tr>
                  <td>Encoder-Wide Model</td>
                  <td>0.70</td>
                  <td>4m 3s</td>
                  <td>0.27GB</td>
                  <td>3.65GB</td>
                </tr>
                <tr>
                  <td>Decoder-Wide Model</td>
                  <td>5.32</td>
                  <td>3m 25s</td>
                  <td>0.30GB</td>
                  <td>3.14GB</td>
                </tr>
                <tr>
                  <td>Encoder-Deep Model</td>
                  <td>0.02</td>
                  <td>4m 27s</td>
                  <td>0.23GB</td>
                  <td>4.49GB</td>
                </tr>
                <tr>
                  <td>Decoder-Deep Model</td>
                  <td>0.00</td>
                  <td>3m 29s</td>
                  <td>0.24GB</td>
                  <td>3.33GB</td>
                </tr>
                <tr>
                  <td>Encoder-Diverse Model</td>
                  <td>6.60</td>
                  <td>2m 46s</td>
                  <td>0.21GB</td>
                  <td>2.90GB</td>
                </tr>                
                <tr>
                  <td>Decoder-Diverse Model</td>
                  <td>8.20</td>
                  <td>2m 48s</td>
                  <td>0.21GB</td>
                  <td>2.90GB</td>
                </tr>

              </tbody>
            </table>

          <div class="spacer"></div>
          <li>Result Analyis
            <ul>
              <div class="small-spacer"></div>
              <li>Machine Translation 괴제에서 Deocder-Wide 모델이 가장 큰 성능 향상을 견인했으며, Diversity를 증가시키는 것 역시 긍정적인 효과를 보임을 확인할 수 있습니다. Deocder-Width가 가장 큰 영향을 미친 것은, Translation 이라는 과제의 특성이라기 보다는 인코딩보다 디코딩의 난이도가 높은 자연어 생성이라는 상위 개념의 특성에서 기인한 결과로 판단됩니다.</li>

              <div class="small-spacer"></div>
              <li>Dialogue Generation에서는 Decoder-Diverse Model을 제외한 모든 모델이 기준 모델보다 저조한 성능을 기록했습니다. 대화 생성이라는 과제의 난이도가 높아, 모델의 수렴이 어려운 상황에서 안정적이지 않은 모델 구조에서 기인한 부정적인 영향이 부정적인 요소로 작용한 것으로 판단됩니다.</li>
              
              <div class="small-spacer"></div>
              <li>Text Summarization에서는 모든 변인 모델들이 기준 모델 성능에 미치지 못함을 확인했습니다. 인코딩에 집중한 모델들이 좋은 성능을 보일 것이라는 예측과 달리, Wide 모델에서 조차 오히려 Decoder-Wide 모델이 Encoder-Wide 모델보다 좋은 성능을 보입니다. 이 역시 자연어 생성에서 시퀀스를 해석하는 것보다 생성하는 것의 난이도가 높아 발생한 현상으로 판단됩니다.</li>
              
              <div class="small-spacer"></div>
              <li>전체적으로 Diversity를 증가 시키는 것이 소폭이나마 긍정적으로 작용할 여지가 있음을 확인했으며, 세부 Task의 성격에 따라 인코딩-디코딩의 중요도가 나뉘기 보다는, 자연어 생성 자체에서 인코딩보다는 디코딩이 어려우며, 때문에 전반적으로 디코더를 강조한 모델에서 조금 더 나은 성능이 보이는 경향성이 유지되는 것으로 보입니다.</li>
            </ul>
          </li>


        </ul>
      </div>
    </div>


    <h2 id="6">6. &hairsp; Conclusion</h2>

    <div class="chapter-box">

      <div class="list-container">
        <ul>
          <li>Machine Translation</li>
          <li>Machine Translation</li>
          <li>Machine Translation</li>
        </ul>
      </div>
    </div>


    <h2 id="7" class="h3-mt">7. &hairsp; Reference</h2>
    <div class="chapter-box">
      <a class="reference" href="https://arxiv.org/abs/1706.03762">&nbsp; Attention Is All You Need</a> 
    </div>

  </div>
</div>


<div class="pagination">
  <a href="{{ '/transformer' | relative_url }}" class="btn-prev"><span>Transformer</span></a>
  <a href="{{ '/transformer_variants' | relative_url }}" class="btn-next"><span>Transformer Variants</span></a>
</div>