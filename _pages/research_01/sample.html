---
layout: default
permalink: /sample
---


<div id="detail">
  <h1 class="title">Seq2Seq Application Methods of Pretrained Language Encoder</h1>
    
  <div class="post">




    <hr>
    <div class="callout">
      <p class="highlighted-text"><span>Project Objective</span></p>
      <div class="small-spacer"></div>
      <p>&nbsp; 이 프로젝트에서는 사전학습된 언어 인코더 모델을 Seq2Seq 아키텍처에서 활용하기 위한 세 가지 방법론을 구현하고, 각 방법론의 성능을 세 가지 자연어 생성 과제에서 비교합니다. 변인이 되는 세 가지 모델구조는 각각 Simple, Fused, Full Model이라고 명명하고 있으며, 각 방법론의 성능은 ~~하게 나왔습니다. 이를 통해 ~~하다는 사실을 확인할 수 있었습니다.
      실험에 대한 자세한 코드 구현은 "~~"에서 확인 할 수 있습니다.
      </p>
    </div>

    <div class="tblContents">
      <ul>

        <li>
          <a href="#1">1. &hairsp; Introduction</a>
        </li>

        <li>
          <a href="#2">2. &hairsp; Background</a>
        </li>

        <li>
          <a href="#3">3. &hairsp; Code Implementation</a>
        </li>

        <li>
          <a href="#4">4. &hairsp; Experimental Setup</a>
        </li>

        <li>
          <a href="#5">5. &hairsp; Results</a>
        </li>

        <li>
          <a href="#6">6. &hairsp; Conclusion</a>
        </li>

        <li>
          <a href="#7">7. &hairsp; Reference</a>
        </li>

      </ul>
    </div>
    <hr>


<!-- 1. Introduction -->    
    <h2 id="1">&hairsp; 1. &hairsp; Introduction</h2>
    <div class="chapter-box">

      <div class="list-container">
        <ul>
          <li>Description</li>
            <p> &nbsp; 사전학습 모델들은 다양한 자연어 처리 과제에서 좋은 성능을 보입니다. 처리하고자 하는 과제의 목적성에 맞게 사전학습 모델의 구조 역시 다양합니다.
            BERT처럼 Encoding을 중시하는 모델구조, GPT처럼 Decoding을 중시하는 구조, 그리고 BART 처럼 Encoder-Decoder로 구성된 모델등.
            그 중에서도 BERT 계열의 모델들이 가장 많음. 
            때문에 Seq2Seq 모델에서 단순히 사전학습된 Seq2Seq 모델을 사용하는 것을 넘어, 이미 잘 구축되어 있는 Pretrained Language Encoder를 Seq2Seq에서 활용하기 위한 세 가지 방법론을 직접 구현하고,
            자연어 생성 문제에서 비교 분석해봅니다.   
            </p>

          <div class="half-spacer"></div>
          <li>Objective
            <ul>
              <li style="font-weight: normal;">사전학습 인코더 모델을 생성과제에서 사용하기 위한 지식 습득</li>
              <li style="font-weight: normal;">세 가지 방법론의 성능 비교 검증</li>
              <li style="font-weight: normal;">실험 결과를 통해, 이후 자연어 생성을 위한 모델링 다양성 확보</li>
            </ul>
          </li>

        </ul>
      </div>

    </div>



<!-- 2. Background -->    
    <h2 id="2">&hairsp; 2. &hairsp; Background</h2>
    <div class="chapter-box">

      <div class="list-container">
        <ul>
          <li>PLE Encoder Model</li>
            <div class="dual-container">
              <img src="{{ 'assets/img/research_01/standard_transformer.png' | relative_url }}" style="width: 27%; height: 25%;">
              <p>&nbsp; Transformer Seq2Seq 모델 구조에서, Encoder를 Pretrained Language Encoder Model로 대체한 모델입니다. 연산은 기존 Transformer Seq2Seq의 연산 과정과 동일하게 적용됩니다. Encoder는 Context Vector를 생성하고, Decoder는 Context Vector와 Teacher Forcing을 활용해 예측값을 생성하는 방식으로 학습이 진행됩니다..
              </p>
            </div>      

          <div class="spacer"></div>
          <li>PLE Fusion Model</li>
          <div class="dual-container">
            <img src="{{ 'assets/img/research_01/fusion.png' | relative_url }}" style="width: 40%; height: 40%;">
            <p>&nbsp; PLE Fused Model은 "Incorporating BERT into Neural Machine Translation" 논문에서 제시한 모델 구조를 사용합니다. 사전학습된 인코더 모델을 그대로 인코더로만 사용하는것이 아니라, 인코딩과 디코딩 과정에 추가적인 정보전달을 위한 모델로 활용합니다.
            </p>
          </div>

          <div class="spacer"></div>
          <li>PLE Encoder Decoder Model</li>          
          <p>&nbsp; PLE Encoder Decoder Model은 Transformer Encoder & Decoder 모두에 Pretrained Language Encoder Model의 학습된 가중치를 활용하는 방법입니다.
            구현에서는 Hugging Face의 EncoderDecoder Model을 활용합니다.
          </p>
          
        </ul>
      </div>
    </div>



<!-- Code Implementation -->    
    <h2 id="3">&hairsp; 3. &hairsp; Code Implementation</h2>
    <div class="chapter-box">

      <div class="list-container">
        <ul>
          <li>PLE Encoder</li>

          <div class="code-container">
            <div class="code-snippet">
              <pre><code class="python">def generate_signal(length, channels, min_timescale=1.0, max_timescale=1.0e4):
        position = np.arange(length)
        num_timescales = channels // 2
        log_timescale_increment = ( math.log(float(max_timescale) / float(min_timescale)) / (float(num_timescales) - 1))
        inv_timescales = min_timescale * np.exp(np.arange(num_timescales).astype(np.float) * -log_timescale_increment)
        scaled_time = np.expand_dims(position, 1) * np.expand_dims(inv_timescales, 0)

        signal = np.concatenate([np.sin(scaled_time), np.cos(scaled_time)], axis=1)
        signal = np.pad(signal, [[0, 0], [0, channels % 2]], 'constant', constant_values=[0.0, 0.0])
        signal =  signal.reshape([1, length, channels])

        return torch.from_numpy(signal).type(torch.FloatTensor)</code></pre>
            </div>
          </div>

          <div class="spacer"></div>
          <li>PLE Fusion</li>

          <div class="code-container">
            <div class="code-snippet">
              <pre><code class="python">def generate_signal(length, channels, min_timescale=1.0, max_timescale=1.0e4):
        position = np.arange(length)
        num_timescales = channels // 2
        log_timescale_increment = ( math.log(float(max_timescale) / float(min_timescale)) / (float(num_timescales) - 1))
        inv_timescales = min_timescale * np.exp(np.arange(num_timescales).astype(np.float) * -log_timescale_increment)
        scaled_time = np.expand_dims(position, 1) * np.expand_dims(inv_timescales, 0)

        signal = np.concatenate([np.sin(scaled_time), np.cos(scaled_time)], axis=1)
        signal = np.pad(signal, [[0, 0], [0, channels % 2]], 'constant', constant_values=[0.0, 0.0])
        signal =  signal.reshape([1, length, channels])

        return torch.from_numpy(signal).type(torch.FloatTensor)</code></pre>
            </div>
          </div>


          <div class="spacer"></div>
          <li>PLE Encoder Decoder</li>

          <div class="code-container">
            <div class="code-snippet">
              <pre><code class="python">def generate_signal(length, channels, min_timescale=1.0, max_timescale=1.0e4):
        position = np.arange(length)
        num_timescales = channels // 2
        log_timescale_increment = ( math.log(float(max_timescale) / float(min_timescale)) / (float(num_timescales) - 1))
        inv_timescales = min_timescale * np.exp(np.arange(num_timescales).astype(np.float) * -log_timescale_increment)
        scaled_time = np.expand_dims(position, 1) * np.expand_dims(inv_timescales, 0)

        signal = np.concatenate([np.sin(scaled_time), np.cos(scaled_time)], axis=1)
        signal = np.pad(signal, [[0, 0], [0, channels % 2]], 'constant', constant_values=[0.0, 0.0])
        signal =  signal.reshape([1, length, channels])

        return torch.from_numpy(signal).type(torch.FloatTensor)</code></pre>
            </div>
          </div>


        </ul>          
      </div>    
    </div>



<!-- 3. Experimental Setup -->
    <h2 id="3">&hairsp; 3. &hairsp; Experimental Setup</h2>
    <div class="chapter-box">

      <div class="list-container">
        <ul>
          <li>Data Setup
            <div class="dual-container">
              <ul>
                <li>Translation Task: &nbsp; WMT'14 En-De</li>
                <li>Dialogue Task: &nbsp; Daily Dialogue</li>
                <li>Summarization Task: &nbsp; CNN Daily</li>
                <li>Tokenizer: &nbsp; AlBERT Tokenizer</li>
              </ul>
              <ul>
                <li>Train Data Volumn: &nbsp; 50,000</li>
                <li>Valid Data Volumn: &nbsp; 5,000</li>
                <li>Test Data Volumn: &nbsp; 100</li>
                <li>Vocab Size: &nbsp; 10,000</li>                
              </ul>
            </div>
          </li>

          <div class="half-spacer"></div>
          <li>Model Setup
            <div class="dual-container">
              <ul>
                <li>PLE Architecture: &nbsp; AlBERT</li>
                <li>PLE Name: &nbsp; albert-v2</li>
                <li>Input Dim: &nbsp; 10,000</li>
                <li>Output Dim: &nbsp; 10,000</li>
              </ul>
              <ul>
                <li>Embedding Dim: &nbsp; 512</li>
                <li>Hidden Dim: &nbsp; 512</li>
                <li>Model Params: &nbsp; 000</li>
                <li>Model Size: &nbsp; 000</li>                
              </ul>
            </div>
          </li>

          <div class="half-spacer"></div>
          <li>Training Setup
            <div class="dual-container">
              <ul>
                <li>Num Epochs: &nbsp; 10</li>
                <li>Batch Size: &nbsp; 32</li>
                <li>Learning Rate: &nbsp; 5e-4</li>
                <li>LR Scheduler: &nbsp; pleature</li>
              </ul>
              <ul>
                <li>Optimizer: &nbsp; AdamW</li>
                <li>Gradient Accumulation Steps: &nbsp; 4
                <li>Teacher Forcing Ratio: &nbsp; 0.5</li>                
              </ul>
            </div>
          </li>
        </ul>
      </div>
    </div>


<!-- 4. Result -->
    <h2 id="4">&hairsp; 4. &hairsp; Result</h2>
    <div class="chapter-box">

      <div class="list-container">
        <ul>
          <li>Machine Translation</li>

            <table class="result-table">
              <thead>
                <tr>
                  <th>Model Type</th>
                  <th>Eval Score</th>
                  <th>Epoch Time</th>
                  <th>Avg GPU</th>
                  <th>Max GPU</th>
                </tr>
              </thead>
              <tbody>
                <tr>
                  <td>RNN Model</td>
                  <td>2.12</td>
                  <td>3m 58s</td>
                  <td>0.22GB</td>
                  <td>0.82GB</td>
                </tr>
                <tr>
                  <td>LSTM Model</td>
                  <td>8.35</td>
                  <td>4m 16s</td>
                  <td>0.27GB</td>
                  <td>1.30GB</td>
                </tr>
                <tr>
                  <td>GRU Model</td>
                  <td>9.75</td>
                  <td>4m 6s</td>
                  <td>0.26GB</td>
                  <td>1.16GB</td>
                </tr>

              </tbody>
            </table>
          
          <div class="half-spacer"></div>
          <li>Dialogue Generation</li>
            <table class="result-table">
              <thead>
                <tr>
                  <th>Model Type</th>
                  <th>Eval Score</th>
                  <th>Epoch Time</th>
                  <th>Avg GPU</th>
                  <th>Max GPU</th>
                </tr>
              </thead>
              <tbody>
                <tr>
                  <td>RNN Model</td>
                  <td>0.10</td>
                  <td>3m 40s</td>
                  <td>0.22GB</td>
                  <td>0.79GB</td>
                </tr>
                <tr>
                  <td>LSTM Model</td>
                  <td>0.37</td>
                  <td>4m 5s</td>
                  <td>0.27GB</td>
                  <td>1.23GB</td>
                </tr>
                <tr>
                  <td>GRU Model</td>
                  <td>2.16</td>
                  <td>3m 58s</td>
                  <td>0.26GB</td>
                  <td>1.11GB</td>
                </tr>

              </tbody>
            </table>

                
          <div class="half-spacer"></div>
          <li>Text Summarization</li>
            <table class="result-table">
              <thead>
                <tr>
                  <th>Model Type</th>
                  <th>Eval Score</th>
                  <th>Epoch Time</th>
                  <th>Avg GPU</th>
                  <th>Max GPU</th>
                </tr>
              </thead>
              <tbody>
                <tr>
                  <td>RNN Model</td>
                  <td>0.00</td>
                  <td>8m 6s</td>
                  <td>0.23GB</td>
                  <td>1.20GB</td>
                </tr>
                <tr>
                  <td>LSTM Model</td>
                  <td>2.23</td>
                  <td>9m 7s</td>
                  <td>0.29GB</td>
                  <td>2.00GB</td>
                </tr>
                <tr>
                  <td>GRU Model</td>
                  <td>2.19</td>
                  <td>8m 42s</td>
                  <td>0.27GB</td>
                  <td>1.82GB</td>
                </tr>

              </tbody>
            </table>

          <div class="half-spacer"></div>
          <li>Result Analysis</li>
            <p> &nbsp; RNN은 다양한 정보를 함유하기에 지나치게 단순하고, LSTM은 GATE구조가 복잡하기에, 그만큼 영향을 받은 요소가 많아 학습 수렴이 어렵습니다. 때문에 모든 학습 결과에서는 GATE를 사용하면서도 단순화된 연산으로 학습의 이점이 큰 GRU가 가장 뛰어난 성능을 보임을 알 수 있습니다. 
            </p>
        </ul>
      </div>
    </div>



<!-- 5. Conclusion -->
    <h2 id="5">&hairsp; 5. &hairsp; Conclusion</h2>
    <div class="chapter-box">

      <div class="list-container">
        <ul>
          <li>RNN, LSTM, GRU 을 활용한 seq2seq 모델링 방식의 이해</li>
            <p>&nbsp; 토큰의 벡터 변환을 위한 Embedding Layer와 실제 연산을 위한 순환 신경망 레이어만으로 구성된 단순산 Encoder, Decoder 구조로 이루어져 있으며, 
            </p>

          <div class="half-spacer"></div>
          <li>세 가지 순환 신경망의 성능 비교 검증</li>
            <p>LSTM은 게이트 메커니즘을 통해 장기 의존성 문제를 해결하는 데 강점을 보입니다. 복잡한 문맥을 학습하고 기억하는 데 탁월하며, 긴 시퀀스 데이터에서 뛰어난 성능을 발휘할 수 있습니다. 하지만 LSTM은 더 많은 파라미터를 가지고 있어서 더 많은 데이터와 계산 리소스가 필요할 수 있습니다.
            </p>

          <div class="half-spacer"></div>
          <li>가설 검증
            <ul>
              <li>LSTM, GRU, RNN 순으로 높은 성능</li>
                <p> LSTM의 연산과정이 가장 복잡하고, 다양한 GATE를 사용하기 떄문에, 좋은 성능을 낼 것이라고 예측했으나, 되려 복잡한 연산으로 인해, 학습 시 수렴에 어려움이 생겼습니다.
                오히려 GATE를 사용하면서도, 연산의 단순화를 도모한 GRU의 학습 성능이 가장 좋은 것을 확인했습니다
                </p>

              <li>RNN, GRU, LSTM 순으로 빠른 학습</li>
                <p> 학습 속도 및 GPU 사용량으로 확인해본 효율성은 가설과 마찬가지로 RNN, GRU, LSTM 순으로 나타났습니다.
                </p>
              <li></li>
            </ul>
          </li>
            
          <div class="half-spacer"></div>
          <li>최소 기준치 확립</li>
            <p>앞서 언급했듯이 순환 신경망을 사용한 Seq2Seq모델은 앞으로 다루게 될 다양한 Seq2Seq 모델들의 시작점입니다. 고도화 된 모델일지라도 하이퍼 파라미터 설정이나, 훈련 방식 등의 이유에서 잘못된 성능이 나올수도 있습니다. 이때, 최소한의 동작은 하는 구나 하는 판단의 근거로 다음과 같은 스코어를 활용할 수 있습니다. <br> 
              기계번역 BLEU Score: 00.00, 대화 생성 Rouge Score: 00.00, 문서 요약 Rouge Score: 00.00 이하의 성능ㅇ
            </p>

        </ul>
      </div>
    </div>



<!-- Reference -->    
    <h2 id="6">&hairsp; 6. Reference</h2>
    <div class="chapter-box">
      <a class="reference" href="https://arxiv.org/abs/1706.03762">&nbsp; Attention Is All You Need</a> <br> 
      <a class="reference" href="https://arxiv.org/abs/2002.06823">&nbsp; Incorporating BERT into Neural Machine Translation</a> <br> 
      <a class="reference" href="https://arxiv.org/abs/1907.12461">&nbsp; Leveraging Pre-trained Checkpoints for Sequence Generation Tasks</a> <br> 
      <a class="reference" href="https://huggingface.co/docs/transformers/model_doc/encoder-decoder">&nbsp; HuggingFace EncoderDecoder Model</a>
    </div>

  </div>
</div>


<div class="pagination">
  <a href="{{ '/balance' | relative_url }}" class="btn-prev"><span>Transformer Balance</span></a>
  <a href="{{ '/eff_train' | relative_url }}" class="btn-next"><span>Efficient Training</span></a>
</div>