---
layout: default
permalink: /transformer
---


<div id="detail">
  <h1 class="title">Transformer Implementation and Comparative Analytics</h1>  
  <div class="post">
    <h2>1. &hairsp; Project Desc</h2>

      <div class="small-spacer"></div>
      <p>
        &nbsp; 앞선 프로젝트에서는 RNN을 기반으로한 Sequence to Sequence 모델 구조의 효용성에 대해 면밀히 살펴봤습니다. 
        하지만 RNN 네트워크는 장기기억력이라는 관점에서 고질적인 문제점을 태생적으로 지니고 있으며, 이를 해결하기 위한 Attention 메커니즘의 활용 역시 기대만큼 좋은 성능을 보이지는 못했습니다.
        Transformer는 어텐션 메커니즘 만으로 문맥 전체적인 내용을 포착하는 뛰어난 성능과 더불어 병렬처리로 인한 학습의 효율성까지 제고시킨 뛰어난 모델 구조입니다.
        이 글에서는 Transformer의 구조에 대한 설명과 더불어, 두가지 방식에서의 Code Implementation 방식을 알아봅니다. 또한 이렇게 구현한 Transformer 모델이 세가지 자연어 생성 과제에서 어떤 성능을 보이는지 비교해봅니다.
      </p>

    <h2>2. &hairsp; Implementation</h2>

    <div class="half-spacer"></div>
    <h3>Transformer</h3>
    <div class="small-spacer"></div>
    <p>Transformer Model은 임베딩과, 인코더, 디코더 그리고 Generator로 구성됩니다. x데이터는 임베딩과 인코더를 거쳐 Memory Vector로 변환되고, 디코더는 y데이터와 메모리 벡터를 사용해 최종 Hidden Vector를 생성합니다.
      그리고 마지막 선형 변환 레이어를 통해 Vocab Size만큼의 Token으로 시퀀스를 변환해서 최종 결과값을 반환합니다.
    </p>
    <div class="small-spacer"></div>

    <div class="code-container">
      <div class="code-snippet">
        <pre><code class="python">
class ScratchModel(nn.Module):

    def __init__(self, config):
        super(ScratchModel, self).__init__()

        self.pad_id = config.pad_id
        self.device = config.device
        self.vocab_size = config.vocab_size

        self.encoder = Encoder(config)
        self.decoder = Decoder(config)
        self.generator = nn.Linear(config.hidden_dim, self.vocab_size)

        self.criterion = nn.CrossEntropyLoss()
        self.out = namedtuple('Out', 'logit loss')


    def pad_mask(self, x):
        return (x != self.pad_id).unsqueeze(1).unsqueeze(2)


    def dec_mask(self, x):
        sz = x.size(1)
        pad_mask = self.pad_mask(x)
        sub_mask = torch.tril(torch.ones((sz, sz), device=self.device)).bool()
        return pad_mask & sub_mask


    @staticmethod
    def shift_y(x):
        return x[:, :-1], x[:, 1:]


    def forward(self, x, y):
        y, label = self.shift_y(y)

        e_mask = self.pad_mask(x) 
        d_mask = self.dec_mask(y)

        memory = self.encoder(x, e_mask)
        dec_out = self.decoder(y, memory, e_mask, d_mask)
        logit = self.generator(dec_out)

        #Getting Outputs
        self.out.logit = logit
        self.out.loss = self.criterion(
            logit.contiguous().view(-1, self.vocab_size), 
            label.contiguous().view(-1)
        )
        
        return self.out
        </code></pre>
      </div>
    </div>
    <hr>

    <div class="spacer"></div>
    <h3>Encoder</h3>
    <div class="small-spacer"></div>
    <p>Transformer Model은 임베딩과, 인코더, 디코더 그리고 Generator로 구성됩니다. x데이터는 임베딩과 인코더를 거쳐 Memory Vector로 변환되고, 디코더는 y데이터와 메모리 벡터를 사용해 최종 Hidden Vector를 생성합니다.
      그리고 마지막 선형 변환 레이어를 통해 Vocab Size만큼의 Token으로 시퀀스를 변환해서 최종 결과값을 반환합니다.
    </p>
    <div class="small-spacer"></div>

    <div class="code-container">
      <div class="code-snippet">
        <pre><code class="python">
class EncoderLayer(nn.Module):

    def __init__(self, config):
        super().__init__()

        self.self_attn = MultiHeadAttention(config)
        self.pff = PositionwiseFeedForward(config)

        hidden_dim = config.hidden_dim
        self.self_attn_norm = nn.LayerNorm(hidden_dim)
        self.pff_norm = nn.LayerNorm(hidden_dim)
        
        self.dropout = nn.Dropout(config.dropout_ratio)


    def forward(self, x, e_mask):
        _x = self.self_attn(x, x, x, e_mask)
        x = self.self_attn_norm(x + self.dropout(_x))
        _x = self.pff(x)
        x = self.pff_norm(x + self.dropout(_x))

        return x



class Encoder(nn.Module):

    def __init__(self, config):
        super().__init__()

        self.device = config.device
        self.emb = Embeddings(config)
        self.dropout = nn.Dropout(config.dropout_ratio)

        self.layers = nn.ModuleList(
            [EncoderLayer(config) for _ in range(config.n_layers)]
        )


    def forward(self, x, e_mask):
        x = self.emb(x)
        for layer in self.layers:
            x = layer(x, e_mask)
        return x

        </code></pre>
      </div>
    </div>
    <hr>


    <div class="spacer"></div>
    <h3>Decoder</h3>
    <div class="small-spacer"></div>
    <p>디코더는 디코더 레이어를 중첩시킨 구조로 이루어져 있습니다. 각각의 디코더 레이어에서는 
    </p>
    <div class="half-spacer"></div>

    <div class="code-container">
      <div class="code-snippet">
        <pre><code class="python">
class DecoderLayer(nn.Module):

    def __init__(self, config):
        super().__init__()

        self.self_attn = MultiHeadAttention(config)
        self.enc_attn = MultiHeadAttention(config)
        self.pff = PositionwiseFeedForward(config)

        hidden_dim = config.hidden_dim
        self.self_attn_norm = nn.LayerNorm(hidden_dim)
        self.enc_attn_norm = nn.LayerNorm(hidden_dim)
        self.pff_norm = nn.LayerNorm(hidden_dim)

        self.dropout = nn.Dropout(config.dropout_ratio)


    def forward(self, x, m, e_mask, d_mask):

        _x = self.self_attn(x, x, x, d_mask)
        x = self.self_attn_norm(x + self.dropout(_x))

        _x = self.enc_attn(x, m, m, e_mask)
        x = self.enc_attn_norm(x + self.dropout(_x))

        _x = self.pff(x)
        x = self.pff_norm(x + self.dropout(_x))

        return x



class Decoder(nn.Module):
    def __init__(self, config):
        super().__init__()

        self.device = config.device
        self.emb = Embeddings(config)
        self.dropout = nn.Dropout(config.dropout_ratio)

        self.layers = nn.ModuleList(
            [DecoderLayer(config) for _ in range(config.n_layers)]
        )


    def forward(self, x, memory, e_mask, d_mask):
        x = self.emb(x)
        for layer in self.layers:
            x = layer(x, memory, e_mask, d_mask)
        return x

        </code></pre>
      </div>
    </div>
    <hr>


    <div class="spacer"></div>
    <h3>Embedding</h3>
    <div class="small-spacer"></div>
    <p>디코더는 디코더 레이어를 중첩시킨 구조로 이루어져 있습니다. 각각의 디코더 레이어에서는 
    </p>
    <div class="half-spacer"></div>

    <div class="code-container">
      <div class="code-snippet">
        <pre><code class="python">
class PositionalEncoding(nn.Module):
    def __init__(self, config):
        super(PositionalEncoding, self).__init__()
        
        max_len = config.max_len if config.task != 'summarization' else config.max_len * 4
        pe = torch.zeros(max_len, config.emb_dim)
        
        position = torch.arange(0, max_len).unsqueeze(1)
        div_term = torch.exp(
            torch.arange(0, config.emb_dim, 2) * -(math.log(10000.0) / config.emb_dim)
        )
        
        pe[:, 0::2] = torch.sin(position * div_term)
        pe[:, 1::2] = torch.cos(position * div_term)
        pe = pe.unsqueeze(0)

        self.register_buffer('pe', pe)
        

    def forward(self, x):
        return x + self.pe[:, :x.size(1)]



class Embeddings(nn.Module):
    def __init__(self, config):
        super(Embeddings, self).__init__()

        self.tok_emb = nn.Embedding(config.vocab_size, config.emb_dim)
        self.scale = math.sqrt(config.emb_dim)

        self.pos_emb = PositionalEncoding(config)
        self.pos_dropout = nn.Dropout(config.dropout_ratio)

        self.use_fc_layer = (config.emb_dim != config.hidden_dim)
        if self.use_fc_layer:
            self.fc = nn.Linear(config.emb_dim, config.hidden_dim)
            self.fc_dropout = nn.Dropout(config.dropout_ratio)


    def forward(self, x):
        out = self.tok_emb(x) * self.scale
        out = self.pos_dropout(self.pos_emb(out))

        if not self.use_fc_layer:
            return out
        return self.fc_dropout(self.fc(out))

        </code></pre>
      </div>
    </div>
    <hr>


    <div class="spacer"></div>
    <h3>Multi Head Attention</h3>
    <div class="small-spacer"></div>
    <p>디코더는 디코더 레이어를 중첩시킨 구조로 이루어져 있습니다. 각각의 디코더 레이어에서는 
    </p>
    <div class="half-spacer"></div>

    <div class="code-container">
      <div class="code-snippet">
        <pre><code class="python">
class MultiHeadAttention(nn.Module):
    def __init__(self, config):
        super().__init__()

        hidden_dim = config.hidden_dim
        self.n_heads = config.n_heads

        assert hidden_dim // self.n_heads
        self.head_dim = hidden_dim // self.n_heads
        
        self.dropout = nn.Dropout(config.dropout_ratio)
        self.linears = clones(nn.Linear(hidden_dim, hidden_dim), 4)
        self.scale = torch.sqrt(torch.FloatTensor([self.head_dim])).to(config.device)


    def forward(self, query, key, value, mask = None):

        orig_shape = list(query.shape)
        split_shape = [query.size(0), -1, self.n_heads, self.head_dim]

        Q, K, V = [lin(x).view(split_shape).transpose(1, 2) \
                   for lin, x in zip(self.linears, (query, key, value))]   

        score = torch.matmul(Q, K.permute(0, 1, 3, 2)) / self.scale

        if mask is not None:
            score = score.masked_fill(mask==0, -1e10)

        attention = torch.softmax(score, dim=-1)

        x = torch.matmul(self.dropout(attention), V)
        
        x = x.permute(0, 2, 1, 3).contiguous()
        x = x.view(orig_shape)

        del Q, K, V

        return self.linears[-1](x)

        </code></pre>
      </div>
    </div>
    <hr>



    <div class="spacer"></div>
    <h3>Position-wise Feed Forward</h3>
    <div class="small-spacer"></div>
    <p>디코더는 디코더 레이어를 중첩시킨 구조로 이루어져 있습니다. 각각의 디코더 레이어에서는 
    </p>
    <div class="half-spacer"></div>

    <div class="code-container">
      <div class="code-snippet">
        <pre><code class="python">
class PositionwiseFeedForward(nn.Module):
    def __init__(self, config):
        super().__init__()

        self.fc_1 = nn.Linear(config.hidden_dim, config.pff_dim)
        self.fc_2 = nn.Linear(config.pff_dim, config.hidden_dim)
        self.dropout = nn.Dropout(config.dropout_ratio)


    def forward(self, x):
        x = self.dropout(F.gelu(self.fc_1(x)))
        return self.fc_2(x)        

        </code></pre>
      </div>
    </div>



    <h2>2. &hairsp; Experimental Setup</h2>

      <div class="half-spacer"></div>
      <h3>Data Setup</h3>

      <div class="img-container">
        <ul>
          <li>Machine Translaton
          </li>
          <p>WMT14의 En-De 데이터셋을 사용.
            데이터는 Source Text와 Target Text로 구성,
            모든 텍스트의 길이는 300자 이하로만 선별.
            source text와 target text의 길이 차이도 50자 이내의 것들만 선별
            최종적으로 Train/Valid/Test 50000, 1000, 100개로 구성
          </p>
          <div class="small-spacer"></div>
          <li>Dialogue Generation
          </li>
          <li>Text Summarization
          </li>          
        </ul>
      </div> 


      <div class="spacer"></div>
      <h3>Model Setup</h3>
      <div class="img-container">
        <ul>
          <li>Scratch Model

            <ul>
              <li>Model Parameters: 15,488,664</li>
              <li>Model Size: 60.085 MB</li>
              <li>여러 문장간 의미를 파악할 수 있도록 Token Type Id에 변형을 가함</li>
              <li>층이 깊어짐에 따라 기울기 소실의 문제점 발생</li>
          </ul>

          </li>

          <li>Torch Model
            
            <ul>
              <li>Model Parameters: 15,488,664</li>
              <li>Model Size: 60.085 MB</li>
              <li>여러 문장간 의미를 파악할 수 있도록 Token Type Id에 변형을 가함</li>
              <li>층이 깊어짐에 따라 기울기 소실의 문제점 발생</li>
            </ul>

          </li>
        </ul>
      </div>              


      <div class="spacer"></div>
      <h3>Training Setup</h3>
      <div class="img-container">
        <ul>
          <li>Scratch Model

            <ul>
              <li>Model Parameters: 10000</li>
              <li>여러 문장간 의미를 파악할 수 있도록 Token Type Id에 변형을 가함</li>
              <li>층이 깊어짐에 따라 기울기 소실의 문제점 발생</li>
          </ul>

          </li>

          <li>Torch Model
          </li>
        </ul>
      </div>              



    <h2>3. &hairsp; Result</h2>
    <div class="small-spacer"></div>
    <h3>Machine Translation</h3>
    <div class="small-spacer"></div>
    <table class="result-table">
      <thead>
        <tr>
          <th>Model Type</th>
          <th>Evaluation Score</th>
          <th>Avg Train Time</th>
          <th>Avg GPU Memory Usage</th>
          <th>Max GPU Memory Usage</th>
        </tr>
      </thead>
      <tbody>
        <tr>
          <td>Scratch Model</td>
          <td>12.16</td>
          <td>53 sec</td>
          <td>0.20 GB</td>
          <td>1.05 GB</td>
        </tr>
        <tr>
          <td>PyTorch Model</td>
          <td>13.28</td>
          <td>45 sec</td>
          <td>0.20GB</td>
          <td>0.95GB</td>
        </tr>

      </tbody>
    </table>


    <div class="half-spacer"></div>
    <h3>Dialogue Generation</h3>
    <div class="small-spacer"></div>
    <table class="result-table">
      <thead>
        <tr>
          <th>Model Type</th>
          <th>Evaluation Score</th>
          <th>Avg Train Time</th>
          <th>Avg GPU Memory Usage</th>
          <th>Max GPU Memory Usage</th>
        </tr>
      </thead>
      <tbody>
        <tr>
          <td>Scratch Model</td>
          <td>1.29</td>
          <td>52 sec</td>
          <td>0.20 GB</td>
          <td>0.91 GB</td>
        </tr>
        <tr>
          <td>PyTorch Model</td>
          <td>2.47</td>
          <td>45 sec</td>
          <td>0.20GB</td>
          <td>0.83GB</td>
        </tr>

      </tbody>
    </table>


    <div class="half-spacer"></div>
    <h3>Text Summarization</h3>
    <div class="small-spacer"></div>
    <table class="result-table">
      <thead>
        <tr>
          <th>Model Type</th>
          <th>Evaluation Score</th>
          <th>Avg Train Time</th>
          <th>Avg GPU Memory Usage</th>
          <th>Max GPU Memory Usage</th>
        </tr>
      </thead>
      <tbody>
        <tr>
          <td>Scratch Model</td>
          <td>5.84</td>
          <td>387 sec</td>
          <td>0.22 GB</td>
          <td>5.63 GB</td>
        </tr>
        <tr>
          <td>PyTorch Model</td>
          <td>7.94</td>
          <td>170 sec</td>
          <td>0.21GB</td>
          <td>2.90GB</td>
        </tr>

      </tbody>
    </table>

    <div class="half-spacer"></div>
    <h3>Result Analytics</h3>
    <div class="small-spacer"></div>
    <p>위의 결과를 보면, 성능, 시간, 메모리 사용 모든 측면에서 Pytorch에서 제공하는 모델이 우수함을 알 수 있습니다. 이는 Attention클래스에서 더욱 다양하고, 정밀한 코드 구현으로 인한 차이가 가장 큽니다. 그래도 대체 불가능할 정도로 차이가 많이 나는것은 아니라는 점이 고무적이긴 합니다.
    </p>

    <h2>4. &hairsp; Conclusion</h2>
    <div class="small-spacer"></div>
    <p> &nbsp; 이전의 RNN 계열의 모델들 보다 속도 측면, 그리고 성능 측면에서 굉장히 진일보 했음을 실험을 통해 직접적으로 확인할 수 있었음. 구현과정에서 복잡성이 증가 했다는 단점역시, Pytorch 라이브러리의 Transformer 모델을 통해 간편하게 사용할 수 있는 편의성이 있음.

    </p>  

    <h2 id="7" class="h3-mt">5. &hairsp; Reference</h2>

    <a class="reference" href="https://arxiv.org/abs/1706.03762">&nbsp; Attention Is All You Need</a> <br> 
    <a class="reference" href="http://nlp.seas.harvard.edu/annotated-transformer/">&nbsp; The Annotated Transformer</a> <br> 
    <a class="reference" href="https://pytorch.org/docs/stable/generated/torch.nn.Transformer.html">&nbsp; Pytorch Transformer Official Page</a>

  </div>
</div>

<div class="pagination">
  <a href="{{ '/rnn_seq2seq_attn' | relative_url }}" class="btn-prev"></a> 
  <a href="{{ '/transformer_variants' | relative_url }}" class="btn-next"></a>
</div>