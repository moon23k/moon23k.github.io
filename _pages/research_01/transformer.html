---
layout: default
permalink: /transformer
---


<div id="detail">
  <h1 class="title">Transformer Implementation from Scratch</h1>  
  <div class="post">

    <hr>
    <div class="callout">
      <p class="highlighted-text"><span>Project Objective</span></p>
      <div class="small-spacer"></div>
      <p>&nbsp; 이 프로젝트에서는 현재 딥러닝 모델링의 가장 유명한 Back Bone 모델인 Transformer를 직접 코드로 처음부터 끝까지 구현하고, 직접 구현한 Scratch Model를 Pytorch 라이브러이에서 제공하는 Transformer모델과 성능 비교하며, 구현이 잘 되었는지를 검증합니다. 
      프로젝트의 목적은 Transformer에 대한 심도있는 이해와 코드 구현이며, 모델의 성능 검증을 위해서는 세가지 자연어 생성 과제를 선정했습니다.
      실험 결과 Torch Model이 Scratch Model대비 효율성과 성능 측면 모두에서 더 나은 성과를 보였습니다. 
      이는 Multi Head Attention을 구현하는 과정에서 고도화의 문제에서 기인한 차이점입니다. 실제 Scratch Model에 Pytorch의 MultiHead Attention 클래스를 사용하면, Torch모델과 거의 동일한 성능및 효율성이 나옴을 확인했습니다.
      </p>
    </div>

    <div class="tblContents">
      <ul>

        <li>
          <a href="#1">1. &hairsp; Introduction</a>
        </li>

        <li>
          <a href="#2">2. &hairsp; Background</a>
        </li>

        <li>
          <a href="#3">3. &hairsp; Code Implementation</a>
        </li>

        <li>
          <a href="#4">4. &hairsp; Experimental Setup</a>
        </li>

        <li>
          <a href="#5">5. &hairsp; Results</a>
        </li>

        <li>
          <a href="#6">6. &hairsp; Conclusion</a>
        </li>

        <li>
          <a href="#7">7. &hairsp; Reference</a>
        </li>

      </ul>
    </div>
    <hr>


<!-- 1. Introduction -->    
    <h2 id="1">1. &hairsp; Introduction</h2>
    <div class="chapter-box">

      <div class="list-container">
        <ul>
          <li>Description</li>
            <p> &nbsp; 앞선 프로젝트에서는 RNN을 기반으로한 Sequence to Sequence 모델 구조의 효용성에 대해 면밀히 살펴봤습니다. 하지만 RNN 네트워크는 장기기억력이라는 관점에서 고질적인 문제점을 태생적으로 지니고 있으며, 이를 해결하기 위한 Attention 메커니즘의 활용 역시 기대만큼 좋은 성능을 보이지는 못했습니다. Transformer는 어텐션 메커니즘 만으로 문맥 전체적인 내용을 포착하는 뛰어난 성능과 더불어 병렬처리로 인한 학습의 효율성까지 제고시킨 뛰어난 모델 구조입니다. <br>

            Transformer는 Attention Mechanism만으로 획기적인 성능 향상과 더불어 효율성 역시 증진시킨 모델입니다.
            현재 활용되는 대부분의 좋은 성능의 모델들은 Transformer를 바탕으로 하고 있으며, 특히 NLP에서는 더욱 그러합니다.
            <br>
            이 프로젝트에서는 Transformer라는 우수한 모델에 대한 이해도를 증진시키기 위해 직접 Transformer를 구현하고, 이를 Pytorch에서 제공하는 Transformer 모델과 비교하며, 구현이 제대로 되었는지, 그리고 어떠한 차이가 있는지를 확인합니다. 
            </p>

          <div class="spacer"></div>
          <li>Objective
            <ul>
              <li style="font-weight: normal;">Transformer 모델에 대한 깊은 이해</li>
              <li style="font-weight: normal;">직접 구현한 Transformer와 Pytorch에서 제공하는 Transformer모델의 성능 비교</li>
            </ul>
          </li>

        </ul>
      </div>

    </div>



<!-- 2. Background -->    
    <h2 id="2">2. &hairsp; Background</h2>
    <div class="chapter-box">
      <div class="list-container">
        <ul>

          <li>Transformer
            <ul>
              <li>트랜스포머는 어텐션 메커니즘을 활용한 신경망 구조로, 장거리 종속성을 효과적으로 다루고 병렬 처리를 가능케 함</li>
              <li>인코더와 디코더로 이루어져 텍스트 처리에 적합하며, 특히 self-attention과 포지션 임베딩을 통해 문장의 의미와 순서 정보를 효과적으로 학습해 자연어 처리에서 혁신적인 성과를 보임</li>
              <li>기존 순환 신경망 대비 훨씬 높은 성능과 학습 효율성을 제공하며, 대규모 데이터셋에서 활용되는 강력한 모델</li>
            </ul>
          </li>
            
          <div class="spacer"></div>
          <li>Encoder & Decoder
            <ul>
              <li>트랜스포머의 인코더는 입력 시퀀스의 단어들을 임베딩하고, 다중 헤드 어텐션과 피드포워드 신경망을 통해 정보를 추상화. 각 인코더 레이어는 잔여 연결과 층 정규화를 활용하여 안정성을 유지하며, 입력 문장의 특징을 계층적으로 추출</li>
              <li>디코더는 인코더의 출력을 기반으로 출력 시퀀스를 생성. 각 디코더 레이어는 다중 헤드 어텐션을 사용하여 인코더의 출력과 현재까지의 디코더 입력에 대한 어텐션을 계산하고, 피드포워드 신경망을 통해 출력을 생성. 잔여 연결과 층 정규화는 안정성을 제공하며 디코딩 과정을 안정화.</li>
              <li>인코더와 디코더는 각각의 레이어를 통해 상호작용하며, 어텐션 메커니즘을 활용해 입력 문장의 문맥 정보를 고려하고 출력 문장을 생성. 트랜스포머의 인코더-디코더 구조는 기존의 번역 모델보다 효과적인 학습과 예측을 가능케 함.</li>
            </ul>
          </li>

          <div class="spacer"></div>
          <li>Multi Head Attention
            <ul>
              <li>트랜스포머의 핵심 구성 요소로, 여러 어텐션 헤드를 병렬로 활용하여 다양한 어텐션 가중치를 학습</li>
              <li>각 헤드는 서로 다른 관점에서 정보를 추출하며, 그 결과를 결합하여 모델이 다양한 특징을 효과적으로 학습하도록 유도</li>
              <li>Attention Score 산출을 위해 Scaled Dot Attention 방식을 사용</li>
            </ul>
          </li>

          <div class="spacer"></div>
          <li>Residual Connections and Layer Normalization
            <ul>
              <li>트랜스포머의 각 인코더와 디코더 레이어에 적용되는 구조로, 다양한 위치의 단어들에 대한 개별적인 신경망을 사용하여 비선형성을 도입</li>
              <li>이를 통해 모델은 단어 간의 상호작용을 강조하고 병렬 계산을 유지하며 효과적인 특징 추출이 가능</li>
              <li>포지션 와이즈 피드 포워드는 모델의 표현력을 향상시키는 중요한 구성 요소</li>
            </ul>
          </li>

          <div class="spacer"></div>
          <li>Residual Connections and Layer Normalization
            <ul>
              <li>트랜스포머에서는 각 레이어에 잔여 연결을 도입하여 그레이디언트 소실 문제를 완화하고 네트워크의 깊이를 키움</li>
              <li>레이어 정규화는 각 레이어의 출력을 정규화하여 안정적인 학습을 도모하고 훈련 과정을 안정화</li>
              <li>이 두 가지 구조는 모델의 안정성과 학습 효율성을 향상시키며, 트랜스포머의 성능에 기여</li>
            </ul>
          </li>

          <div class="spacer"></div>
          <li>Embeddings
            <ul>
              <li>Transformer의 Embedding은 Token Embedding과 Positional Encoding으로 구성</li>
              <li>Token Embedding은 시퀀스 토큰들을 고차원 벡터로 변환해 의미를 표현하기 위해 사용하며, 학습을 통해 업데아트 됨</li>
              <li>Positional Encoding은 병렬 처리과정에서 시퀀스의 위치정보를 반영하기 위한 레이어로 Token Embedding에 더해져 순서 정보를 모델에 제공</li>
            </ul>
          </li>

        </ul>
      </div>
    </div>




<!-- Code Implementation -->    
    <h2 id="3">3. &hairsp; Code Implementation</h2>

    <div class="chapter-box">

      <div class="list-container">
        <ul>
          <li>Scratch Model Implementation</li>

            <div class="code-container">
              <div class="code-snippet">
                <pre><code class="python">class MultiHeadAttention(nn.Module):
    def __init__(self, config):
        super().__init__()

        hidden_dim = config.hidden_dim
        self.n_heads = config.n_heads

        assert hidden_dim // self.n_heads
        self.head_dim = hidden_dim // self.n_heads
        
        self.dropout = nn.Dropout(config.dropout_ratio)
        self.linears = clones(nn.Linear(hidden_dim, hidden_dim), 4)
        self.scale = torch.sqrt(torch.FloatTensor([self.head_dim])).to(config.device)


    def forward(self, query, key, value, mask = None):

        orig_shape = list(query.shape)
        split_shape = [query.size(0), -1, self.n_heads, self.head_dim]

        Q, K, V = [lin(x).view(split_shape).transpose(1, 2) \
                   for lin, x in zip(self.linears, (query, key, value))]   

        score = torch.matmul(Q, K.permute(0, 1, 3, 2)) / self.scale

        if mask is not None:
            score = score.masked_fill(mask==0, -1e10)

        attention = torch.softmax(score, dim=-1)

        x = torch.matmul(self.dropout(attention), V)
        
        x = x.permute(0, 2, 1, 3).contiguous()
        x = x.view(orig_shape)

        del Q, K, V

        return self.linears[-1](x)



class PositionwiseFeedForward(nn.Module):
    def __init__(self, config):
        super().__init__()

        self.fc_1 = nn.Linear(config.hidden_dim, config.pff_dim)
        self.fc_2 = nn.Linear(config.pff_dim, config.hidden_dim)
        self.dropout = nn.Dropout(config.dropout_ratio)


    def forward(self, x):
        x = self.dropout(F.gelu(self.fc_1(x)))
        return self.fc_2(x)        



class EncoderLayer(nn.Module):
    def __init__(self, config):
        super().__init__()

        self.self_attn = MultiHeadAttention(config)
        self.pff = PositionwiseFeedForward(config)

        hidden_dim = config.hidden_dim
        self.self_attn_norm = nn.LayerNorm(hidden_dim)
        self.pff_norm = nn.LayerNorm(hidden_dim)
        
        self.dropout = nn.Dropout(config.dropout_ratio)


    def forward(self, x, e_mask):
        _x = self.self_attn(x, x, x, e_mask)
        x = self.self_attn_norm(x + self.dropout(_x))
        _x = self.pff(x)
        x = self.pff_norm(x + self.dropout(_x))

        return x



class DecoderLayer(nn.Module):
    def __init__(self, config):
        super().__init__()

        self.self_attn = MultiHeadAttention(config)
        self.enc_attn = MultiHeadAttention(config)
        self.pff = PositionwiseFeedForward(config)

        hidden_dim = config.hidden_dim
        self.self_attn_norm = nn.LayerNorm(hidden_dim)
        self.enc_attn_norm = nn.LayerNorm(hidden_dim)
        self.pff_norm = nn.LayerNorm(hidden_dim)

        self.dropout = nn.Dropout(config.dropout_ratio)


    def forward(self, x, m, e_mask, d_mask):

        _x = self.self_attn(x, x, x, d_mask)
        x = self.self_attn_norm(x + self.dropout(_x))

        _x = self.enc_attn(x, m, m, e_mask)
        x = self.enc_attn_norm(x + self.dropout(_x))

        _x = self.pff(x)
        x = self.pff_norm(x + self.dropout(_x))

        return x



class Encoder(nn.Module):
    def __init__(self, config):
        super().__init__()

        self.device = config.device
        self.emb = Embeddings(config)
        self.dropout = nn.Dropout(config.dropout_ratio)

        self.layers = nn.ModuleList(
            [EncoderLayer(config) for _ in range(config.n_layers)]
        )


    def forward(self, x, e_mask):
        x = self.emb(x)
        for layer in self.layers:
            x = layer(x, e_mask)
        return x



class Decoder(nn.Module):
    def __init__(self, config):
        super().__init__()

        self.device = config.device
        self.emb = Embeddings(config)
        self.dropout = nn.Dropout(config.dropout_ratio)

        self.layers = nn.ModuleList(
            [DecoderLayer(config) for _ in range(config.n_layers)]
        )


    def forward(self, x, memory, e_mask, d_mask):
        x = self.emb(x)
        for layer in self.layers:
            x = layer(x, memory, e_mask, d_mask)
        return x
     


class ScratchModel(nn.Module):
    def __init__(self, config):
        super(ScratchModel, self).__init__()

        self.pad_id = config.pad_id
        self.device = config.device
        self.vocab_size = config.vocab_size

        self.encoder = Encoder(config)
        self.decoder = Decoder(config)
        self.generator = nn.Linear(config.hidden_dim, self.vocab_size)

        self.criterion = nn.CrossEntropyLoss()
        self.out = namedtuple('Out', 'logit loss')


    def pad_mask(self, x):
        return (x != self.pad_id).unsqueeze(1).unsqueeze(2)


    def dec_mask(self, x):
        sz = x.size(1)
        pad_mask = self.pad_mask(x)
        sub_mask = torch.tril(torch.ones((sz, sz), device=self.device)).bool()
        return pad_mask & sub_mask


    @staticmethod
    def shift_y(x):
        return x[:, :-1], x[:, 1:]


    def forward(self, x, y):
        y, label = self.shift_y(y)

        e_mask = self.pad_mask(x) 
        d_mask = self.dec_mask(y)

        memory = self.encoder(x, e_mask)
        dec_out = self.decoder(y, memory, e_mask, d_mask)
        logit = self.generator(dec_out)

        #Getting Outputs
        self.out.logit = logit
        self.out.loss = self.criterion(
            logit.contiguous().view(-1, self.vocab_size), 
            label.contiguous().view(-1)
        )
        
        return self.out</code></pre>
            </div>
          </div>


          <div class="spacer"></div>
          <li>Pytorch Model Implementation</li>

            <div class="code-container">
              <div class="code-snippet">
                <pre><code class="python">class Encoder(nn.Module):
    def __init__(self, config):
        super(Encoder, self).__init__()
        
        layer = nn.TransformerEncoderLayer(
            d_model=config.hidden_dim,
            nhead=config.n_heads,
            dim_feedforward=config.pff_dim,
            dropout=config.dropout_ratio,
            activation='gelu',
            batch_first=True
        )

        self.embeddings = Embeddings(config)
        self.layers = clones(layer, config.n_layers)


    def forward(self, x, e_mask):

        x = self.embeddings(x)
        
        for layer in self.layers:
            x = layer(x, src_key_padding_mask=e_mask)
        
        return x



class Decoder(nn.Module):
    def __init__(self, config):
        super(Decoder, self).__init__()

        layer = nn.TransformerDecoderLayer(
            d_model=config.hidden_dim,
            nhead=config.n_heads,
            dim_feedforward=config.pff_dim,
            dropout=config.dropout_ratio,
            activation='gelu',
            batch_first=True
        )

        self.embeddings = Embeddings(config)
        self.layers = clones(layer, config.n_layers)


    def forward(self, x, memory, e_mask=None, d_mask=None):
        
        x = self.embeddings(x)
        
        for layer in self.layers:
            x = layer(
                x, memory, 
                memory_key_padding_mask=e_mask,
                tgt_mask=d_mask,
            )

        return x



class TorchModel(nn.Module):
    def __init__(self, config):
        super(TorchModel, self).__init__()
        
        self.device = config.device
        self.pad_id = config.pad_id
        self.vocab_size = config.vocab_size

        self.encoder = Encoder(config)
        self.decoder = Decoder(config)
        self.generator = nn.Linear(config.hidden_dim, self.vocab_size)

        self.criterion = nn.CrossEntropyLoss()
        self.out = namedtuple('Out', 'logit loss')

    
    def pad_mask(self, x):
        return x == self.pad_id


    @staticmethod    
    def shift_y(x):
        return x[:, :-1], x[:, 1:]    


    def dec_mask(self, x):
        sz = x.size(1)
        return torch.triu(torch.full((sz, sz), float('-inf')), diagonal=1).to(self.device)


    def forward(self, x, y):
        y, label = self.shift_y(y)

        #Masking
        e_mask = self.pad_mask(x)
        d_mask = self.dec_mask(y)
        
        #Actual Processing
        memory = self.encoder(x, e_mask)
        dec_out = self.decoder(y, memory, e_mask, d_mask)
        logit = self.generator(dec_out)
        
        #Getting Outputs
        self.out.logit = logit
        self.out.loss = self.criterion(
            logit.contiguous().view(-1, self.vocab_size), 
            label.contiguous().view(-1)
        )
        
        return self.out</code></pre>
            </div>
          </div>
        </div>
      </div>




<!-- 4. Experimental Setup -->
    <h2 id="4">4. Experimental Setup</h2>
    <div class="chapter-box">

      <div class="list-container">
        <ul>
          <li>Data Setup
            <div class="dual-container">
              <ul>
                <li><b>Translation Task</b>: &nbsp; WMT'14 En-De</li>
                <li><b>Dialogue Task</b>: &nbsp; Daily Dialogue</li>
                <li><b>Summarization Task</b>: &nbsp; CNN Daily</li>
                <li><b>Tokenizer</b>: &nbsp; Word-Level Tokenizer</li>
              </ul>
              <ul>
                <li><b>Train Data Volumn</b>: &nbsp; 50,000</li>
                <li><b>Valid Data Volumn</b>: &nbsp; 5,000</li>
                <li><b>Test Data Volumn</b>: &nbsp; 100</li>
                <li><b>Vocab Size</b>: &nbsp; 10,000</li>                
              </ul>
            </div>
          </li>

          <br class="half-spacer">
          <li>Model Setup
            <div class="dual-container">
              <ul>
                <li><b>Input Dim</b>: &nbsp; 10,000</li>
                <li><b>Output Dim</b>: &nbsp; 10,000</li>
                <li><b>Embedding Dim</b>: &nbsp; 512</li>
              </ul>
              <ul>
                <li><b>Hidden Dim</b>: &nbsp; 512</li>
                <li><b>Model Params</b>: &nbsp; 000</li>
                <li><b>Model Size</b>: &nbsp; 000</li>                
              </ul>
            </div>
          </li>

          <br class="half-spacer">
          <li>Training Setup
            <div class="dual-container">
              <ul>
                <li><b>Num Epochs</b>: &nbsp; 10</li>
                <li><b>Batch Size</b>: &nbsp; 32</li>
                <li><b>Learning Rate</b>: &nbsp; 5e-4</li>
                <li><b>LR Scheduler</b>: &nbsp; pleature</li>
              </ul>
              <ul>
                <li><b>Optimizer</b>: &nbsp; AdamW</li>
                <li><b>Gradient Accumulation Steps</b>: &nbsp; 4
                <li><b>Teacher Forcing Ratio</b>: &nbsp; 0.5</li>                
              </ul>
            </div>
          </li>
        </ul>
      </div>
    </div>



<!-- Result --> 
    <h2 id="5">5. &hairsp; Result</h2>

    <div class="chapter-box">

      <div class="list-container">
        <ul>
          <li>Machine Translation</li>

            <table class="result-table">
              <thead>
                <tr>
                  <th>Model Type</th>
                  <th>Eval Score</th>
                  <th>Epoch Time</th>
                  <th>Avg GPU</th>
                  <th>Max GPU</th>
                </tr>
              </thead>
              <tbody>
                <tr>
                  <td>Scratch Model</td>
                  <td>12.16</td>
                  <td>53s</td>
                  <td>0.20 GB</td>
                  <td>1.05 GB</td>
                </tr>
                <tr>
                  <td>PyTorch Model</td>
                  <td>13.28</td>
                  <td>45s</td>
                  <td>0.20GB</td>
                  <td>0.95GB</td>
                </tr>

              </tbody>
            </table>

          <div class="spacer"></div>
          <li>Dialogue Generation</li>

            <table class="result-table">
              <thead>
                <tr>
                  <th>Model Type</th>
                  <th>Eval Score</th>
                  <th>Epoch Time</th>
                  <th>Avg GPU</th>
                  <th>Max GPU</th>
                </tr>
              </thead>
              <tbody>
                <tr>
                  <td>Scratch Model</td>
                  <td>12.16</td>
                  <td>53s</td>
                  <td>0.20 GB</td>
                  <td>1.05 GB</td>
                </tr>
                <tr>
                  <td>PyTorch Model</td>
                  <td>13.28</td>
                  <td>45s</td>
                  <td>0.20GB</td>
                  <td>0.95GB</td>
                </tr>

              </tbody>
            </table>

          <div class="spacer"></div>
          <li>Text Summarization</li>

            <table class="result-table">
              <thead>
                <tr>
                  <th>Model Type</th>
                  <th>Eval Score</th>
                  <th>Epoch Time</th>
                  <th>Avg GPU</th>
                  <th>Max GPU</th>
                </tr>
              </thead>
              <tbody>
                <tr>
                  <td>Scratch Model</td>
                  <td>12.16</td>
                  <td>53s</td>
                  <td>0.20 GB</td>
                  <td>1.05 GB</td>
                </tr>
                <tr>
                  <td>PyTorch Model</td>
                  <td>13.28</td>
                  <td>45s</td>
                  <td>0.20GB</td>
                  <td>0.95GB</td>
                </tr>

              </tbody>
            </table>
        </ul>
      </div>
    </div>



    <h2 id="6">6. &hairsp; Conclusion</h2>

    <div class="chapter-box">

      <div class="list-container">
        <ul>
          <li>Machine Translation</li>
          <li>Machine Translation</li>
          <li>Machine Translation</li>
        </ul>
      </div>
    </div>


    <h2 id="7" class="h3-mt">7. &hairsp; Reference</h2>
    <div class="chapter-box">
      <a class="reference" href="https://arxiv.org/abs/1706.03762">&nbsp; Attention Is All You Need</a> <br> 
      <a class="reference" href="http://nlp.seas.harvard.edu/annotated-transformer/">&nbsp; The Annotated Transformer</a> <br> 
      <a class="reference" href="https://pytorch.org/docs/stable/generated/torch.nn.Transformer.html">&nbsp; Pytorch Transformer Official Page</a>
    </div>

  </div>
</div>


<div class="pagination">
  <a href="{{ '/rnn_seq2seq_attn' | relative_url }}" class="btn-prev"><span>RNN Seq2sSeq with Attention</span></a>
  <a href="{{ '/transformer_balance' | relative_url }}" class="btn-next"><span>Transformer Balance</span></a>
</div>