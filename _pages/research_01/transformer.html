---
layout: default
permalink: /transformer
---


<div id="detail">
  <h1 class="title">Transformer Implementation from Scratch</h1>  
  <div class="post">

    <hr>
    <div class="callout">
      <p class="highlighted-text"><span>Project Objective</span></p>
      <div class="small-spacer"></div>
      <p>&nbsp; 이 프로젝트에서는 현재 딥러닝 모델링의 가장 유명한 Back Bone 모델인 Transformer를 직접 코드로 처음부터 끝까지 구현하고, 직접 구현한 Scratch Model를 Pytorch 라이브러이에서 제공하는 Transformer모델과 성능 비교하며, 구현이 잘 되었는지를 검증합니다. 
      프로젝트의 목적은 Transformer에 대한 심도있는 이해와 코드 구현이며, 모델의 성능 검증을 위해서는 세가지 자연어 생성 과제를 선정했습니다.
      실험 결과 Torch Model이 Scratch Model대비 효율성과 성능 측면 모두에서 더 나은 성과를 보였습니다. 
      이는 Multi Head Attention을 구현하는 과정에서 고도화의 문제에서 기인한 차이점입니다. 실제 Scratch Model에 Pytorch의 MultiHead Attention 클래스를 사용하면, Torch모델과 거의 동일한 성능및 효율성이 나옴을 확인했습니다.
      </p>
    </div>

    <div class="tblContents">
      <ul>

        <li>
          <a href="#1">1. &hairsp; Introduction</a>
        </li>

        <li>
          <a href="#2">2. &hairsp; Background</a>
        </li>

        <li>
          <a href="#3">3. &hairsp; Code Implementation</a>
        </li>

        <li>
          <a href="#4">4. &hairsp; Experimental Setup</a>
        </li>

        <li>
          <a href="#5">5. &hairsp; Results</a>
        </li>

        <li>
          <a href="#6">6. &hairsp; Conclusion</a>
        </li>

        <li>
          <a href="#7">7. &hairsp; Reference</a>
        </li>

      </ul>
    </div>
    <hr>


<!-- 1. Introduction -->    
    <h2 id="1">1. &hairsp; Introduction</h2>
    <div class="chapter-box">

      <div class="list-container">
        <ul>
          <li>Description</li>
            <p> &nbsp; 앞선 프로젝트에서는 RNN을 기반으로한 Sequence to Sequence 모델 구조의 효용성에 대해 면밀히 살펴봤습니다. 하지만 RNN 네트워크는 장기기억력이라는 관점에서 고질적인 문제점을 태생적으로 지니고 있으며, 이를 해결하기 위한 Attention 메커니즘의 활용 역시 기대만큼 좋은 성능을 보이지는 못했습니다. Transformer는 어텐션 메커니즘 만으로 문맥 전체적인 내용을 포착하는 뛰어난 성능과 더불어 병렬처리로 인한 학습의 효율성까지 제고시킨 뛰어난 모델 구조입니다. <br>

            Transformer는 Attention Mechanism만으로 획기적인 성능 향상과 더불어 효율성 역시 증진시킨 모델입니다.
            현재 활용되는 대부분의 좋은 성능의 모델들은 Transformer를 바탕으로 하고 있으며, 특히 NLP에서는 더욱 그러합니다.
            <br>
            이 프로젝트에서는 Transformer라는 우수한 모델에 대한 이해도를 증진시키기 위해 직접 Transformer를 구현하고, 이를 Pytorch에서 제공하는 Transformer 모델과 비교하며, 구현이 제대로 되었는지, 그리고 어떠한 차이가 있는지를 확인합니다. 
            </p>

          <br class="half-spacer">
          <li>Objective
            <ul>
              <li style="font-weight: normal;">Transformer 모델에 대한 깊은 이해</li>
              <li style="font-weight: normal;">직접 구현한 Transformer와 Pytorch에서 제공하는 Transformer모델의 성능 비교</li>
            </ul>
          </li>

        </ul>
      </div>

    </div>



<!-- 2. Background -->    
    <h2 id="2">2. &hairsp; Background</h2>
    <div class="chapter-box">

      <div class="list-container">
        <ul>
          <li>Transformer</li>
            <p>&nbsp; 어텐션은 기존 단일 벡터 공간에 모든 시퀀스의 정보를 압축적으로 표현하면서 발생하는 제약을 극복하기 위한 연산 기법을 의미합니다. 매시점마다, 디코딩 과정에서 입력시퀀스에 집중할만한 정보에 더욱 집중해서 정보를 추출하기 위한 기법입니다. 기본적으로 어텐션 연산은 쿼리, 키, 밸류값을 기반으로 행해집니다. 쿼리와 키 값을 토대로, 쿼리와 키 간의 유사도를 측정하고, 유사도 수치를 확률값으로 변환합니다. 이후, 어텐션 확률값을 밸류에 곱해 최종적인 어텐션 값을 도출합니다.

            이 프로젝트에서는 총 세가지의 어텐션 기법을 사용하는데, 각각은 additive, dot product, scaled dot product입니다. 

            Additive Attention은 Bahdanau 어텐션으로도 불리는 유명한 어텐션 기법이고, 이후 루옹에 의한 어텐션 후속 연구에서 가장 많이 사용되는 Dot Product와, 이를 개선한, 그리고 Transformer 어텐션에서 활용되는 Scaled Dot Product를 선정했습니다. 각 어텐션에 대한 상세는 아래와 같습니다.
            </p>
            

          <div class="spacer"></div>
          <li>Multi Head Attention</li>

          <div class="dual-container">
            <img src="{{ 'assets/img/research_01/bahdanau.png' | relative_url }}" style="width: 27%; height: 20%; display: block; margin: 0 1em;">
            <p>Additive Attention, 혹은 바다나우 어텐션으로도 불리는 이 어텐션 기법은 이름에서 드러나듯, 어텐션 연산에서 각 값을 더해주는 방식을 사용합니다. 
            t시점의 예측을 위해, Encoder의 Hidden State들과 t-1 시점의 Hidden State를 비교합니다.
            </p>
          </div>

            <div class="latex-container">
              <div class="equation-item">
                  <p class="latex-equation">\( \mathbf{Attention \space Score} = W^T_a \tanh(W_b s_{t-1} + W_c H) \)</p>
              </div>
              <div class="equation-item">
                  <p class="latex-equation">\( \mathbf{Attention \space Distribution} = softmax(Attention \space Score) \)</p>
              </div>
              <div class="equation-item">
                  <p class="latex-equation">\( \mathbf{Attention \space Value} = H \cdot Attention \space Distribution \)</p>
              </div>
              <div class="equation-item">
                  <p class="latex-equation">\( {\Large s_t} = \text{Decoder}(y_{t-1}, s_{t-1}, c_t) \)</p>
              </div>
            </div>

            <p>&nbsp; Additive Attention, 혹은 바다나우 어텐션으로도 불리는 이 어텐션 기법은 이름에서 드러나듯, 어텐션 연산에서 각 값을 더해주는 방식을 사용합니다. 
            </p>

          <div class="spacer"></div>
          <li>Module Connection</li>

          <div class="dual-container">
            <img src="{{ 'assets/img/research_01/luong.png' | relative_url }}" style="width: 30%; height: 40%; display: block; margin: 0 1em;">
            <p>Dot Product Attention은 이름에서도 드러나듯, 유사도 측정 과정에서 벡터간의 내적(Dot Product)를 사용합니다. 내적 연산은 두 벡터간의 유사도가 클수록 큰값이, 작을수록 작은 값이 도출되는 연산입니다. 별도의 학습 가능한 가중치가 필요하지 않기에 연산과정이 매우 간단하고 효율적이라는 장점이 존재합니다.
            </p>
          </div>


            <div class="latex-container">
              <div class="equation-item">
                  <p class="latex-equation">\( \mathbf{Attention \space Score} = \frac{QK^T}{\sqrt{d_k}}V \)</p>
              </div>
              <div class="equation-item">
                  <p class="latex-equation">\( \mathbf{Attention \space Distribution} = softmax(Attention \space Score) \)</p>
              </div>
              <div class="equation-item">
                  <p class="latex-equation">\( Attention \space Value = W^T_a \tanh(W_b s_{t-1} + W_c h_i) \)</p>
              </div>
              <div class="equation-item">
                  <p class="latex-equation">\( {\Large s_t} = \text{Decoder}(y_{t-1}, s_{t-1}, c_t) \)</p>
              </div>              
            </div>

            <p> Luong Attention이라고도 불리며, 이는 Additive Attention의 복잡한 연산과정을, 단순한 내적을 통한 유사도 계산으로 대체하며, 계산량을 줄인 어텐션 기법입니다.

            </p>

          <div class="half-spacer"></div>
          <li>Embeddings</li>
            <p> 앞서 소개한 Dot Product Attention 기법이, 연산 효율성과 성능 측면에서도 좋은 모습을 보이고는 있지만, 다음과 같은 문제점이 존재합니다.
              1. 
            </p>

        </ul>
      </div>
    </div>




<!-- Code Implementation -->    
    <h2 id="3">3. &hairsp; Code Implementation</h2>

    <div class="chapter-box">

      <div class="list-container">
        <ul>
          <li>Scratch Model Implementation</li>

            <div class="code-container">
              <div class="code-snippet">
                <pre><code class="python">class Transformer(nn.Module):
    def __init__(self, config):
        super(Transformer, self).__init__()

        self.pad_id = config.pad_id
        self.device = config.device
        self.vocab_size = config.vocab_size

        self.encoder = Encoder(config)
        self.decoder = Decoder(config)
        self.generator = nn.Linear(config.hidden_dim, self.vocab_size)


    def pad_mask(self, x):
        return (x != self.pad_id).unsqueeze(1).unsqueeze(2)


    def dec_mask(self, x):
        sz = x.size(1)
        pad_mask = self.pad_mask(x)
        sub_mask = torch.tril(torch.ones((sz, sz), device=self.device)).bool()
        return pad_mask & sub_mask


    @staticmethod
    def shift_y(x):
        return x[:, :-1], x[:, 1:]


    def forward(self, x, y):
        y, label = self.shift_y(y)

        e_mask = self.pad_mask(x) 
        d_mask = self.dec_mask(y)

        memory = self.encoder(x, e_mask)
        dec_out = self.decoder(y, memory, e_mask, d_mask)
        
        return self.generator(dec_out)</code></pre>
            </div>
          </div>


          <div class="spacer"></div>
          <li>Pytorch Model Implementation</li>

            <div class="code-container">
              <div class="code-snippet">
                <pre><code class="python">class Transformer(nn.Module):
    def __init__(self, config):
        super(Transformer, self).__init__()

        self.pad_id = config.pad_id
        self.device = config.device
        self.vocab_size = config.vocab_size

        self.encoder = Encoder(config)
        self.decoder = Decoder(config)
        self.generator = nn.Linear(config.hidden_dim, self.vocab_size)


    def pad_mask(self, x):
        return (x != self.pad_id).unsqueeze(1).unsqueeze(2)


    def dec_mask(self, x):
        sz = x.size(1)
        pad_mask = self.pad_mask(x)
        sub_mask = torch.tril(torch.ones((sz, sz), device=self.device)).bool()
        return pad_mask & sub_mask


    @staticmethod
    def shift_y(x):
        return x[:, :-1], x[:, 1:]


    def forward(self, x, y):
        y, label = self.shift_y(y)

        e_mask = self.pad_mask(x) 
        d_mask = self.dec_mask(y)

        memory = self.encoder(x, e_mask)
        dec_out = self.decoder(y, memory, e_mask, d_mask)
        
        return self.generator(dec_out)</code></pre>
            </div>
          </div>
        </div>
      </div>






<!-- 4. Experimental Setup -->
    <h2 id="4">4. Experimental Setup</h2>
    <div class="chapter-box">

      <div class="list-container">
        <ul>
          <li>Data Setup
            <div class="dual-container">
              <ul>
                <li><b>Translation Task</b>: &nbsp; WMT'14 En-De</li>
                <li><b>Dialogue Task</b>: &nbsp; Daily Dialogue</li>
                <li><b>Summarization Task</b>: &nbsp; CNN Daily</li>
                <li><b>Tokenizer</b>: &nbsp; Word-Level Tokenizer</li>
              </ul>
              <ul>
                <li><b>Train Data Volumn</b>: &nbsp; 50,000</li>
                <li><b>Valid Data Volumn</b>: &nbsp; 5,000</li>
                <li><b>Test Data Volumn</b>: &nbsp; 100</li>
                <li><b>Vocab Size</b>: &nbsp; 10,000</li>                
              </ul>
            </div>
          </li>

          <br class="half-spacer">
          <li>Model Setup
            <div class="dual-container">
              <ul>
                <li><b>Input Dim</b>: &nbsp; 10,000</li>
                <li><b>Output Dim</b>: &nbsp; 10,000</li>
                <li><b>Embedding Dim</b>: &nbsp; 512</li>
              </ul>
              <ul>
                <li><b>Hidden Dim</b>: &nbsp; 512</li>
                <li><b>Model Params</b>: &nbsp; 000</li>
                <li><b>Model Size</b>: &nbsp; 000</li>                
              </ul>
            </div>
          </li>

          <br class="half-spacer">
          <li>Training Setup
            <div class="dual-container">
              <ul>
                <li><b>Num Epochs</b>: &nbsp; 10</li>
                <li><b>Batch Size</b>: &nbsp; 32</li>
                <li><b>Learning Rate</b>: &nbsp; 5e-4</li>
                <li><b>LR Scheduler</b>: &nbsp; pleature</li>
              </ul>
              <ul>
                <li><b>Optimizer</b>: &nbsp; AdamW</li>
                <li><b>Gradient Accumulation Steps</b>: &nbsp; 4
                <li><b>Teacher Forcing Ratio</b>: &nbsp; 0.5</li>                
              </ul>
            </div>
          </li>
        </ul>
      </div>
    </div>



<!-- Result --> 
    <h2 id="5">5. &hairsp; Result</h2>

    <div class="chapter-box">

      <div class="list-container">
        <ul>
          <li>Machine Translation</li>

            <table class="result-table">
              <thead>
                <tr>
                  <th>Model Type</th>
                  <th>Eval Score</th>
                  <th>Epoch Time</th>
                  <th>Avg GPU</th>
                  <th>Max GPU</th>
                </tr>
              </thead>
              <tbody>
                <tr>
                  <td>Scratch Model</td>
                  <td>12.16</td>
                  <td>53s</td>
                  <td>0.20 GB</td>
                  <td>1.05 GB</td>
                </tr>
                <tr>
                  <td>PyTorch Model</td>
                  <td>13.28</td>
                  <td>45s</td>
                  <td>0.20GB</td>
                  <td>0.95GB</td>
                </tr>

              </tbody>
            </table>

          <div class="spacer"></div>
          <li>Dialogue Generation</li>

            <table class="result-table">
              <thead>
                <tr>
                  <th>Model Type</th>
                  <th>Eval Score</th>
                  <th>Epoch Time</th>
                  <th>Avg GPU</th>
                  <th>Max GPU</th>
                </tr>
              </thead>
              <tbody>
                <tr>
                  <td>Scratch Model</td>
                  <td>12.16</td>
                  <td>53s</td>
                  <td>0.20 GB</td>
                  <td>1.05 GB</td>
                </tr>
                <tr>
                  <td>PyTorch Model</td>
                  <td>13.28</td>
                  <td>45s</td>
                  <td>0.20GB</td>
                  <td>0.95GB</td>
                </tr>

              </tbody>
            </table>

          <div class="spacer"></div>
          <li>Text Summarization</li>

            <table class="result-table">
              <thead>
                <tr>
                  <th>Model Type</th>
                  <th>Eval Score</th>
                  <th>Epoch Time</th>
                  <th>Avg GPU</th>
                  <th>Max GPU</th>
                </tr>
              </thead>
              <tbody>
                <tr>
                  <td>Scratch Model</td>
                  <td>12.16</td>
                  <td>53s</td>
                  <td>0.20 GB</td>
                  <td>1.05 GB</td>
                </tr>
                <tr>
                  <td>PyTorch Model</td>
                  <td>13.28</td>
                  <td>45s</td>
                  <td>0.20GB</td>
                  <td>0.95GB</td>
                </tr>

              </tbody>
            </table>
        </ul>
      </div>
    </div>



    <h2>4. &hairsp; Conclusion</h2>

    <div class="chapter-box">

      <div class="list-container">
        <ul>
          <li>Machine Translation</li>
          <li>Machine Translation</li>
          <li>Machine Translation</li>
        </ul>
      </div>
    </div>


    <h2 id="7" class="h3-mt">5. &hairsp; Reference</h2>
    <div class="chapter-box">
      <a class="reference" href="https://arxiv.org/abs/1706.03762">&nbsp; Attention Is All You Need</a> <br> 
      <a class="reference" href="http://nlp.seas.harvard.edu/annotated-transformer/">&nbsp; The Annotated Transformer</a> <br> 
      <a class="reference" href="https://pytorch.org/docs/stable/generated/torch.nn.Transformer.html">&nbsp; Pytorch Transformer Official Page</a>
    </div>

  </div>
</div>


<div class="pagination">
  <a href="{{ '/rnn_seq2seq_attn' | relative_url }}" class="btn-prev"><span>RNN Seq2sSeq with Attention</span></a>
  <a href="{{ '/transformer_variants' | relative_url }}" class="btn-next"><span>Transformer Variants</span></a>
</div>