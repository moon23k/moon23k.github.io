---
layout: default
permalink: /transformer
---


<div id="detail">
  <h3 class="title">Transformer 모델 구현과 자연어 생성 과제에서의 성능 측정</h3>

  <div class="tblContents">
    <ul>
      <li>
        <a href="#1">1. Introduction</a>
      </li>
      
      <li>
        <a href="#2">2. Transformer Architecure</a>
      </li>

      <li>
        <a href="#3">3. Code Implementation</a>
        <ul>
          <li>
            <a href="#3.1">3.1 From Scratch</a>
          </li>
          <li>
            <a href="#3.2">3.2 Pytorch</a>
          </li>
        </ul>
      </li>


      <li>
        <a href="#4">4. Experimental Setup</a>
        <ul>
          <li>
            <a href="#4.1">4.1 Data Setups</a>
          </li>
          <li>
            <a href="#4.2">4.2 Model Setups</a>
          </li>
          <li>
            <a href="#4.3">4.3 Training Setups</a>
          </li>
          <li>
            <a href="#4.4">4.4 Model desc</a>
          </li>
        </ul>
      </li>

      <li>
        <a href="#5">5. Results</a>
      </li>

      <li>
        <a href="#6">6. Conclusion</a>
      </li>

      <li>
        <a href="#7">7. Reference</a>
      </li>

    </ul>
  </div>


  
  <div class="post">

    <h2 id="1">1. Introduction</h2>
      <p>
        &nbsp; 앞선 프로젝트에서는 RNN을 기반으로한 Sequence to Sequence 모델 구조의 효용성에 대해 면밀히 살펴봤습니다. 
        하지만 RNN 네트워크는 장기기억력이라는 관점에서 고질적인 문제점을 태생적으로 지니고 있으며, 이를 해결하기 위한 Attention 메커니즘의 활용 역시 기대만큼 좋은 성능을 보이지는 못했습니다.
        Transformer는 어텐션 메커니즘 만으로 문맥 전체적인 내용을 포착하는 뛰어난 성능과 더불어 병렬처리로 인한 학습의 효율성까지 제고시킨 뛰어난 모델 구조입니다.
        이 글에서는 Transformer의 구조에 대한 설명과 더불어, 두가지 방식에서의 Code Implementation 방식을 알아봅니다. 또한 이렇게 구현한 Transformer 모델이 세가지 자연어 생성 과제에서 어떤 성능을 보이는지 비교해봅니다.
      </p>



    <h2 id="2">2. Architecture</h2>
        <div class="img-container">
          <ul>
            <li>Encoder Decoder Architecture
              <ul>
                <li>Encoder</li>
                <li>Decoder</li>
              </ul>
            </li>

            <li>Embeddings
              <ul>
                <li>Token Embedding</li>
                <li>Position Embedding</li>
              </ul>
            </li>

            <li>Multi-Head Attention
              <ul>
                <li>다수의 어텐션 헤드를 통해 입력 시퀀스의 의미를 다각도로 포착</li>
                <li>어텐션은 주어진 쿼리, 키, 밸류 쌍 간의 관련성을 계산해 가중 평균을 산출</li>
              </ul>
            </li>

            <li>Position Wise Feed Forward
              <ul>
                <li>다수의 어텐션 헤드를 통해 입력 시퀀스의 의미를 다각도로 포착</li>
                <li>어텐션은 주어진 쿼리, 키, 밸류 쌍 간의 관련성을 계산해 가중 평균을 산출</li>
              </ul>
            </li>

            <li>Encoder Layer
              <ul>
                <li>다수의 어텐션 헤드를 통해 입력 시퀀스의 의미를 다각도로 포착</li>
                <li>어텐션은 주어진 쿼리, 키, 밸류 쌍 간의 관련성을 계산해 가중 평균을 산출</li>
              </ul>
            </li>

            <li>Decoder Layer
              <ul>
                <li>다수의 어텐션 헤드를 통해 입력 시퀀스의 의미를 다각도로 포착</li>
                <li>어텐션은 주어진 쿼리, 키, 밸류 쌍 간의 관련성을 계산해 가중 평균을 산출</li>
              </ul>
            </li>            

            <li>Encoder
              <ul>
                <li>다수의 어텐션 헤드를 통해 입력 시퀀스의 의미를 다각도로 포착</li>
                <li>어텐션은 주어진 쿼리, 키, 밸류 쌍 간의 관련성을 계산해 가중 평균을 산출</li>
              </ul>
            </li>

            <li>Decoder
              <ul>
                <li>다수의 어텐션 헤드를 통해 입력 시퀀스의 의미를 다각도로 포착</li>
                <li>어텐션은 주어진 쿼리, 키, 밸류 쌍 간의 관련성을 계산해 가중 평균을 산출</li>
              </ul>
            </li>
          
          </ul>
        </div>


    <h2 id="3">3. Code Implementation</h2>
      <h3 id="3.1">3.1 From Scratch</h3>

        <pre><code class="python">def fib(n):
        a, b = 0, 1
          while a < n:
            print(a, end=' ')
            a, b = b, a+b
            print()
        fib(1000)</code></pre>


      <h3 id="3.2">3.2 Pytorch</h3>
      




    <h2 id="4">4. Experimental Setup</h2>
      <h3 id="4.1">4.1 Data Setups</h3>

      <h3 id="4.2">4.2 Model Setups</h3>
      
      <h3 id="4.3">4.3 Training Setups</h3>
      
      <h3 id="4.4">4.4 Model desc</h3>


    <h2 id="5">5. Result</h2>
      기존 RNN 네트워크의 Sequence to Sequence 모델에서 Attention을 활용하는 방식의 성능을 포함시켜서 성능 비교 ㄱㄱ

    <h2 id="6">6. Conclusion</h2>


    <h2 id="7">7. Reference</h2>



  </div>
</div>

<div class="pagination">
  <a href="{{ '/project_02' | relative_url }}" class="btn-prev"></a>
  <a href="{{ '/project_04' | relative_url }}" class="btn-next"></a>
</div>