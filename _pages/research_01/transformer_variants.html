---
layout: default
permalink: /transformer_variants
---


<div id="detail">
  <h1 class="title">Transformer Variants Comparison Study</h1>  
  <div class="post">

    <hr>
    <div class="callout">
      <p class="highlighted-text"><span>Project Objective</span></p>
      <div class="small-spacer"></div>
      <p>&nbsp; Transformer는 Attention을 활용한 뛰어난 성능을 바탕으로 다양한 분야의 딥러닝 모델의 표준이 되었습니다. 하지만 Standard Transformer는 여전히 개선될 여지가 많이 남아 있습니다. Transformer 모델 자체의 개선을 위한 연구는 상대적으로 부족하며, Transformer 변주 모델들에 대한 자연어 생성과제에서의 비교연구는 부재합니다.
      이 프로젝트에서는 앞서 언급한 문제를 직접 해결하고, Transformer의 모델 디자인적인 변주를 통해 어떠한 성능 변화를 이끌어낼수 있는지 직접 확인해봅니다.
      </p>
    </div>

    <div class="tblContents">
      <ul>

        <li>
          <a href="#1">1. &hairsp; Introduction</a>
        </li>

        <li>
          <a href="#2">2. &hairsp; Standard Transformer</a>
          <ul>
            <li>
              <a href="#2.1">2.1 &hairsp; Architecture</a>
            </li>
            <li>
              <a href="#2.2">2.2 &hairsp; Code Implementation</a>
            </li>                                    
          </ul>
        </li>        
        
        <li>
          <a href="#3">3. &hairsp; Recurrent Transformer</a>
          <ul>
            <li>
              <a href="#3.1">3.1 &hairsp; Architecture</a>
            </li>
            <li>
              <a href="#3.2">3.2 &hairsp; Code Implementation</a>
            </li>                                    
          </ul>
        </li>

        <li>
          <a href="#4">4. &hairsp; Evolved Transformer</a>
          <ul>
            <li>
              <a href="#4.1">4.1 &hairsp; Architecture</a>
            </li>
            <li>
              <a href="#4.2">4.2 &hairsp; Code Implementation</a>
            </li>                                    
          </ul>
        </li>        

        <li>
          <a href="#5">5. &hairsp; Experimental Setup</a>
        </li>

        <li>
          <a href="#6">6. &hairsp; Results</a>
        </li>

        <li>
          <a href="#7">7. &hairsp; Conclusion</a>
        </li>

        <li>
          <a href="#8">8. &hairsp; Reference</a>
        </li>

      </ul>
    </div>
    <hr>


<!-- 1. Introduction -->    
    <h2 id="1">1. &hairsp; Introduction</h2>
    <div class="chapter-box">

      <div class="list-container">
        <ul>
          <li>Description</li>
            <p> &nbsp; 순환 신경망은 Transformer의 도래 이전, 시퀀셜 데이터 처리를 위한 딥러닝 모델링의 대표적인 방식이었습니다. 비록 현재는 Transformer가 기존 순환 신경망의 자리를 성공적으로 대체했지만, 간단하고 직관적인 모델링이 가능하다는 점에서 여전히 활용 가치가 있습니다. 뿐만 아니라, 자연어 생성을 위한 가장 기초적인 모델 구현 방식으로써, 이후 다양한 모델의 성능을 비교할 수 있는 기준점으로 작용가능하기에, 순환 신경망에 대한 연구는 가장 기초적이면서 필수적인 연구라고 할 수 있습니다. <br>

            이 프로젝트에서는 대표적 순환 신경망 구조인 RNN, LSTM, GRU를 사용해 Sequence to Sequene 모델을 구현하고, 이를 기계 번역, 대화 생성, 문서 요약이라는 세 가지 자연어 생성과에서의 성능을 비교 분석합니다.
            </p>

          <br class="half-spacer">
          <li>Hypothesis
            <ul>
              <li style="font-weight: normal;">Evolved Transformer</li>
              <li style="font-weight: normal;">RNN, GRU, LSTM 순으로 빠른 학습</li>
              <li style="font-weight: normal;">LSTM이 특히 긴 텍스트를 다루는 문서 요약에서 강점을 지닐 것으로 예측</li>
              <li style="font-weight: normal;">상대적으로 시퀀스의 길이가 짧은 대화 생성에서는 LSTM이 약세를 보일 것으로 예측</li>
            </ul>
          </li>

          <br class="half-spacer">
          <li>Objective
            <ul>
              <li style="font-weight: normal;">Standard Transformer, Recurrent Transformer, Evolved Trasnformer 구현을 통한, Transformer 모델링 다양화 방식에 대한 깊은 이해</li>
              <li style="font-weight: normal;">Standard Transformer, Recurrent Transformer, Evolved Trasnformer 모델의 자연어 생성 능력 비교 검증</li>
              <li style="font-weight: normal;">가설에 대한 검증</li>
            </ul>
          </li>

        </ul>
      </div>

    </div>



<!-- Standard Transformer -->
    <h2 id="1">1. &hairsp; Standard Transformer</h2>
    
    <div class="chapter-box">
        <h3 id="1.1">1.1 &hairsp; Architecture</h3>
        <div class="dual-container">
            <img src="{{ 'assets/img/research_01/standard_transformer.png' | relative_url }}" style="width: 250px; height: 350px;">
            <p>
            &nbsp; 앞선 프로젝트에서는 RNN을 기반으로한 Sequence to Sequence 모델 구조의 효용성에 대해 면밀히 살펴봤습니다. 
            하지만 RNN 네트워크는 장기기억력이라는 관점에서 고질적인 문제점을 태생적으로 지니고 있으며, 이를 해결하기 위한 Attention 메커니즘의 활용 역시 기대만큼 좋은 성능을 보이지는 못했습니다.
            Transformer는 어텐션 메커니즘 만으로 문맥 전체적인 내용을 포착하는 뛰어난 성능과 더불어 병렬처리로 인한 학습의 효율성까지 제고시킨 뛰어난 모델 구조입니다.
            이 글에서는 Transformer의 구조에 대한 설명과 더불어, 두가지 방식에서의 Code Implementation 방식을 알아봅니다. 또한 이렇게 구현한 Transformer 모델이 세가지 자연어 생성 과제에서 어떤 성능을 보이는지 비교해봅니다.
            </p>
        </div>

        <br class="spacer">
        <br class="spacer">
        <h3 id="1.2">1.2 &hairsp; Code Implementation</h3>
        <div class="list-container">
          <ul>
            <li>Encoder</li>        
                <div class="code-container">
                  <div class="code-snippet">
                    <pre><code class="python">class StandardEncoder(nn.Module):
    def __init__(self, config):
        super(StandardEncoder, self).__init__()
        self.embeddings = Embeddings(config)
        layer = nn.TransformerEncoderLayer(
            d_model=config.hidden_dim,
            nhead=config.n_heads,
            dim_feedforward=config.pff_dim,
            dropout=config.dropout_ratio,
            activation='gelu',
            batch_first=True
        )
        self.layers = clones(layer, config.n_layers)

    def forward(self, x, e_mask):
        x = self.embeddings(x)
        for layer in self.layers:
            x = layer(x, src_key_padding_mask=e_mask)
        return x</code></pre>
                  </div>
                </div>

            <br class="half-spacer">
            <li>Decoder</li>        
                <div class="code-container">
                  <div class="code-snippet">
                    <pre><code class="python">class StandardDecoder(nn.Module):
    def __init__(self, config):
        super(StandardDecoder, self).__init__()
        self.embeddings = Embeddings(config)
        layer = nn.TransformerDecoderLayer(
            d_model=config.hidden_dim,
            nhead=config.n_heads,
            dim_feedforward=config.pff_dim,
            dropout=config.dropout_ratio,
            activation='gelu',
            batch_first=True
        )
        self.layers = clones(layer, config.n_layers)

    def forward(self, x, memory, e_mask, d_mask):
        x = self.embeddings(x)
        for layer in self.layers:
            x = layer(x, memory, memory_key_padding_mask=e_mask, tgt_mask=d_mask)
        return x</code></pre>
                  </div>
                </div>

          </ul>
        </div>
    </div>


<!-- Recurrent Transformer -->
    <h2 id="2">2. &hairsp; Recurrent Transformer</h2>
    <div class="chapter-box">
        <h3 id="2.1">2.1 &hairsp; Architecture</h3>
        <div class="dual-container">
            <img src="{{ 'assets/img/research_01/recurrent_transformer.png' | relative_url }}" style="width: 250px; height: 350px;">
            <p>
            &nbsp; 앞선 프로젝트에서는 RNN을 기반으로한 Sequence to Sequence 모델 구조의 효용성에 대해 면밀히 살펴봤습니다. 
            하지만 RNN 네트워크는 장기기억력이라는 관점에서 고질적인 문제점을 태생적으로 지니고 있으며, 이를 해결하기 위한 Attention 메커니즘의 활용 역시 기대만큼 좋은 성능을 보이지는 못했습니다.
            Transformer는 어텐션 메커니즘 만으로 문맥 전체적인 내용을 포착하는 뛰어난 성능과 더불어 병렬처리로 인한 학습의 효율성까지 제고시킨 뛰어난 모델 구조입니다.
            이 글에서는 Transformer의 구조에 대한 설명과 더불어, 두가지 방식에서의 Code Implementation 방식을 알아봅니다. 또한 이렇게 구현한 Transformer 모델이 세가지 자연어 생성 과제에서 어떤 성능을 보이는지 비교해봅니다.
            </p>
        </div>
    

        <h3 id="2.2">2.2 &hairsp; Code Implementation</h3>
        <div class="list-container">
          <ul>
            <li>Signal Generating Method</li>        
                <div class="code-container">
                  <div class="code-snippet">
                    <pre><code class="python">def generate_signal(length, channels, min_timescale=1.0, max_timescale=1.0e4):
    position = np.arange(length)
    num_timescales = channels // 2
    log_timescale_increment = ( math.log(float(max_timescale) / float(min_timescale)) / (float(num_timescales) - 1))
    inv_timescales = min_timescale * np.exp(np.arange(num_timescales).astype(np.float) * -log_timescale_increment)
    scaled_time = np.expand_dims(position, 1) * np.expand_dims(inv_timescales, 0)

    signal = np.concatenate([np.sin(scaled_time), np.cos(scaled_time)], axis=1)
    signal = np.pad(signal, [[0, 0], [0, channels % 2]], 'constant', constant_values=[0.0, 0.0])
    signal =  signal.reshape([1, length, channels])

    return torch.from_numpy(signal).type(torch.FloatTensor)</code></pre>
                  </div>
                </div>

          <div class="half-spacer"></div>
            <li>Encoder</li>        
                <div class="code-container">
                  <div class="code-snippet">
                    <pre><code class="python">class RecurrentEncoder(nn.Module):
    def __init__(self, config):
        super(RecurrentEncoder, self).__init__()    

        self.n_layers = config.n_layers
        self.norm = nn.LayerNorm(config.hidden_dim)
        self.embedding = nn.Embedding(config.emb_dim)
        
        self.time_signal = generate_signal(
            512, config.hidden_dim
        ).to(config.device)

        self.pos_signal = generate_signal(
            config.n_layers, config.hidden_dim
        ).to(config.device)
        
        self.layer = nn.TransformerEncoderLayer(
            d_model=config.hidden_dim,
            nhead=config.n_heads,
            dim_feedforward=config.pff_dim,
            dropout=config.dropout_ratio,
            batch_first=True
        )
        

    def forward(self, x, e_mask):
        x = self.embedding(x)
        seq_len = x.size(1)

        for l in range(self.n_layers):
            x += self.time_signal[:, :seq_len, :]
            x += self.pos_signal[:, l, :].unsqueeze(1).repeat(1, seq_len, 1)
            x = self.layer(x, src_key_padding_mask=e_mask)
        
        return self.norm(x)</code></pre>
                  </div>
                </div>

          <div class="half-spacer"></div>
            <li>Decoder</li>        
                <div class="code-container">
                  <div class="code-snippet">
                    <pre><code class="python">class RecurrentDecoder(nn.Module):
    def __init__(self, config):
        super(RecurrentDecoder, self).__init__()    

        self.n_layers = config.n_layers
        self.norm = nn.LayerNorm(config.hidden_dim)
        self.embedding = nn.Embedding(config.emb_dim)
        
        self.time_signal = generate_signal(
            512, config.hidden_dim
        ).to(config.device)
        
        self.pos_signal = generate_signal(
            config.n_layers, config.hidden_dim
        ).to(config.device)

        self.layer = nn.TransformerDecoderLayer(
            d_model=config.hidden_dim,
            nhead=config.n_heads,
            dim_feedforward=config.pff_dim,
            dropout=config.dropout_ratio,
            batch_first=True
        )



    def forward(self, x, m, e_mask, d_mask):
        x = self.embedding(x)
        seq_len = x.size(1)

        for l in range(self.n_layers):
            x += self.time_signal[:, :seq_len, :]
            x += self.pos_signal[:, l, :].unsqueeze(1).repeat(1, seq_len, 1)
            x = self.layer(
                tgt=x, memory=m,
                memory_key_padding_mask=e_mask, 
                tgt_mask=d_mask
            )

        return self.norm(x)</code></pre>
                  </div>
                </div>

          </ul>
        </div>
    </div>


<!-- Evolved Transformer -->
    <h2 id="3">3. &hairsp; Evolved Transformer</h2>
    <div class="chapter-box">
        <h3 id="3.1">3.1 &hairsp; Architecture</h3>
        <div class="dual-container">
            <img src="{{ 'assets/img/research_01/evolved_transformer.png' | relative_url }}" style="width: 250px; height: 350px;">
            <p>
            &nbsp; 앞선 프로젝트에서는 RNN을 기반으로한 Sequence to Sequence 모델 구조의 효용성에 대해 면밀히 살펴봤습니다. 
            하지만 RNN 네트워크는 장기기억력이라는 관점에서 고질적인 문제점을 태생적으로 지니고 있으며, 이를 해결하기 위한 Attention 메커니즘의 활용 역시 기대만큼 좋은 성능을 보이지는 못했습니다.
            Transformer는 어텐션 메커니즘 만으로 문맥 전체적인 내용을 포착하는 뛰어난 성능과 더불어 병렬처리로 인한 학습의 효율성까지 제고시킨 뛰어난 모델 구조입니다.
            이 글에서는 Transformer의 구조에 대한 설명과 더불어, 두가지 방식에서의 Code Implementation 방식을 알아봅니다. 또한 이렇게 구현한 Transformer 모델이 세가지 자연어 생성 과제에서 어떤 성능을 보이는지 비교해봅니다.
            </p>
        </div>
    

        <h3 id="3.2">3.2 &hairsp; Code Implementation</h3>
        <div class="list-container">
          <ul>
            <li>Gated Convolution & Separable Convolution</li>        
                <div class="code-container">
                  <div class="code-snippet">
                    <pre><code class="python">class GatedConvolution(nn.Module):
    def __init__(self, hidden_dim, kernel_size=3, padding=1):
        super(GatedConvolution,self).__init__()
        
        self.conv = nn.Conv1d(
            in_channels=hidden_dim, 
            out_channels=hidden_dim * 2,
            kernel_size=kernel_size, 
            padding=padding, bias=True
        )

        init.xavier_uniform_(self.conv.weight, gain=1)

    def forward(self,x):
        convoluted = self.conv(x.transpose(1,2)).transpose(1,2)
        out, gate = convoluted.split(int(convoluted.size(-1) / 2), -1)
        out = out * torch.sigmoid(gate)
        return out


class SeparableConv1D(nn.Module):
    def __init__(self, in_channels, out_channels, kernel_size):
        super(SeparableConv1D, self).__init__()

        self.depth_wise = nn.Conv1d(
            in_channels=in_channels,
            out_channels=in_channels,
            kernel_size=kernel_size,
            padding="same",
            groups=in_channels
        )
        
        self.point_wise = nn.Conv1d(
            in_channels=in_channels,
            out_channels=out_channels,
            kernel_size=1
        )

    def forward(self, x):
        out = self.depth_wise(x)
        out = self.point_wise(out)
        return out</code></pre>
                  </div>
                </div>


          <div class="half-spacer"></div>
            <li>Encoder Cell</li>        
                <div class="code-container">
                  <div class="code-snippet">
                    <pre><code class="python">class EncoderCell(nn.Module):
    def __init__(self, config):
        super(EncoderCell, self).__init__()

        self.pad_id = config.pad_id
        self.glu = GatedConvolution(config.hidden_dim)
        
        self.attention = nn.MultiheadAttention(
            config.hidden_dim, config.n_heads, batch_first=True
        )

        self.mid_layer_norm = nn.LayerNorm(config.pff_dim)
        self.layer_norms = nn.ModuleList([nn.LayerNorm(config.hidden_dim) for _ in range(4)])

        self.left_net = nn.Sequential(
            nn.Linear(config.hidden_dim, config.pff_dim),
            nn.ReLU(),
            nn.Dropout(config.dropout_ratio)
        )

        self.right_net = nn.Sequential(
            nn.Conv1d(in_channels=config.hidden_dim, 
                      out_channels=config.hidden_dim//2, 
                      kernel_size=3, padding=1),
            nn.ReLU(),
            nn.Dropout(config.dropout_ratio)
        )

        self.sep_conv = SeparableConv1D(
            config.pff_dim, config.hidden_dim // 2, 9
        )

        self.pff = nn.Sequential(
            nn.Linear(config.hidden_dim, config.pff_dim),
            nn.SiLU(),
            nn.Linear(config.pff_dim, config.hidden_dim)
        )


    def forward(self, x, e_mask):
        ### Block_01
        B01_out = self.glu(self.layer_norms[0](x)) #Dim:512


        ### Block_02
        B02_normed = self.layer_norms[1](B01_out)        

        left_out = self.left_net(B02_normed)
        right_out = self.right_net(B02_normed.transpose(1, 2)).transpose(1, 2)

        right_out = F.pad(
            input=right_out, 
            pad=(0, left_out.size(-1) - right_out.size(-1), 0,0,0,0), 
            mode='constant', value=self.pad_id
        ) #Dim:2048          

        B02_out = left_out + right_out


        ### Block_03
        B03_out = self.mid_layer_norm(B02_out)
        
        B03_out = self.sep_conv(
            B03_out.transpose(1, 2)
        ).transpose(1, 2) #Dim:256
        
        B03_out = F.pad(
            input=B03_out,
            pad=(0, B01_out.size(-1) - B03_out.size(-1), 0, 0, 0, 0),
            mode='constant', value=self.pad_id
        )
        
        B03_out += B01_out #Dim:512


        ### Block_04
        B04_out = self.layer_norms[2](B03_out)
        
        attention_out = self.attention(
            B04_out, B04_out, B04_out,
            key_padding_mask = e_mask,
            need_weights=False
        )[0]
        
        B04_out += attention_out #Dim:512


        ### Block_05 & 06
        out = self.layer_norms[3](B04_out)
        out = self.pff(out) + B04_out #Dim:512
        return out </code></pre>
                  </div>
                </div>

          <div class="half-spacer"></div>
            <li>Decoder Cell</li>        
                <div class="code-container">
                  <div class="code-snippet">
                    <pre><code class="python">class DecoderCell(nn.Module):
    def __init__(self, config):
        super(DecoderCell, self).__init__()
        
        self.pad_id = config.pad_id
        self.dropout = nn.Dropout(config.dropout_ratio)

        self.attention = nn.MultiheadAttention(
            config.hidden_dim, config.n_heads
        )

        self.mid_layer_norm = nn.LayerNorm(config.hidden_dim * 2)
        
        self.layer_norms = nn.ModuleList(
            [nn.LayerNorm(config.hidden_dim) for _ in range(5)]
        )        

        self.left_attn = nn.MultiheadAttention(
            config.hidden_dim, config.n_heads * 2, batch_first=True
        )

        self.right_attn = nn.MultiheadAttention(
            config.hidden_dim, config.n_heads, batch_first=True
        )

        self.left_net = nn.Sequential(
            SeparableConv1D(config.hidden_dim, config.hidden_dim * 2, 11), 
            nn.ReLU()
        )
        
        self.right_net = SeparableConv1D(
            config.hidden_dim, config.hidden_dim // 2, 7
        )
        
        self.sep_conv = SeparableConv1D(
            config.hidden_dim * 2, config.hidden_dim, 7
        )


        self.self_attn = nn.MultiheadAttention(
            config.hidden_dim, config.n_heads * 2, batch_first=True
        )

        self.src_attn = nn.MultiheadAttention(
            config.hidden_dim, config.n_heads, batch_first=True
        )

        self.pff = nn.Sequential(
            nn.Linear(config.hidden_dim, config.pff_dim),
            nn.ReLU(),
            nn.Linear(config.pff_dim, config.hidden_dim)
        )


    def forward(self, x, memory, e_mask, d_mask):

        ### Block_01
        B01_out = self.layer_norms[0](x)

        left_out = self.left_attn(
            B01_out, B01_out, B01_out,
            attn_mask=d_mask,
            need_weights=False
        )[0]

        right_out = self.right_attn(
            B01_out, B01_out, B01_out,
            attn_mask=d_mask,
            need_weights=False
        )[0]

        B01_out = left_out + right_out


        ### Block_02
        B02_out = self.layer_norms[1](B01_out)
        left_out = self.left_net(B02_out.transpose(1, 2)).transpose(1, 2)
        right_out = self.right_net(B02_out.transpose(1, 2)).transpose(1, 2)

        right_out = F.pad(
            input=right_out, 
            pad=(0, left_out.size(-1) - right_out.size(-1), 0,0,0,0), 
            mode='constant', value=self.pad_id
        ) #Dim:1024
                             
        B02_out = left_out + right_out #Dim: 1024

        ### Block_03
        B03_out = self.mid_layer_norm(B02_out)
        B03_out = self.sep_conv(B03_out.transpose(1, 2)).transpose(1, 2)
        B03_out += B01_out


        ### Block_04
        B04_out = self.layer_norms[2](B03_out)
        
        B04_out = self.self_attn(
            B04_out, B04_out, B04_out,
            attn_mask=d_mask,
            need_weights=False
        )[0]

        B04_out += B03_out


        ### Block_05
        B05_out = self.layer_norms[3](B04_out)
        
        B05_out = self.src_attn(
            B05_out, memory, memory,
            key_padding_mask=e_mask,
            need_weights=False
        )[0]

        B05_out += B04_out        


        ### Block_06 & Block_07
        out = self.layer_norms[4](B05_out)
        out = self.pff(out) + B05_out #Dim:512

        return out</code></pre>
                  </div>
                </div>

          <div class="half-spacer"></div>
            <li>Encoder & Decoder</li>        
                <div class="code-container">
                  <div class="code-snippet">
                    <pre><code class="python">class EvolvedEncoder(nn.Module):
    def __init__(self, config):
        super(EvolvedEncoder, self).__init__()

        self.embeddings = Embeddings(config)
        self.cells = clones(EncoderCell(config), config.n_layers//2)


    def forward(self, x, e_mask):
        x = self.embeddings(x)
        for cell in self.cells:
            x = cell(x, src_key_padding_mask=e_mask)
        return x



class EvolvedDecoder(nn.Module):
    def __init__(self, config):
        super(EvolvedDecoder, self).__init__()

        self.embeddings = Embeddings(config)
        self.cells = clones(DecoderCell(config), config.n_layers//2)


    def forward(self, x, memory, e_mask, d_mask):
        x = self.embeddings(x)
        for cell in self.cells:
            x = cell(x, memory, e_mask, d_mask)

        return x</code></pre>
                  </div>
                </div>


          </ul>
        </div>
    </div>


<!-- 4. Experimental Setup -->
    <h2 id="4">4. &hairsp; Experimental Setup</h2>
    <div class="chapter-box">

      <div class="list-container">
        <ul>
          <li>Data Setup
            <div class="dual-container">
              <ul>
                <li><b>Translation Task</b>: &nbsp; WMT'14 En-De</li>
                <li><b>Dialogue Task</b>: &nbsp; Daily Dialogue</li>
                <li><b>Summarization Task</b>: &nbsp; CNN Daily</li>
                <li><b>Tokenizer</b>: &nbsp; Word-Level Tokenizer</li>
              </ul>
              <ul>
                <li><b>Train Data Volumn</b>: &nbsp; 50,000</li>
                <li><b>Valid Data Volumn</b>: &nbsp; 5,000</li>
                <li><b>Test Data Volumn</b>: &nbsp; 100</li>
                <li><b>Vocab Size</b>: &nbsp; 10,000</li>                
              </ul>
            </div>
          </li>

          <br class="half-spacer">
          <li>Model Setup
            <div class="dual-container">
              <ul>
                <li><b>Input Dim</b>: &nbsp; 10,000</li>
                <li><b>Output Dim</b>: &nbsp; 10,000</li>
                <li><b>Embedding Dim</b>: &nbsp; 512</li>
              </ul>
              <ul>
                <li><b>Hidden Dim</b>: &nbsp; 512</li>
                <li><b>Model Params</b>: &nbsp; 000</li>
                <li><b>Model Size</b>: &nbsp; 000</li>                
              </ul>
            </div>
          </li>

          <br class="half-spacer">
          <li>Training Setup
            <div class="dual-container">
              <ul>
                <li><b>Num Epochs</b>: &nbsp; 10</li>
                <li><b>Batch Size</b>: &nbsp; 32</li>
                <li><b>Learning Rate</b>: &nbsp; 5e-4</li>
                <li><b>LR Scheduler</b>: &nbsp; pleature</li>
              </ul>
              <ul>
                <li><b>Optimizer</b>: &nbsp; AdamW</li>
                <li><b>Gradient Accumulation Steps</b>: &nbsp; 4
                <li><b>Teacher Forcing Ratio</b>: &nbsp; 0.5</li>                
              </ul>
            </div>
          </li>
        </ul>
      </div>
    </div>



<!-- 5. Result -->
    <h2 id="5">5. &hairsp; Result</h2>
    <div class="chapter-box">

      <div class="list-container">
        <ul>
          <li>Machine Translation</li>

            <table class="result-table">
              <thead>
                <tr>
                  <th>Model Type</th>
                  <th>Eval Score</th>
                  <th>Epoch Time</th>
                  <th>Avg GPU</th>
                  <th>Max GPU</th>
                </tr>
              </thead>
              <tbody>
                <tr>
                  <td>RNN Model</td>
                  <td>2.12</td>
                  <td>3m 58s</td>
                  <td>0.22GB</td>
                  <td>0.82GB</td>
                </tr>
                <tr>
                  <td>LSTM Model</td>
                  <td>8.35</td>
                  <td>4m 16s</td>
                  <td>0.27GB</td>
                  <td>1.30GB</td>
                </tr>
                <tr>
                  <td>GRU Model</td>
                  <td>9.75</td>
                  <td>4m 6s</td>
                  <td>0.26GB</td>
                  <td>1.16GB</td>
                </tr>

              </tbody>
            </table>
          
          <div class="half-spacer"></div>
          <li>Dialogue Generation</li>
            <table class="result-table">
              <thead>
                <tr>
                  <th>Model Type</th>
                  <th>Eval Score</th>
                  <th>Epoch Time</th>
                  <th>Avg GPU</th>
                  <th>Max GPU</th>
                </tr>
              </thead>
              <tbody>
                <tr>
                  <td>RNN Model</td>
                  <td>0.10</td>
                  <td>3m 40s</td>
                  <td>0.22GB</td>
                  <td>0.79GB</td>
                </tr>
                <tr>
                  <td>LSTM Model</td>
                  <td>0.37</td>
                  <td>4m 5s</td>
                  <td>0.27GB</td>
                  <td>1.23GB</td>
                </tr>
                <tr>
                  <td>GRU Model</td>
                  <td>2.16</td>
                  <td>3m 58s</td>
                  <td>0.26GB</td>
                  <td>1.11GB</td>
                </tr>

              </tbody>
            </table>

                
          <div class="half-spacer"></div>
          <li>Text Summarization</li>
            <table class="result-table">
              <thead>
                <tr>
                  <th>Model Type</th>
                  <th>Eval Score</th>
                  <th>Epoch Time</th>
                  <th>Avg GPU</th>
                  <th>Max GPU</th>
                </tr>
              </thead>
              <tbody>
                <tr>
                  <td>RNN Model</td>
                  <td>0.00</td>
                  <td>8m 6s</td>
                  <td>0.23GB</td>
                  <td>1.20GB</td>
                </tr>
                <tr>
                  <td>LSTM Model</td>
                  <td>2.23</td>
                  <td>9m 7s</td>
                  <td>0.29GB</td>
                  <td>2.00GB</td>
                </tr>
                <tr>
                  <td>GRU Model</td>
                  <td>2.19</td>
                  <td>8m 42s</td>
                  <td>0.27GB</td>
                  <td>1.82GB</td>
                </tr>

              </tbody>
            </table>

          <br class="half-spacer">  
          <li>Result Analysis</li>
            <p> &nbsp; RNN은 다양한 정보를 함유하기에 지나치게 단순하고, LSTM은 GATE구조가 복잡하기에, 그만큼 영향을 받은 요소가 많아 학습 수렴이 어렵습니다. 때문에 모든 학습 결과에서는 GATE를 사용하면서도 단순화된 연산으로 학습의 이점이 큰 GRU가 가장 뛰어난 성능을 보임을 알 수 있습니다. 
            </p>
        </ul>
      </div>
    </div>


<!-- Conclusion -->   
    <h2 id="6">6. &hairsp; Conclusion</h2>
    <div class="chapter-box">
        <p> &nbsp; 이전의 RNN 계열의 모델들 보다 속도 측면, 그리고 성능 측면에서 굉장히 진일보 했음을 실험을 통해 직접적으로 확인할 수 있었음. 구현과정에서 복잡성이 증가 했다는 단점역시, Pytorch 라이브러리의 Transformer 모델을 통해 간편하게 사용할 수 있는 편의성이 있음.

        </p>  
    </div>


<!-- Reference -->   
    <h2 id="7">5. &hairsp; Reference</h2>
    <div class="chapter-box">
        <a class="reference" href="https://arxiv.org/abs/1706.03762">&nbsp; Attention Is All You Need</a> <br> 
        <a class="reference" href="https://arxiv.org/abs/1807.03819/">&nbsp; Universal Transformers</a> <br> 
        <a class="reference" href="https://arxiv.org/abs/1901.11117">&nbsp; The Evolved Transformers</a>
    </div>

  </div>
</div>


<div class="pagination">
  <a href="{{ '/transformer_balance' | relative_url }}" class="btn-prev"><span>Transformer Balance</span></a>
  <a href="{{ '/plm_fusion' | relative_url }}" class="btn-next"><span>PLM Fusion</span></a>
</div>