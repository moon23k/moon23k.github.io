---
layout: default
permalink: /transformer_variants
---


<div id="detail">
  <h1 class="title">Transformer Variants Comparison Study</h1>  
  <div class="post">

    <hr>
    <div class="callout">
      <p class="highlighted-text"><span>Project Objective</span></p>
      <div class="small-spacer"></div>
      <p>&nbsp; Transformer는 Attention을 활용한 뛰어난 성능을 바탕으로 다양한 분야의 딥러닝 모델의 표준이 되었습니다. 하지만 Standard Transformer는 여전히 개선될 여지가 많이 남아 있습니다. Transformer 모델 자체의 개선을 위한 연구는 상대적으로 부족하며, Transformer 변주 모델들에 대한 자연어 생성과제에서의 비교연구는 부재합니다.
      이 프로젝트에서는 앞서 언급한 문제를 직접 해결하고, Transformer의 모델 디자인적인 변주를 통해 어떠한 성능 변화를 이끌어낼수 있는지 직접 확인해봅니다.
      </p>
    </div>

    <div class="tblContents">
      <ul>

        <li>
          <a href="#1">1. Standard Transformer</a>
          <ul>
            <li>
              <a href="#1.1">1.1 Architecture</a>
            </li>
            <li>
              <a href="#1.2">1.2 Code Implementation</a>
            </li>                                    
          </ul>
        </li>        
        
        <li>
          <a href="#2">2. Recurrent Transformer</a>
          <ul>
            <li>
              <a href="#2.1">2.1 Architecture</a>
            </li>
            <li>
              <a href="#2.2">2.2 Code Implementation</a>
            </li>                                    
          </ul>
        </li>

        <li>
          <a href="#3">3. Evolved Transformer</a>
          <ul>
            <li>
              <a href="#3.1">2.1 Architecture</a>
            </li>
            <li>
              <a href="#3.2">2.2 Code Implementation</a>
            </li>                                    
          </ul>
        </li>        

        <li>
          <a href="#4">4. Experimental Setup</a>
        </li>

        <li>
          <a href="#5">5. Results</a>
        </li>

        <li>
          <a href="#6">6. Conclusion</a>
        </li>

        <li>
          <a href="#7">7. Reference</a>
        </li>

      </ul>
    </div>
    <hr>


<!-- Standard Transformer -->
    <h2 id="1">1. &hairsp; Standard Transformer</h2>
    <h3 id="1.1">1.1 &hairsp; Architecture</h3>
    <div class="dual-container">
        <img src="{{ 'assets/img/research_01/standard_transformer.png' | relative_url }}" style="width: 250px; height: 350px;">
        <p>
        &nbsp; 앞선 프로젝트에서는 RNN을 기반으로한 Sequence to Sequence 모델 구조의 효용성에 대해 면밀히 살펴봤습니다. 
        하지만 RNN 네트워크는 장기기억력이라는 관점에서 고질적인 문제점을 태생적으로 지니고 있으며, 이를 해결하기 위한 Attention 메커니즘의 활용 역시 기대만큼 좋은 성능을 보이지는 못했습니다.
        Transformer는 어텐션 메커니즘 만으로 문맥 전체적인 내용을 포착하는 뛰어난 성능과 더불어 병렬처리로 인한 학습의 효율성까지 제고시킨 뛰어난 모델 구조입니다.
        이 글에서는 Transformer의 구조에 대한 설명과 더불어, 두가지 방식에서의 Code Implementation 방식을 알아봅니다. 또한 이렇게 구현한 Transformer 모델이 세가지 자연어 생성 과제에서 어떤 성능을 보이는지 비교해봅니다.
        </p>
    </div>
    <hr>

    <h3 id="1.2">1.2 &hairsp; Code Implementation</h3>
    <div class="list-container">
      <ul>
        <li>Encoder</li>        
            <div class="code-container">
              <div class="code-snippet">
                <pre><code class="python">class StandardEncoder(nn.Module):
    def __init__(self, config):
        super(StandardEncoder, self).__init__()
        self.embeddings = Embeddings(config)
        layer = nn.TransformerEncoderLayer(
            d_model=config.hidden_dim,
            nhead=config.n_heads,
            dim_feedforward=config.pff_dim,
            dropout=config.dropout_ratio,
            activation='gelu',
            batch_first=True
        )
        self.layers = clones(layer, config.n_layers)

    def forward(self, x, e_mask):
        x = self.embeddings(x)
        for layer in self.layers:
            x = layer(x, src_key_padding_mask=e_mask)
        return x</code></pre>
              </div>
            </div>

      <div class="half-spacer"></div>
        <li>Decoder</li>        
            <div class="code-container">
              <div class="code-snippet">
                <pre><code class="python">class StandardDecoder(nn.Module):
    def __init__(self, config):
        super(StandardDecoder, self).__init__()
        self.embeddings = Embeddings(config)
        layer = nn.TransformerDecoderLayer(
            d_model=config.hidden_dim,
            nhead=config.n_heads,
            dim_feedforward=config.pff_dim,
            dropout=config.dropout_ratio,
            activation='gelu',
            batch_first=True
        )
        self.layers = clones(layer, config.n_layers)

    def forward(self, x, memory, e_mask, d_mask):
        x = self.embeddings(x)
        for layer in self.layers:
            x = layer(x, memory, memory_key_padding_mask=e_mask, tgt_mask=d_mask)
        return x</code></pre>
              </div>
            </div>

      </ul>
    </div>
    <hr>


<!-- Recurrent Transformer -->
    <h2 id="2">2. &hairsp; Recurrent Transformer</h2>
    <h3 id="2.1">2.1 &hairsp; Architecture</h3>
    <div class="dual-container">
        <img src="{{ 'assets/img/research_01/recurrent_transformer.png' | relative_url }}" style="width: 250px; height: 350px;">
        <p>
        &nbsp; 앞선 프로젝트에서는 RNN을 기반으로한 Sequence to Sequence 모델 구조의 효용성에 대해 면밀히 살펴봤습니다. 
        하지만 RNN 네트워크는 장기기억력이라는 관점에서 고질적인 문제점을 태생적으로 지니고 있으며, 이를 해결하기 위한 Attention 메커니즘의 활용 역시 기대만큼 좋은 성능을 보이지는 못했습니다.
        Transformer는 어텐션 메커니즘 만으로 문맥 전체적인 내용을 포착하는 뛰어난 성능과 더불어 병렬처리로 인한 학습의 효율성까지 제고시킨 뛰어난 모델 구조입니다.
        이 글에서는 Transformer의 구조에 대한 설명과 더불어, 두가지 방식에서의 Code Implementation 방식을 알아봅니다. 또한 이렇게 구현한 Transformer 모델이 세가지 자연어 생성 과제에서 어떤 성능을 보이는지 비교해봅니다.
        </p>
    </div>
    <hr>

    <h3 id="2.2">2.2 &hairsp; Code Implementation</h3>
    <div class="list-container">
      <ul>
        <li>Signal Generating Method</li>        
            <div class="code-container">
              <div class="code-snippet">
                <pre><code class="python">def generate_signal(length, channels, min_timescale=1.0, max_timescale=1.0e4):
    position = np.arange(length)
    num_timescales = channels // 2
    log_timescale_increment = ( math.log(float(max_timescale) / float(min_timescale)) / (float(num_timescales) - 1))
    inv_timescales = min_timescale * np.exp(np.arange(num_timescales).astype(np.float) * -log_timescale_increment)
    scaled_time = np.expand_dims(position, 1) * np.expand_dims(inv_timescales, 0)

    signal = np.concatenate([np.sin(scaled_time), np.cos(scaled_time)], axis=1)
    signal = np.pad(signal, [[0, 0], [0, channels % 2]], 'constant', constant_values=[0.0, 0.0])
    signal =  signal.reshape([1, length, channels])

    return torch.from_numpy(signal).type(torch.FloatTensor)</code></pre>
              </div>
            </div>

      <div class="half-spacer"></div>
        <li>Encoder</li>        
            <div class="code-container">
              <div class="code-snippet">
                <pre><code class="python">class RecurrentEncoder(nn.Module):
    def __init__(self, config):
        super(RecurrentEncoder, self).__init__()    

        self.n_layers = config.n_layers
        self.norm = nn.LayerNorm(config.hidden_dim)
        self.embedding = nn.Embedding(config.emb_dim)
        
        self.time_signal = generate_signal(
            512, config.hidden_dim
        ).to(config.device)

        self.pos_signal = generate_signal(
            config.n_layers, config.hidden_dim
        ).to(config.device)
        
        self.layer = nn.TransformerEncoderLayer(
            d_model=config.hidden_dim,
            nhead=config.n_heads,
            dim_feedforward=config.pff_dim,
            dropout=config.dropout_ratio,
            batch_first=True
        )
        

    def forward(self, x, e_mask):
        x = self.embedding(x)
        seq_len = x.size(1)

        for l in range(self.n_layers):
            x += self.time_signal[:, :seq_len, :]
            x += self.pos_signal[:, l, :].unsqueeze(1).repeat(1, seq_len, 1)
            x = self.layer(x, src_key_padding_mask=e_mask)
        
        return self.norm(x)</code></pre>
              </div>
            </div>

      <div class="half-spacer"></div>
        <li>Decoder</li>        
            <div class="code-container">
              <div class="code-snippet">
                <pre><code class="python">class RecurrentDecoder(nn.Module):
    def __init__(self, config):
        super(RecurrentDecoder, self).__init__()    

        self.n_layers = config.n_layers
        self.norm = nn.LayerNorm(config.hidden_dim)
        self.embedding = nn.Embedding(config.emb_dim)
        
        self.time_signal = generate_signal(
            512, config.hidden_dim
        ).to(config.device)
        
        self.pos_signal = generate_signal(
            config.n_layers, config.hidden_dim
        ).to(config.device)

        self.layer = nn.TransformerDecoderLayer(
            d_model=config.hidden_dim,
            nhead=config.n_heads,
            dim_feedforward=config.pff_dim,
            dropout=config.dropout_ratio,
            batch_first=True
        )



    def forward(self, x, m, e_mask, d_mask):
        x = self.embedding(x)
        seq_len = x.size(1)

        for l in range(self.n_layers):
            x += self.time_signal[:, :seq_len, :]
            x += self.pos_signal[:, l, :].unsqueeze(1).repeat(1, seq_len, 1)
            x = self.layer(
                tgt=x, memory=m,
                memory_key_padding_mask=e_mask, 
                tgt_mask=d_mask
            )

        return self.norm(x)</code></pre>
              </div>
            </div>

      </ul>
    </div>
    <hr>


<!-- Evolved Transformer -->
    <h2 id="3">3. &hairsp; Evolved Transformer</h2>
    <h3 id="3.1">3.1 &hairsp; Architecture</h3>
    <div class="dual-container">
        <img src="{{ 'assets/img/research_01/evolved_transformer.png' | relative_url }}" style="width: 250px; height: 350px;">
        <p>
        &nbsp; 앞선 프로젝트에서는 RNN을 기반으로한 Sequence to Sequence 모델 구조의 효용성에 대해 면밀히 살펴봤습니다. 
        하지만 RNN 네트워크는 장기기억력이라는 관점에서 고질적인 문제점을 태생적으로 지니고 있으며, 이를 해결하기 위한 Attention 메커니즘의 활용 역시 기대만큼 좋은 성능을 보이지는 못했습니다.
        Transformer는 어텐션 메커니즘 만으로 문맥 전체적인 내용을 포착하는 뛰어난 성능과 더불어 병렬처리로 인한 학습의 효율성까지 제고시킨 뛰어난 모델 구조입니다.
        이 글에서는 Transformer의 구조에 대한 설명과 더불어, 두가지 방식에서의 Code Implementation 방식을 알아봅니다. 또한 이렇게 구현한 Transformer 모델이 세가지 자연어 생성 과제에서 어떤 성능을 보이는지 비교해봅니다.
        </p>
    </div>
    <hr>

    <h3 id="3.2">3.2 &hairsp; Code Implementation</h3>
    <div class="list-container">
      <ul>
        <li>Gated Convolution & Separable Convolution</li>        
            <div class="code-container">
              <div class="code-snippet">
                <pre><code class="python">class GatedConvolution(nn.Module):
    def __init__(self, hidden_dim, kernel_size=3, padding=1):
        super(GatedConvolution,self).__init__()
        
        self.conv = nn.Conv1d(
            in_channels=hidden_dim, 
            out_channels=hidden_dim * 2,
            kernel_size=kernel_size, 
            padding=padding, bias=True
        )

        init.xavier_uniform_(self.conv.weight, gain=1)

    def forward(self,x):
        convoluted = self.conv(x.transpose(1,2)).transpose(1,2)
        out, gate = convoluted.split(int(convoluted.size(-1) / 2), -1)
        out = out * torch.sigmoid(gate)
        return out


class SeparableConv1D(nn.Module):
    def __init__(self, in_channels, out_channels, kernel_size):
        super(SeparableConv1D, self).__init__()

        self.depth_wise = nn.Conv1d(
            in_channels=in_channels,
            out_channels=in_channels,
            kernel_size=kernel_size,
            padding="same",
            groups=in_channels
        )
        
        self.point_wise = nn.Conv1d(
            in_channels=in_channels,
            out_channels=out_channels,
            kernel_size=1
        )

    def forward(self, x):
        out = self.depth_wise(x)
        out = self.point_wise(out)
        return out</code></pre>
              </div>
            </div>


      <div class="half-spacer"></div>
        <li>Encoder Cell</li>        
            <div class="code-container">
              <div class="code-snippet">
                <pre><code class="python">class EncoderCell(nn.Module):
    def __init__(self, config):
        super(EncoderCell, self).__init__()

        self.pad_id = config.pad_id
        self.glu = GatedConvolution(config.hidden_dim)
        
        self.attention = nn.MultiheadAttention(
            config.hidden_dim, config.n_heads, batch_first=True
        )

        self.mid_layer_norm = nn.LayerNorm(config.pff_dim)
        self.layer_norms = nn.ModuleList([nn.LayerNorm(config.hidden_dim) for _ in range(4)])

        self.left_net = nn.Sequential(
            nn.Linear(config.hidden_dim, config.pff_dim),
            nn.ReLU(),
            nn.Dropout(config.dropout_ratio)
        )

        self.right_net = nn.Sequential(
            nn.Conv1d(in_channels=config.hidden_dim, 
                      out_channels=config.hidden_dim//2, 
                      kernel_size=3, padding=1),
            nn.ReLU(),
            nn.Dropout(config.dropout_ratio)
        )

        self.sep_conv = SeparableConv1D(
            config.pff_dim, config.hidden_dim // 2, 9
        )

        self.pff = nn.Sequential(
            nn.Linear(config.hidden_dim, config.pff_dim),
            nn.SiLU(),
            nn.Linear(config.pff_dim, config.hidden_dim)
        )


    def forward(self, x, e_mask):
        ### Block_01
        B01_out = self.glu(self.layer_norms[0](x)) #Dim:512


        ### Block_02
        B02_normed = self.layer_norms[1](B01_out)        

        left_out = self.left_net(B02_normed)
        right_out = self.right_net(B02_normed.transpose(1, 2)).transpose(1, 2)

        right_out = F.pad(
            input=right_out, 
            pad=(0, left_out.size(-1) - right_out.size(-1), 0,0,0,0), 
            mode='constant', value=self.pad_id
        ) #Dim:2048          

        B02_out = left_out + right_out


        ### Block_03
        B03_out = self.mid_layer_norm(B02_out)
        
        B03_out = self.sep_conv(
            B03_out.transpose(1, 2)
        ).transpose(1, 2) #Dim:256
        
        B03_out = F.pad(
            input=B03_out,
            pad=(0, B01_out.size(-1) - B03_out.size(-1), 0, 0, 0, 0),
            mode='constant', value=self.pad_id
        )
        
        B03_out += B01_out #Dim:512


        ### Block_04
        B04_out = self.layer_norms[2](B03_out)
        
        attention_out = self.attention(
            B04_out, B04_out, B04_out,
            key_padding_mask = e_mask,
            need_weights=False
        )[0]
        
        B04_out += attention_out #Dim:512


        ### Block_05 & 06
        out = self.layer_norms[3](B04_out)
        out = self.pff(out) + B04_out #Dim:512
        return out </code></pre>
              </div>
            </div>

      <div class="half-spacer"></div>
        <li>Decoder Cell</li>        
            <div class="code-container">
              <div class="code-snippet">
                <pre><code class="python">class DecoderCell(nn.Module):
    def __init__(self, config):
        super(DecoderCell, self).__init__()
        
        self.pad_id = config.pad_id
        self.dropout = nn.Dropout(config.dropout_ratio)

        self.attention = nn.MultiheadAttention(
            config.hidden_dim, config.n_heads
        )

        self.mid_layer_norm = nn.LayerNorm(config.hidden_dim * 2)
        
        self.layer_norms = nn.ModuleList(
            [nn.LayerNorm(config.hidden_dim) for _ in range(5)]
        )        

        self.left_attn = nn.MultiheadAttention(
            config.hidden_dim, config.n_heads * 2, batch_first=True
        )

        self.right_attn = nn.MultiheadAttention(
            config.hidden_dim, config.n_heads, batch_first=True
        )

        self.left_net = nn.Sequential(
            SeparableConv1D(config.hidden_dim, config.hidden_dim * 2, 11), 
            nn.ReLU()
        )
        
        self.right_net = SeparableConv1D(
            config.hidden_dim, config.hidden_dim // 2, 7
        )
        
        self.sep_conv = SeparableConv1D(
            config.hidden_dim * 2, config.hidden_dim, 7
        )


        self.self_attn = nn.MultiheadAttention(
            config.hidden_dim, config.n_heads * 2, batch_first=True
        )

        self.src_attn = nn.MultiheadAttention(
            config.hidden_dim, config.n_heads, batch_first=True
        )

        self.pff = nn.Sequential(
            nn.Linear(config.hidden_dim, config.pff_dim),
            nn.ReLU(),
            nn.Linear(config.pff_dim, config.hidden_dim)
        )


    def forward(self, x, memory, e_mask, d_mask):

        ### Block_01
        B01_out = self.layer_norms[0](x)

        left_out = self.left_attn(
            B01_out, B01_out, B01_out,
            attn_mask=d_mask,
            need_weights=False
        )[0]

        right_out = self.right_attn(
            B01_out, B01_out, B01_out,
            attn_mask=d_mask,
            need_weights=False
        )[0]

        B01_out = left_out + right_out


        ### Block_02
        B02_out = self.layer_norms[1](B01_out)
        left_out = self.left_net(B02_out.transpose(1, 2)).transpose(1, 2)
        right_out = self.right_net(B02_out.transpose(1, 2)).transpose(1, 2)

        right_out = F.pad(
            input=right_out, 
            pad=(0, left_out.size(-1) - right_out.size(-1), 0,0,0,0), 
            mode='constant', value=self.pad_id
        ) #Dim:1024
                             
        B02_out = left_out + right_out #Dim: 1024

        ### Block_03
        B03_out = self.mid_layer_norm(B02_out)
        B03_out = self.sep_conv(B03_out.transpose(1, 2)).transpose(1, 2)
        B03_out += B01_out


        ### Block_04
        B04_out = self.layer_norms[2](B03_out)
        
        B04_out = self.self_attn(
            B04_out, B04_out, B04_out,
            attn_mask=d_mask,
            need_weights=False
        )[0]

        B04_out += B03_out


        ### Block_05
        B05_out = self.layer_norms[3](B04_out)
        
        B05_out = self.src_attn(
            B05_out, memory, memory,
            key_padding_mask=e_mask,
            need_weights=False
        )[0]

        B05_out += B04_out        


        ### Block_06 & Block_07
        out = self.layer_norms[4](B05_out)
        out = self.pff(out) + B05_out #Dim:512

        return out</code></pre>
              </div>
            </div>

      <div class="half-spacer"></div>
        <li>Encoder & Decoder</li>        
            <div class="code-container">
              <div class="code-snippet">
                <pre><code class="python">class EvolvedEncoder(nn.Module):
    def __init__(self, config):
        super(EvolvedEncoder, self).__init__()

        self.embeddings = Embeddings(config)
        self.cells = clones(EncoderCell(config), config.n_layers//2)


    def forward(self, x, e_mask):
        x = self.embeddings(x)
        for cell in self.cells:
            x = cell(x, src_key_padding_mask=e_mask)
        return x



class EvolvedDecoder(nn.Module):
    def __init__(self, config):
        super(EvolvedDecoder, self).__init__()

        self.embeddings = Embeddings(config)
        self.cells = clones(DecoderCell(config), config.n_layers//2)


    def forward(self, x, memory, e_mask, d_mask):
        x = self.embeddings(x)
        for cell in self.cells:
            x = cell(x, memory, e_mask, d_mask)

        return x</code></pre>
              </div>
            </div>


      </ul>
    </div>
    <hr>


<!-- Experimental Setup -->
    <h2 id="4">4. &hairsp; Experimental Setup</h2>

    <div class="half-spacer"></div>
    <table class="result-table">
      <thead>
        <tr>
          <th>Data Setup</th>
          <th>Model Setup</th>
          <th>Training Setup</th>
        </tr>
      </thead>
      <tbody>
        <tr>

          <td>
            <ul>
              <li><b>Translation Task</b>: &nbsp; WMT'14 En-De</li>
              <li><b>Dialogue Task</b>: &nbsp; Daily Dialogue</li>
              <li><b>Summarization Task</b>: &nbsp; CNN Daily</li>
              <li><b>Train Data Volumn</b>: &nbsp; 50,000</li>
              <li><b>Valid Data Volumn</b>: &nbsp; 5,000</li>
              <li><b>Test Data Volumn</b>: &nbsp; 100</li>
              <li><b>Tokenizer</b>: &nbsp; Word-Level Tokenizer</li>
              <li><b>Vocab Size</b>: &nbsp; 10,000</li>
            </ul>
          </td>

          <td>
            <ul>
              <li><b>Input Dim</b>: &nbsp; 10,000</li>
              <li><b>Output Dim</b>: &nbsp; 10,000</li>
              <li><b>Embedding Dim</b>: &nbsp; 512</li>
              <li><b>Hidden Dim</b>: &nbsp; 512</li>
              <li><b>Model Params</b>: &nbsp; 000</li>
              <li><b>Model Size</b>: &nbsp; 000</li>
            </ul>
          </td>

          <td>
            <ul>
              <li><b>Num Epochs</b>: &nbsp; 10</li>
              <li><b>Batch Size</b>: &nbsp; 32</li>
              <li><b>Learning Rate</b>: &nbsp; 5e-4</li>
              <li><b>LR Scheduler</b>: &nbsp; pleature</li>
              <li><b>Optimizer</b>: &nbsp; AdamW</li>
              <li><b>Gradient Accumulation Steps</b>: &nbsp; 4</li>
            </ul>
          </td>          
        </tr>

      </tbody>
    </table>
    <div class="half-spacer"></div>
    <hr>



<!-- Result -->   
    <h2 id="5">5. &hairsp; Result</h2>
    <div class="small-spacer"></div>
    <table class="result-table">
      <thead>
        <tr>
          <th>Model Type</th>
          <th>Machine Translation</th>
          <th>Dialogue Generation</th>
          <th>Text Summarization</th>
        </tr>
      </thead>
      <tbody>
        <tr>
          <td>Scratch Model</td>
          <td>174</td>
          <td>7.00</td>
          <td>79</td>
        </tr>
        <tr>
          <td>PyTorch Model</td>
          <td>69</td>
          <td>5.47</td>
          <td>78</td>
        </tr>

      </tbody>
    </table>
    <hr>


<!-- Conclusion -->   
    <h2 id="6">6. &hairsp; Conclusion</h2>
    <div class="small-spacer"></div>
    <p> &nbsp; 이전의 RNN 계열의 모델들 보다 속도 측면, 그리고 성능 측면에서 굉장히 진일보 했음을 실험을 통해 직접적으로 확인할 수 있었음. 구현과정에서 복잡성이 증가 했다는 단점역시, Pytorch 라이브러리의 Transformer 모델을 통해 간편하게 사용할 수 있는 편의성이 있음.

    </p>  
    <hr>


<!-- Reference -->   
    <h2 id="7">5. &hairsp; Reference</h2>
    <div class="small-spacer"></div>

    <a class="reference" href="https://arxiv.org/abs/1706.03762">&nbsp; Attention Is All You Need</a> <br> 
    <a class="reference" href="https://arxiv.org/abs/1807.03819/">&nbsp; Universal Transformers</a> <br> 
    <a class="reference" href="https://arxiv.org/abs/1901.11117">&nbsp; The Evolved Transformers</a>
    <hr>

  </div>
</div>


<div class="pagination">
  <a href="{{ '/transformer' | relative_url }}" class="btn-prev"><span>Transformer</span></a>
  <a href="{{ '/plm_fusion' | relative_url }}" class="btn-next"><span>PLM Fusion</span></a>
</div>