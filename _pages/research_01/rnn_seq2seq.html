---
layout: default
permalink: /rnn_seq2seq
---


<div id="detail">
  <h1 class="title">Sequence to Sequence Modeling with RNN Cells</h1>  
  <div class="post">
    <h2 id="1">1. &hairsp; Project Desc</h2>

      <div class="small-spacer"></div>

      <h3>RNN Network</h3>
      <div style="text-align: center; margin: 0.5em 0;">
        <img src="{{ 'assets/img/research_01/fnn_rnn.png' | relative_url }}" style="width: 480px; height: 200px; display: block; margin: 0 auto;">
      </div>

      <p>
        &nbsp; 딥러닝 모델의 가장 기초적인 레이어인 FFNN(Feedforward Neural Network)은 데이터의 입력부터 모델의 최종 출력값의 산출 까지 모든 과정에서의 결과값이 단일 방향으로만 전달됩니다. 하지만 FFNN은 언어와 같은 시퀀셜 데이터의 순서 정보를 반영하기 어렵다는 한계점을 지니고 있습니다. 이를 해결하기 위해 고안된 신경망 네트워크 구조가 바로 RNN (Recurrent Neural Network)입니다. RNN은 이전 Time Step에서의 결과과 자기 회귀적으로 다시금 레이어의 입력값으로 주어지기 때문에, 순차적인 정보를 처리 할 수 있는 구조를 지닙니다. 대표적인 RNN 레이어를 구성하는 Cell들로는 RNN, LSTM, GRU가 있으며, 각 Cell에 대한 상세는 아래와 같습니다.  
      </p>

      <div class="spacer"></div>

      <h3>RNN Cells</h3>
        
        <div class="img-container">
          <ul>
            <li>RNN
              <ul>
                <li>다음은 LaTeX 수식의 예제입니다: \( \int_{a}^{b} x^2 \,dx \)</li>
                <li>여러 문장간 의미를 파악할 수 있도록 Token Type Id에 변형을 가함</li>
                <li>층이 깊어짐에 따라 기울기 소실의 문제점 발생</li>
              </ul>
            </li>
          </ul>
          <img src="{{ 'assets/img/research_01/rnn_cell.png' | relative_url }}">
        </div>

        <div class="img-container">
          <ul>
            <li>LSTM
              <ul>
                <li>다음은 LaTeX 수식의 예제입니다: \( \int_{a}^{b} x^2 \,dx \)</li>
                <li>RNN의 Gradient Vanishing문제를 일부 해소</li>
                <li>Gate 활용</li>
              </ul>
            </li>
          </ul>
          <img src="{{ 'assets/img/research_01/lstm_cell.png' | relative_url }}">
        </div>

        <div class="img-container">
          <ul>
            <li>GRU
              <ul>
                <li>다음은 LaTeX 수식의 예제입니다: \( \int_{a}^{b} x^2 \,dx \)</li>
                <li>LSTM의 복잡함을 단순화 시키면서도, 성능을 비슷하게 유지하기 위해 개량된 Cell구조</li>
                <li>최대 수용가능한 문장길이 512 → 2048로 확장</li>
              </ul>
            </li>
          </ul>
          <img src="{{ 'assets/img/research_01/gru_cell.png' | relative_url }}">
        </div>

      <div class="spacer"></div>
      <h3>Sequence to Sequnce</h3>
        <div style="text-align: center; margin: 0.5em 0;">
          <img src="{{ 'assets/img/research_01/seq2seq.png' | relative_url }}" style="width: 450px; height: 180px; display: block; margin: 0 auto;">
        </div>      
      <p>
      </p>


    <h2 id="2">2. Experimental Setup</h2>

      <div class="small-spacer"></div>
      <h3>Data Setup</h3>
        <div class="img-container">
          <ul>
            <li>Machine Translation</li>
            <p>기계 번역을 위해서는 WMT14의 En-De 데이터셋을 사용
              최소길이 100, 최대길이 300자 사이의 데이터들로만 선별
            </p>
            
            <div class="half-spacer"></div>
            <li>Dialogue Generation</li>
            <p>Daily Dialogue와 Persona Chat의 일부분을 발췌해서 데이터셋 구성.
              Daily Dialogue와 Persona Chat 모두 멀티턴 발화 데이터이기에, Daily Dialogue Dataset은 전체 대화를 하나의 싱글턴으로 분할해서 데이터에 포함시키고, Persona Chat은 첫번째 발화만을 선별해서 포함시킴.
              데이터 길이는 300자 이하의 대화 데이터만을 포함
            </p>              
            
            <div class="half-spacer"></div>
            <li>Text Summarization</li>
            <p>
            </p>

            <div class="half-spacer"></div>
            <li>Tokenizer
              <p>BPE Tokenizer를 활용. Vocab Size는 15,000개로, 각 Task마다 별도로 Tokenizer를 학습시켜 사용
              </p>              
            </li>                        
          </ul>
        </div>


      <div class="spacer"></div>
      <h3>Model Setup</h3>
        <div class="img-container">
          <ul>
            <li>RNN Network Setup
            </li>
            <li>Dimension Setup
              <p>
              </p>
            </li>
            <li>Text Summarization
            </li>       
          </ul>
        </div>


      <div class="spacer"></div>
      <h3>Training Setup</h3>
        <div class="img-container">
          <ul>
            <li>Machine Translation
            </li>
            <li>Dialogue Generation
            </li>
            <li>Text Summarization
            </li>
            <li>Tokenizer
            </li>                        
          </ul>
        </div>        



    <h2 id="5" class="h3-mt">3. Result</h2>


    <h2 id="6" class="h3-mt">4. Conclusion</h2>


    <h2 id="7" class="h3-mt">5. Reference</h2>
    <a class="reference" href="https://arxiv.org/abs/1409.3215">&nbsp; Sequence to Sequence Learning with Neural Networks</a> <br> 
    <a class="reference" href="https://arxiv.org/abs/1912.05911">&nbsp; Recurrent Neural Networks (RNNs): A gentle Introduction and Overview</a> <br>
    <a class="reference" href="https://arxiv.org/abs/1909.09586">&nbsp; Understanding LSTM</a> <br> 
    <a class="reference" href="https://arxiv.org/abs/1412.3555">&nbsp; Empirical Evaluation of Gated Recurrent Neural Networks on Sequence Modeling</a>

  </div>
</div>

<div class="pagination">
  <a href="{{ '/transformer_variants' | relative_url }}" class="btn-prev"></a>
  <a href="{{ '/rnn_seq2seq_attn' | relative_url }}" class="btn-next"></a>
</div>