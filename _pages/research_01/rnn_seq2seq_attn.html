---
layout: default
permalink: /rnn_seq2seq_attn
---


<div id="detail">
  <h1 class="title">RNN Seq2Seq with Attention Analytics</h1>  
  <div class="post">

    <hr>
    <div class="callout">
      <p class="highlighted-text"><span>Project Overview</span></p>
      <div class="small-spacer"></div>
      <p>&nbsp; 이 프로젝트에서는 GRU Seq2Seq 모델의 성능을 개선시키기 위해 Luong이 제시한 세가지 Global Attention 기법을 적용시키고, 각 방법론 별의 성능 개선 여부를 확인합니다. Attention 방법론은 Additive, Dot Product, Scaled Dot Product 방식을 사용했습니다. 결과적으로 성능면에서는 ~~~ 순으로, 효율성에서는 ~~~ 순으로 우수하다는 결과를 확인했습니다. 
      </p>
    </div>

    <div class="tblContents">
      <ul>

        <li>
          <a href="#1">1. &hairsp; Introduction</a>
        </li>

        <li>
          <a href="#2">2. &hairsp; Background</a>
        </li>

        <li>
          <a href="#3">3. &hairsp; Architecture</a>
        </li>

        <li>
          <a href="#4">4. &hairsp; Experimental Setup</a>
        </li>

        <li>
          <a href="#5">5. &hairsp; Results</a>
        </li>

        <li>
          <a href="#6">6. &hairsp; Conclusion</a>
        </li>

        <li>
          <a href="#7">7. &hairsp; Reference</a>
        </li>

      </ul>
    </div>
    <hr>


<!-- 1. Introduction -->    
    <h2 id="1">1. &hairsp; Introduction</h2>
    <div class="chapter-box">

      <div class="list-container">
        <ul>
          <li>Description</li>
            <p> &nbsp; 순환 신경망은 Transformer의 도래 이전, 시퀀셜 데이터 처리를 위한 딥러닝 모델링의 대표적인 방식이었습니다. 비록 현재는 Transformer가 기존 순환 신경망의 자리를 성공적으로 대체했지만, 간단하고 직관적인 모델링이 가능하다는 점에서 여전히 활용 가치가 있습니다. 뿐만 아니라, 자연어 생성을 위한 가장 기초적인 모델 구현 방식으로써, 이후 다양한 모델의 성능을 비교할 수 있는 기준점으로 작용가능하기에, 순환 신경망에 대한 연구는 가장 기초적이면서 필수적인 연구라고 할 수 있습니다. <br>

            이 프로젝트에서는 대표적 순환 신경망 구조인 RNN, LSTM, GRU를 사용해 Sequence to Sequene 모델을 구현하고, 이를 기계 번역, 대화 생성, 문서 요약이라는 세 가지 자연어 생성과에서의 성능을 비교 분석합니다.
            </p>

          <div class="spacer"></div>
          <li>Objective
            <ul>
              <li style="font-weight: normal;">RNN, LSTM, GRU 을 활용한 seq2seq 모델링 방식의 이해</li>
              <li style="font-weight: normal;">세 가지 순환 신경망의 성능 비교 검증</li>
              <li style="font-weight: normal;">가설에 대한 검증</li>
              <li style="font-weight: normal;">실험 결과를 통해, 이후 자연어 생성을 위한 모델링 성능의 기준점을 확립</li>
            </ul>
          </li>

        </ul>
      </div>

    </div>


<!-- 2. Background -->    
    <h2 id="2">2. &hairsp; Background</h2>
    <div class="chapter-box">

      <div class="list-container">
        <ul>
          <li>Attention</li>
            <p>&nbsp; 어텐션은 기존 단일 벡터 공간에 모든 시퀀스의 정보를 압축적으로 표현하면서 발생하는 제약을 극복하기 위한 연산 기법을 의미합니다. 매시점마다, 디코딩 과정에서 입력시퀀스에 집중할만한 정보에 더욱 집중해서 정보를 추출하기 위한 기법입니다. 기본적으로 어텐션 연산은 쿼리, 키, 밸류값을 기반으로 행해집니다. 쿼리와 키 값을 토대로, 쿼리와 키 간의 유사도를 측정하고, 유사도 수치를 확률값으로 변환합니다. 이후, 어텐션 확률값을 밸류에 곱해 최종적인 어텐션 값을 도출합니다.

            이 프로젝트에서는 총 세가지의 어텐션 기법을 사용하는데, 각각은 additive, dot product, scaled dot product입니다. 

            Additive Attention은 Bahdanau 어텐션으로도 불리는 유명한 어텐션 기법이고, 이후 루옹에 의한 어텐션 후속 연구에서 가장 많이 사용되는 Dot Product와, 이를 개선한, 그리고 Transformer 어텐션에서 활용되는 Scaled Dot Product를 선정했습니다. 각 어텐션에 대한 상세는 아래와 같습니다.
            </p>

            <div class="gray-box">
              <p>1. Attention Score 구하기 <br>
                2. Attention Distribution 구하기 <br>
                3. Attention Value 구하기 
              </p>
            </div>

            

          <div class="spacer"></div>
          <li>Additive Attention</li>

          <div class="dual-container">
            <img src="{{ 'assets/img/research_01/bahdanau.png' | relative_url }}" style="width: 27%; height: 20%; display: block; margin: 0 1em;">
            <p>Additive Attention, 혹은 바다나우 어텐션으로도 불리는 이 어텐션 기법은 이름에서 드러나듯, 어텐션 연산에서 각 값을 더해주는 방식을 사용합니다. 
            t시점의 예측을 위해, Encoder의 Hidden State들과 t-1 시점의 Hidden State를 비교합니다.
            </p>
          </div>

            <div class="latex-container">
              <div class="equation-item">
                  <p class="latex-equation">\( \mathbf{Attention \space Score} = W^T_a \tanh(W_b s_{t-1} + W_c H) \)</p>
              </div>
              <div class="equation-item">
                  <p class="latex-equation">\( \mathbf{Attention \space Distribution} = softmax(Attention \space Score) \)</p>
              </div>
              <div class="equation-item">
                  <p class="latex-equation">\( \mathbf{Attention \space Value} = H \cdot Attention \space Distribution \)</p>
              </div>
              <div class="equation-item">
                  <p class="latex-equation">\( {\Large s_t} = \text{Decoder}(y_{t-1}, s_{t-1}, c_t) \)</p>
              </div>
            </div>

            <p>&nbsp; Additive Attention, 혹은 바다나우 어텐션으로도 불리는 이 어텐션 기법은 이름에서 드러나듯, 어텐션 연산에서 각 값을 더해주는 방식을 사용합니다. 
            </p>

          <div class="spacer"></div>
          <li>Dot Product Attention</li>

          <div class="dual-container">
            <img src="{{ 'assets/img/research_01/luong.png' | relative_url }}" style="width: 30%; height: 40%; display: block; margin: 0 1em;">
            <p>Dot Product Attention은 이름에서도 드러나듯, 유사도 측정 과정에서 벡터간의 내적(Dot Product)를 사용합니다. 내적 연산은 두 벡터간의 유사도가 클수록 큰값이, 작을수록 작은 값이 도출되는 연산입니다. 별도의 학습 가능한 가중치가 필요하지 않기에 연산과정이 매우 간단하고 효율적이라는 장점이 존재합니다.
            </p>
          </div>


            <div class="latex-container">
              <div class="equation-item">
                  <p class="latex-equation">\( \mathbf{Attention \space Score} = \frac{QK^T}{\sqrt{d_k}}V \)</p>
              </div>
              <div class="equation-item">
                  <p class="latex-equation">\( \mathbf{Attention \space Distribution} = softmax(Attention \space Score) \)</p>
              </div>
              <div class="equation-item">
                  <p class="latex-equation">\( Attention \space Value = W^T_a \tanh(W_b s_{t-1} + W_c h_i) \)</p>
              </div>
              <div class="equation-item">
                  <p class="latex-equation">\( {\Large s_t} = \text{Decoder}(y_{t-1}, s_{t-1}, c_t) \)</p>
              </div>              
            </div>

            <p> Luong Attention이라고도 불리며, 이는 Additive Attention의 복잡한 연산과정을, 단순한 내적을 통한 유사도 계산으로 대체하며, 계산량을 줄인 어텐션 기법입니다.

            </p>

          <div class="half-spacer"></div>
          <li>Scaled Dot Product Attention</li>
            <p> 앞서 소개한 Dot Product Attention 기법이, 연산 효율성과 성능 측면에서도 좋은 모습을 보이고는 있지만, 다음과 같은 문제점이 존재합니다.
              1. 
            </p>

        </ul>
      </div>
    </div>



<!-- Architecture -->    
    <h2 id="3">3. &hairsp; Architecture</h2>
    <div class="chapter-box">
      <div class="list-container">
        <ul>
          <li>Sequence to Sequence</li>
            <img src="{{ 'assets/img/research_01/seq2seq.png' | relative_url }}" style="width: 65%; height: 100%; display: block; margin: 1em auto;">
            <p>&nbsp; Sequence to Sequence 모델은 Encoder와 Decoder로 구성된 모델 구조를 의미합니다. 이때 Encoder는 입력값에 대한 전체 정보를 하나의 벡터 공간에 투영하고, 디코더는 그 문맥 벡터를 사용해서, 매시점마다 결과 토큰을 생성합니다.
            </p>          

            <div class="dual-container">
              <p>RNN을 활용한 sequence to sequence 모델의 구조는 위의 그림과 같습니다. Encoder와 Decoder로 구성되며, 인코더는 입력값의 정보를 포함하는 Contect Vector를 생성하고, 디코더는 이 벡터값을 사용해, 매 시점마다의 결과값을 만들어 냅니다.
                이러한 구조의 문제점은 주어진 시퀀스의 모든 정보를 한정된 벡터 공간에 투영시켜야 한다는 것입니다.
              </p>
              <img src="{{ 'assets/img/research_01/limit.png' | relative_url }}" style="width: 45%; height: 25%; display: block; margin: 0 1em;">
            </div>

            <div class="code-container">
              <div class="code-snippet">
                <pre><code class="python">class Encoder(nn.Module):
    def __init__(self, config):
        super(Encoder, self).__init__()
        
        self.embedding = nn.Embedding(
            config.vocab_size, 
            config.emb_dim
        )
        self.dropout = nn.Dropout(config.dropout_ratio)

        if config.model_type == 'rnn':
            self.net = nn.RNN(**config.kwargs)
        elif config.model_type == 'lstm':
            self.net = nn.LSTM(**config.kwargs)
        elif config.model_type == 'gru':
            self.net = nn.GRU(**config.kwargs)


    def forward(self, x):
        x = self.dropout(self.embedding(x)) 
        _, hiddens = self.net(x)
        return hiddens



class Decoder(nn.Module):
    def __init__(self, config):
        super(Decoder, self).__init__()
    
        self.embedding = nn.Embedding(
            config.vocab_size, 
            config.emb_dim
        )

        self.dropout = nn.Dropout(config.dropout_ratio)
        
        if config.model_type == 'rnn':
            self.net = nn.RNN(**config.kwargs)
        elif config.model_type == 'lstm':
            self.net = nn.LSTM(**config.kwargs)
        elif config.model_type == 'gru':
            self.net = nn.GRU(**config.kwargs)
    
        self.fc_out = nn.Linear(
            config.hidden_dim * config.direction, 
            config.vocab_size
        )

    
    
    def forward(self, x, hiddens):
        x = self.dropout(self.embedding(x.unsqueeze(1)))
        out, hiddens = self.net(x, hiddens)
        out = self.fc_out(out)
        return out.squeeze(1), hiddens



class Seq2Seq(nn.Module):
    def __init__(self, config):
        super(Seq2Seq, self).__init__()

        self.device = config.device
        self.pad_id = config.pad_id
        self.vocab_size = config.vocab_size
        
        self.encoder = Encoder(config)
        self.decoder = Decoder(config)

        self.out = namedtuple('Out', 'logit loss')
        self.criterion = nn.CrossEntropyLoss().to(self.device)

    
    def forward(self, x, y, teacher_forcing_ratio=0.5):
        
        batch_size, max_len = y.shape
        
        outputs = torch.Tensor(batch_size, max_len, self.vocab_size)
        outputs = outputs.fill_(self.pad_id).to(self.device)

        dec_input = y[:, 0]
        hiddens = self.encoder(x)

        for t in range(1, max_len):
            out, hiddens = self.decoder(dec_input, hiddens)
            outputs[:, t] = out
            pred = out.argmax(-1)
            teacher_force = random.random() < teacher_forcing_ratio
            dec_input = y[:, t] if teacher_force else pred

        logit = outputs[:, 1:] 
        
        self.out.logit = logit
        self.out.loss = self.criterion(
            logit.contiguous().view(-1, self.vocab_size), 
            y[:, 1:].contiguous().view(-1)
        )
        
        return self.out </code></pre>
                </div>
              </div>
        

          <div class="spacer"></div>
          <div class="spacer"></div>
          <li>Sequence to Sequence with Attention</li>
            <img src="{{ 'assets/img/research_01/seq2seq_with_attn.png' | relative_url }}" style="width: 75%; height: 100%; display: block; margin: 1em auto;">
            <p>&nbsp; Sequence to Sequence with Attention 모델은 Sequence to Sequence와 마찬가지로, Encoder, Decoder, Seq2SeqModel로 이루어져 있지만, Decoder 부분만 상이합니다. 사용자의 설정에 따라 Additive, Dot Product, Scaled Dot Product일때 어텐션 연산을 달리 사용가능하도록 구현했습니다. 
            </p>
            <div class="code-container">
              <div class="code-snippet">
                <pre><code class="python">class Decoder(nn.Module):
    def __init__(self, config):
        super(Decoder, self).__init__()

        H = config.hidden_dim
        self.embedding = nn.Embedding(config.vocab_size, H)
        self.dropout = nn.Dropout(config.dropout_ratio)
        self.fc_out = nn.Linear(H, config.vocab_size)

        self.attn_type = config.attn_type
        if self.attn_type == 'additive':
            self.W_q = nn.Linear(H, H, bias=False)
            self.W_k = nn.Linear(H, H, bias=False)
            self.V = nn.Linear(H, 1, bias=False) 
            self.gru = nn.GRU(H * 2, H, batch_first=True)
        else:
            self.gru = nn.GRU(H, H, batch_first=True)
            self.concat = nn.Linear(H * 2, H)
            


    def forward(self, x, hidden, enc_outputs):
        out = self.dropout(self.embedding(x))
        
        #Additive Attention
        if self.attn_type == 'additive':
            attn_value = self.attention(hidden, enc_outputs)
            new_input = torch.cat((out, attn_value), dim=-1)
            dec_out, hidden = self.gru(new_input, hidden)

        #Dot-Product or Scaled Dot-Product Attention
        elif 'dot-product' in self.attn_type:
            dec_out, hidden = self.gru(out, hidden)
            attn_value = self.attention(dec_out, enc_outputs)
            dec_out = torch.cat((dec_out, attn_value), dim=-1)
            dec_out = torch.tanh(self.concat(dec_out))

        return self.fc_out(dec_out).squeeze(), hidden


    def attention(self, q, k):
        if self.attn_type == 'additive':
            q = q.permute(1, 0, 2)
            attn_score = self.V(torch.tanh(self.W_q(q) + self.W_k(k)))
            attn_score = attn_score.permute(0, 2, 1)
        else:
            attn_score = torch.bmm(q, k.permute(0,2,1))
            if 'scaled' in self.attn_type:
                attn_score /= attn_score.size(-1)
        
        attn_weight = F.softmax(attn_score, dim=-1)
        attn_value = torch.bmm(attn_weight, k)
        return attn_value</code></pre>
                </div>
              </div>


        </ul>
      </div>
    </div>



<!-- 4. Experimental Setup -->
    <h2 id="4">4. Experimental Setup</h2>
    <div class="chapter-box">

      <div class="list-container">
        <ul>
          <li>Data Setup
            <div class="dual-container">
              <ul>
                <li><b>Translation Task</b>: &nbsp; WMT'14 En-De</li>
                <li><b>Dialogue Task</b>: &nbsp; Daily Dialogue</li>
                <li><b>Summarization Task</b>: &nbsp; CNN Daily</li>
                <li><b>Tokenizer</b>: &nbsp; Word-Level Tokenizer</li>
              </ul>
              <ul>
                <li><b>Train Data Volumn</b>: &nbsp; 50,000</li>
                <li><b>Valid Data Volumn</b>: &nbsp; 5,000</li>
                <li><b>Test Data Volumn</b>: &nbsp; 100</li>
                <li><b>Vocab Size</b>: &nbsp; 10,000</li>                
              </ul>
            </div>
          </li>

          <div class="spacer"></div>
          <li>Model Setup
            <div class="dual-container">
              <ul>
                <li><b>Input Dim</b>: &nbsp; 10,000</li>
                <li><b>Output Dim</b>: &nbsp; 10,000</li>
                <li><b>Hidden Dim</b>: &nbsp; 512</li>
              </ul>
              <ul>
                <li><b>Additive Model Params</b>: &nbsp; 1,000,000</li>
                <li><b>Dot Product Model Params</b>: &nbsp; 1,000,000</li>
                <li><b>Scaled Dot Product Model Params</b>: &nbsp; 1,000,000</li>
              </ul>
            </div>
          </li>

          <div class="spacer"></div>
          <li>Training Setup
            <div class="dual-container">
              <ul>
                <li><b>Num Epochs</b>: &nbsp; 10</li>
                <li><b>Batch Size</b>: &nbsp; 32</li>
                <li><b>Learning Rate</b>: &nbsp; 5e-4</li>
                <li><b>LR Scheduler</b>: &nbsp; pleature</li>
              </ul>
              <ul>
                <li><b>Optimizer</b>: &nbsp; AdamW</li>
                <li><b>Gradient Accumulation Steps</b>: &nbsp; 4
                <li><b>Teacher Forcing Ratio</b>: &nbsp; 0.5</li>                
              </ul>
            </div>
          </li>
        </ul>
      </div>
    </div>


<!-- 5. Result -->
    <h2 id="5">5. &hairsp; Result</h2>
    <div class="chapter-box">

      <div class="list-container">
        <ul>
          <li>Machine Translation</li>

            <table class="result-table">
              <thead>
                <tr>
                  <th>Attention Type</th>
                  <th>Eval Score</th>
                  <th>Epoch Time</th>
                  <th>Avg GPU</th>
                  <th>Max GPU</th>
                </tr>
              </thead>
              <tbody>
                <tr>
                  <td>Additive</td>
                  <td>2.12</td>
                  <td>3m 58s</td>
                  <td>0.22GB</td>
                  <td>0.82GB</td>
                </tr>
                <tr>
                  <td>Dot Product</td>
                  <td>8.35</td>
                  <td>4m 16s</td>
                  <td>0.27GB</td>
                  <td>1.30GB</td>
                </tr>
                <tr>
                  <td>Scaled Dot Product</td>
                  <td>9.75</td>
                  <td>4m 6s</td>
                  <td>0.26GB</td>
                  <td>1.16GB</td>
                </tr>

              </tbody>
            </table>
          
          <div class="half-spacer"></div>
          <li>Dialogue Generation</li>
            <table class="result-table">
              <thead>
                <tr>
                  <th>Attention Type</th>
                  <th>Eval Score</th>
                  <th>Epoch Time</th>
                  <th>Avg GPU</th>
                  <th>Max GPU</th>
                </tr>
              </thead>
              <tbody>
                <tr>
                  <td>Additive</td>
                  <td>2.12</td>
                  <td>3m 58s</td>
                  <td>0.22GB</td>
                  <td>0.82GB</td>
                </tr>
                <tr>
                  <td>Dot Product</td>
                  <td>8.35</td>
                  <td>4m 16s</td>
                  <td>0.27GB</td>
                  <td>1.30GB</td>
                </tr>
                <tr>
                  <td>Scaled Dot Product</td>
                  <td>9.75</td>
                  <td>4m 6s</td>
                  <td>0.26GB</td>
                  <td>1.16GB</td>
                </tr>

              </tbody>
            </table>

                
          <div class="half-spacer"></div>
          <li>Text Summarization</li>
            <table class="result-table">
              <thead>
                <tr>
                  <th>Attention Type</th>
                  <th>Eval Score</th>
                  <th>Epoch Time</th>
                  <th>Avg GPU</th>
                  <th>Max GPU</th>
                </tr>
              </thead>
              <tbody>
                <tr>
                  <td>Additive</td>
                  <td>2.12</td>
                  <td>3m 58s</td>
                  <td>0.22GB</td>
                  <td>0.82GB</td>
                </tr>
                <tr>
                  <td>Dot Product</td>
                  <td>8.35</td>
                  <td>4m 16s</td>
                  <td>0.27GB</td>
                  <td>1.30GB</td>
                </tr>
                <tr>
                  <td>Scaled Dot Product</td>
                  <td>9.75</td>
                  <td>4m 6s</td>
                  <td>0.26GB</td>
                  <td>1.16GB</td>
                </tr>

              </tbody>
            </table>

          <br class="half-spacer">  
          <li>Result Analysis</li>
            <p> &nbsp; RNN은 다양한 정보를 함유하기에 지나치게 단순하고, LSTM은 GATE구조가 복잡하기에, 그만큼 영향을 받은 요소가 많아 학습 수렴이 어렵습니다. 때문에 모든 학습 결과에서는 GATE를 사용하면서도 단순화된 연산으로 학습의 이점이 큰 GRU가 가장 뛰어난 성능을 보임을 알 수 있습니다. 
            </p>
        </ul>
      </div>
    </div>



<!-- 6. Conclusion -->
    <h2 id="6">6. &hairsp; Conclusion</h2>
    <div class="chapter-box">

      <div class="list-container">
        <ul>
          <li>RNN, LSTM, GRU 을 활용한 seq2seq 모델링 방식의 이해</li>
            <p>&nbsp; 토큰의 벡터 변환을 위한 Embedding Layer와 실제 연산을 위한 순환 신경망 레이어만으로 구성된 단순산 Encoder, Decoder 구조로 이루어져 있으며, 
            </p>

          <br class="half-spacer">
          <li>세 가지 순환 신경망의 성능 비교 검증</li>
            <p>LSTM은 게이트 메커니즘을 통해 장기 의존성 문제를 해결하는 데 강점을 보입니다. 복잡한 문맥을 학습하고 기억하는 데 탁월하며, 긴 시퀀스 데이터에서 뛰어난 성능을 발휘할 수 있습니다. 하지만 LSTM은 더 많은 파라미터를 가지고 있어서 더 많은 데이터와 계산 리소스가 필요할 수 있습니다.
            </p>

          <br class="half-spacer">
          <li>가설 검증
            <ul>
              <li>LSTM, GRU, RNN 순으로 높은 성능</li>
                <p> LSTM의 연산과정이 가장 복잡하고, 다양한 GATE를 사용하기 떄문에, 좋은 성능을 낼 것이라고 예측했으나, 되려 복잡한 연산으로 인해, 학습 시 수렴에 어려움이 생겼습니다.
                오히려 GATE를 사용하면서도, 연산의 단순화를 도모한 GRU의 학습 성능이 가장 좋은 것을 확인했습니다
                </p>

              <li>RNN, GRU, LSTM 순으로 빠른 학습</li>
                <p> 학습 속도 및 GPU 사용량으로 확인해본 효율성은 가설과 마찬가지로 RNN, GRU, LSTM 순으로 나타났습니다.
                </p>
              <li></li>
            </ul>
          </li>
            

          <br class="half-spacer">
          <li>최소 기준치 확립</li>
            <p>앞서 언급했듯이 순환 신경망을 사용한 Seq2Seq모델은 앞으로 다루게 될 다양한 Seq2Seq 모델들의 시작점입니다. 고도화 된 모델일지라도 하이퍼 파라미터 설정이나, 훈련 방식 등의 이유에서 잘못된 성능이 나올수도 있습니다. 이때, 최소한의 동작은 하는 구나 하는 판단의 근거로 다음과 같은 스코어를 활용할 수 있습니다. <br> 
              기계번역 BLEU Score: 00.00, 대화 생성 Rouge Score: 00.00, 문서 요약 Rouge Score: 00.00 이하의 성능ㅇ
            </p>

        </ul>
      </div>
    </div>



<!-- Reference -->    
    <h2>5. Reference</h2>
    <div class="chapter-box">
      <a class="reference" href="https://arxiv.org/abs/1409.3215">&nbsp; Sequence to Sequence Learning with Neural Networks</a> <br> 
      <a class="reference" href="https://arxiv.org/abs/1409.0473">&nbsp; Neural Machine Translation by Jointly Learning to Align and Translate</a> <br> 
      <a class="reference" href="https://arxiv.org/abs/1508.04025">&nbsp; Effective Approaches to Attention-based Neural Machine Translation</a>
    </div>


  </div>
</div>


<div class="pagination">
  <a href="{{ '/rnn_seq2seq' | relative_url }}" class="btn-prev"><span>RNN Seq2Seq</span></a>
  <a href="{{ '/transformer' | relative_url }}" class="btn-next"><span>Transformer</span></a>
</div>