---
layout: default
permalink: /rnn_seq2seq_attn
---


<div id="detail">
  <h1 class="title">RNN Seq2Seq with Attention Analytics</h1>  
  <div class="post">

    <hr>
    <div class="callout">
      <p class="highlighted-text"><span>Project Overview</span></p>
      <div class="small-spacer"></div>
      <p>&nbsp; 이 프로젝트에서는 GRU Seq2Seq 모델의 성능을 개선시키기 위해 Luong이 제시한 세가지 Global Attention 기법을 적용시키고, 각 방법론 별의 성능 개선 여부를 확인합니다. Attention 방법론은 Dot Product, General, Concat 방식을 사용했습니다. 기존 RNN Seq2Seq와의 성능 비교를 위해, 모든 변인은 이전 프로젝트에서의 설정값과 동일하게 적용했으며, 오직 Attention 메커니즘의 적용 여부만을 유일 변인으로 남겨두었습니다. 
      자연어 생성 능력 확인을 위한 과제로는 기계번역, 대화생성, 문서요약을 선정했으며, 결과는...
      </p>
    </div>

    <div class="tblContents">
      <ul>

        <li>
          <a href="#1">1. &hairsp; Introduction</a>
        </li>

        <li>
          <a href="#2">2. &hairsp; Background</a>
        </li>

        <li>
          <a href="#3">3. &hairsp; Architecture</a>
        </li>

        <li>
          <a href="#4">4. &hairsp; Experimental Setup</a>
        </li>

        <li>
          <a href="#5">5. &hairsp; Results</a>
        </li>

        <li>
          <a href="#6">6. &hairsp; Conclusion</a>
        </li>

        <li>
          <a href="#7">7. &hairsp; Reference</a>
        </li>

      </ul>
    </div>
    <hr>


<!-- 1. Introduction -->    
    <h2 id="1">1. &hairsp; Introduction</h2>
    <div class="chapter-box">

      <div class="list-container">
        <ul>
          <li>Description</li>
            <p> &nbsp; 이전 프로젝트에서는 RNN Seq2Seq 모델에 대해 자세히 살펴보았습니다. 하지만 기존의 방식에서는 시퀀스의 모든 정보를 한정된 벡터에 압축하여 표현하다보니, 필연적으로 정보의 소실이 발생하게 됩니다. 그리고 이러한 정보의 소실은 시퀀스의 길이가 길어질 수록 뚜렸하게 발생합니다. 기존의 문제를 보완하기 위해, 매번 생성 시점마다 입력시퀀스와 관련성이 깊은 주요 정보만에 더욱 집중해서 처리하기 위한 방식이 바로 Attention입니다. 

            <br>

            가장 대표적인 Attention 적용 방식은 Bahdanau Attention을 사용하는 방식이며, 이에 대한 레퍼런스 코드들도 즐비하게 존재합니다. 하지만 상대적으로 Luong Attetnion에 대한 레퍼런스는 적으며, Luong이 제시한 세 가지 Global Attention마다의 자연어 생성 성능을 비교 검증하는 연구는 존재하지 않습니다.

            <br>

            때문에 이 프로젝트에서는 레퍼런스의 부재를 직접 해소하기 위한 일련의 실험 과정을 공유합니다. 모델 구조의 경우 이전 실험에서 가장 안정적인 성능을 보였던 GRU Seq2Seq 구조를 선택했으며, Hyperparameter를 포함한 실험 변수는 모두 이전 실험과 동일하게 적용했습니다. 덕분에 Attention 적용 여부에 따른 성능 변화 추이를 보다 객관적으로 바라볼 수 있습니다. 

            </p>

          <div class="spacer"></div>
          <li>Objective
            <ul>
              <li style="font-weight: normal;">기존 RNN Seq2Seq 개선을 위한 Attention 적용 방식의 이해</li>
              <li style="font-weight: normal;">세 가지 Luong Attention 적용에 따른 성능 변화 확인</li>
            </ul>
          </li>

        </ul>
      </div>

    </div>


<!-- 2. Background -->    
    <h2 id="2">2. &hairsp; Background</h2>
    <div class="chapter-box">

      <div class="list-container">
        <ul>
          <li>Attention</li>
            <p>&nbsp; 어텐션은 기존 단일 벡터 공간에 모든 시퀀스의 정보를 압축적으로 표현하면서 발생하는 제약을 극복하기 위한 연산 기법을 의미합니다. 매시점마다, 디코딩 과정에서 입력시퀀스에 집중할만한 정보에 더욱 집중해서 정보를 추출하기 위한 기법입니다. 기본적으로 어텐션 연산은 쿼리, 키, 밸류값을 기반으로 행해집니다. 쿼리와 키 값을 토대로, 쿼리와 키 간의 유사도를 측정하고, 유사도 수치를 확률값으로 변환합니다. 이후, 어텐션 확률값을 밸류에 곱해 최종적인 어텐션 값을 도출합니다.

            이 프로젝트에서는 총 세가지의 어텐션 기법을 사용하는데, 각각은 additive, dot product, scaled dot product입니다. 

            루옹 어텐션을 사용했습니다.
            </p>


          <div class="spacer"></div>
          <li>Dot Product Attention
            <ul>
              <li>\( score = s_t^T h_j \)</li>
              <li>Dot Product Attention은 이름에서도 드러나듯, 유사도 측정 과정에서 벡터간의 내적(Dot Product)를 사용합니다. 내적 연산은 두 벡터간의 유사도가 클수록 큰값이, 작을수록 작은 값이 도출되는 연산입니다. 별도의 학습 가능한 가중치가 필요하지 않기에 연산과정이 매우 간단하고 효율적이라는 장점이 존재합니다.</li>
            </ul>
          </li>


          <div class="spacer"></div>
          <li>General Attention
            <ul>
              <li>\( score = s_t^T W_a h_j \)</li>
              <li>Dot Product Attention은 이름에서도 드러나듯, 유사도 측정 과정에서 벡터간의 내적(Dot Product)를 사용합니다. 내적 연산은 두 벡터간의 유사도가 클수록 큰값이, 작을수록 작은 값이 도출되는 연산입니다. 별도의 학습 가능한 가중치가 필요하지 않기에 연산과정이 매우 간단하고 효율적이라는 장점이 존재합니다.</li>
            </ul>
          </li>

          <div class="half-spacer"></div>
          <li>Concat Attention
            <ul>
              <li>\( score = v_a^T tanh(W_a s_t + U_a h_j) \)</li>
              <li>Dot Product Attention은 이름에서도 드러나듯, 유사도 측정 과정에서 벡터간의 내적(Dot Product)를 사용합니다. 내적 연산은 두 벡터간의 유사도가 클수록 큰값이, 작을수록 작은 값이 도출되는 연산입니다. 별도의 학습 가능한 가중치가 필요하지 않기에 연산과정이 매우 간단하고 효율적이라는 장점이 존재합니다.</li>
            </ul>
          </li>

        </ul>
      </div>
    </div>



<!-- Architecture -->    
    <h2 id="3">3. &hairsp; Architecture</h2>
    <div class="chapter-box">
      <div class="list-container">
        <ul>
          <li>Sequence to Sequence</li>
            <img src="{{ 'assets/img/research_01/seq2seq.png' | relative_url }}" style="width: 65%; height: 100%; display: block; margin: 1em auto;">
            <p>&nbsp; Sequence to Sequence 모델은 Encoder와 Decoder로 구성된 모델 구조를 의미합니다. 이때 Encoder는 입력값에 대한 전체 정보를 하나의 벡터 공간에 투영하고, 디코더는 그 문맥 벡터를 사용해서, 매시점마다 결과 토큰을 생성합니다.
            </p>          

            <div class="dual-container">
              <p>RNN을 활용한 sequence to sequence 모델의 구조는 위의 그림과 같습니다. Encoder와 Decoder로 구성되며, 인코더는 입력값의 정보를 포함하는 Contect Vector를 생성하고, 디코더는 이 벡터값을 사용해, 매 시점마다의 결과값을 만들어 냅니다.
                이러한 구조의 문제점은 주어진 시퀀스의 모든 정보를 한정된 벡터 공간에 투영시켜야 한다는 것입니다.
              </p>
              <img src="{{ 'assets/img/research_01/limit.png' | relative_url }}" style="width: 45%; height: 25%; display: block; margin: 0 1em;">
            </div>

            <div class="code-container">
              <div class="code-snippet">
                <pre><code class="python">class Encoder(nn.Module):
    def __init__(self, config):
        super(Encoder, self).__init__()
        
        self.embedding = nn.Embedding(
            config.vocab_size, 
            config.emb_dim
        )
        self.dropout = nn.Dropout(config.dropout_ratio)

        if config.model_type == 'rnn':
            self.net = nn.RNN(**config.kwargs)
        elif config.model_type == 'lstm':
            self.net = nn.LSTM(**config.kwargs)
        elif config.model_type == 'gru':
            self.net = nn.GRU(**config.kwargs)


    def forward(self, x):
        x = self.dropout(self.embedding(x)) 
        _, hiddens = self.net(x)
        return hiddens



class Decoder(nn.Module):
    def __init__(self, config):
        super(Decoder, self).__init__()
    
        self.embedding = nn.Embedding(
            config.vocab_size, 
            config.emb_dim
        )

        self.dropout = nn.Dropout(config.dropout_ratio)
        
        if config.model_type == 'rnn':
            self.net = nn.RNN(**config.kwargs)
        elif config.model_type == 'lstm':
            self.net = nn.LSTM(**config.kwargs)
        elif config.model_type == 'gru':
            self.net = nn.GRU(**config.kwargs)
    
        self.fc_out = nn.Linear(
            config.hidden_dim * config.direction, 
            config.vocab_size
        )

    
    
    def forward(self, x, hiddens):
        x = self.dropout(self.embedding(x.unsqueeze(1)))
        out, hiddens = self.net(x, hiddens)
        out = self.fc_out(out)
        return out.squeeze(1), hiddens



class Seq2Seq(nn.Module):
    def __init__(self, config):
        super(Seq2Seq, self).__init__()

        self.device = config.device
        self.pad_id = config.pad_id
        self.vocab_size = config.vocab_size
        
        self.encoder = Encoder(config)
        self.decoder = Decoder(config)

        self.out = namedtuple('Out', 'logit loss')
        self.criterion = nn.CrossEntropyLoss().to(self.device)

    
    def forward(self, x, y, teacher_forcing_ratio=0.5):
        
        batch_size, max_len = y.shape
        
        outputs = torch.Tensor(batch_size, max_len, self.vocab_size)
        outputs = outputs.fill_(self.pad_id).to(self.device)

        dec_input = y[:, 0]
        hiddens = self.encoder(x)

        for t in range(1, max_len):
            out, hiddens = self.decoder(dec_input, hiddens)
            outputs[:, t] = out
            pred = out.argmax(-1)
            teacher_force = random.random() < teacher_forcing_ratio
            dec_input = y[:, t] if teacher_force else pred

        logit = outputs[:, 1:] 
        
        self.out.logit = logit
        self.out.loss = self.criterion(
            logit.contiguous().view(-1, self.vocab_size), 
            y[:, 1:].contiguous().view(-1)
        )
        
        return self.out </code></pre>
                </div>
              </div>
        

          <div class="spacer"></div>
          <div class="spacer"></div>
          <li>Sequence to Sequence with Attention</li>
            <img src="{{ 'assets/img/research_01/seq2seq_with_attn.png' | relative_url }}" style="width: 75%; height: 100%; display: block; margin: 1em auto;">
            <p>&nbsp; Sequence to Sequence with Attention 모델은 Sequence to Sequence와 마찬가지로, Encoder, Decoder, Seq2SeqModel로 이루어져 있지만, Decoder 부분만 상이합니다. 사용자의 설정에 따라 Additive, Dot Product, Scaled Dot Product일때 어텐션 연산을 달리 사용가능하도록 구현했습니다. 
            </p>
            <div class="code-container">
              <div class="code-snippet">
                <pre><code class="python">class Decoder(nn.Module):
    def __init__(self, config):
        super(Decoder, self).__init__()

        H = config.hidden_dim
        self.embedding = nn.Embedding(config.vocab_size, H)
        self.dropout = nn.Dropout(config.dropout_ratio)
        self.fc_out = nn.Linear(H, config.vocab_size)

        self.attn_type = config.attn_type
        if self.attn_type == 'additive':
            self.W_q = nn.Linear(H, H, bias=False)
            self.W_k = nn.Linear(H, H, bias=False)
            self.V = nn.Linear(H, 1, bias=False) 
            self.gru = nn.GRU(H * 2, H, batch_first=True)
        else:
            self.gru = nn.GRU(H, H, batch_first=True)
            self.concat = nn.Linear(H * 2, H)
            


    def forward(self, x, hidden, enc_outputs):
        out = self.dropout(self.embedding(x))
        
        #Additive Attention
        if self.attn_type == 'additive':
            attn_value = self.attention(hidden, enc_outputs)
            new_input = torch.cat((out, attn_value), dim=-1)
            dec_out, hidden = self.gru(new_input, hidden)

        #Dot-Product or Scaled Dot-Product Attention
        elif 'dot-product' in self.attn_type:
            dec_out, hidden = self.gru(out, hidden)
            attn_value = self.attention(dec_out, enc_outputs)
            dec_out = torch.cat((dec_out, attn_value), dim=-1)
            dec_out = torch.tanh(self.concat(dec_out))

        return self.fc_out(dec_out).squeeze(), hidden


    def attention(self, q, k):
        if self.attn_type == 'additive':
            q = q.permute(1, 0, 2)
            attn_score = self.V(torch.tanh(self.W_q(q) + self.W_k(k)))
            attn_score = attn_score.permute(0, 2, 1)
        else:
            attn_score = torch.bmm(q, k.permute(0,2,1))
            if 'scaled' in self.attn_type:
                attn_score /= attn_score.size(-1)
        
        attn_weight = F.softmax(attn_score, dim=-1)
        attn_value = torch.bmm(attn_weight, k)
        return attn_value</code></pre>
                </div>
              </div>


        </ul>
      </div>
    </div>



<!-- 4. Experimental Setup -->
    <h2 id="4">4. Experimental Setup</h2>
    <div class="chapter-box">

      <div class="list-container">
        <ul>
          <li>Data Setup
            <div class="dual-container">
              <ul>
                <li><b>Translation Task</b>: &nbsp; WMT'14 En-De</li>
                <li><b>Dialogue Task</b>: &nbsp; Daily Dialogue</li>
                <li><b>Summarization Task</b>: &nbsp; CNN Daily</li>
                <li><b>Tokenizer</b>: &nbsp; Word-Level Tokenizer</li>
              </ul>
              <ul>
                <li><b>Train Data Volumn</b>: &nbsp; 50,000</li>
                <li><b>Valid Data Volumn</b>: &nbsp; 5,000</li>
                <li><b>Test Data Volumn</b>: &nbsp; 100</li>
                <li><b>Vocab Size</b>: &nbsp; 10,000</li>                
              </ul>
            </div>
          </li>

          <div class="spacer"></div>
          <li>Model Setup
            <div class="dual-container">
              <ul>
                <li><b>Input Dim</b>: &nbsp; 10,000</li>
                <li><b>Output Dim</b>: &nbsp; 10,000</li>
                <li><b>Hidden Dim</b>: &nbsp; 512</li>
              </ul>
              <ul>
                <li><b>Additive Model Params</b>: &nbsp; 1,000,000</li>
                <li><b>Dot Product Model Params</b>: &nbsp; 1,000,000</li>
                <li><b>Scaled Dot Product Model Params</b>: &nbsp; 1,000,000</li>
              </ul>
            </div>
          </li>

          <div class="spacer"></div>
          <li>Training Setup
            <div class="dual-container">
              <ul>
                <li><b>Num Epochs</b>: &nbsp; 10</li>
                <li><b>Batch Size</b>: &nbsp; 32</li>
                <li><b>Learning Rate</b>: &nbsp; 5e-4</li>
                <li><b>LR Scheduler</b>: &nbsp; ReduceLROnPlateau</li>
              </ul>
              <ul>
                <li><b>Optimizer</b>: &nbsp; AdamW</li>
                <li><b>Gradient Accumulation Steps</b>: &nbsp; 4
                <li><b>Teacher Forcing Ratio</b>: &nbsp; 0.5</li>                
              </ul>
            </div>
          </li>
        </ul>
      </div>
    </div>


<!-- 5. Result -->
    <h2 id="5">5. &hairsp; Result</h2>
    <div class="chapter-box">

      <div class="list-container">
        <ul>
          <li>Machine Translation</li>

            <table class="result-table">
              <thead>
                <tr>
                  <th>Attention Type</th>
                  <th>Eval Score</th>
                  <th>Epoch Time</th>
                  <th>Avg GPU</th>
                  <th>Max GPU</th>
                </tr>
              </thead>
              <tbody>
                <tr>
                  <td>Additive</td>
                  <td>2.12</td>
                  <td>3m 58s</td>
                  <td>0.22GB</td>
                  <td>0.82GB</td>
                </tr>
                <tr>
                  <td>Dot Product</td>
                  <td>8.35</td>
                  <td>4m 16s</td>
                  <td>0.27GB</td>
                  <td>1.30GB</td>
                </tr>
                <tr>
                  <td>Scaled Dot Product</td>
                  <td>9.75</td>
                  <td>4m 6s</td>
                  <td>0.26GB</td>
                  <td>1.16GB</td>
                </tr>

              </tbody>
            </table>
          
          <div class="half-spacer"></div>
          <li>Dialogue Generation</li>
            <table class="result-table">
              <thead>
                <tr>
                  <th>Attention Type</th>
                  <th>Eval Score</th>
                  <th>Epoch Time</th>
                  <th>Avg GPU</th>
                  <th>Max GPU</th>
                </tr>
              </thead>
              <tbody>
                <tr>
                  <td>Additive</td>
                  <td>2.12</td>
                  <td>3m 58s</td>
                  <td>0.22GB</td>
                  <td>0.82GB</td>
                </tr>
                <tr>
                  <td>Dot Product</td>
                  <td>8.35</td>
                  <td>4m 16s</td>
                  <td>0.27GB</td>
                  <td>1.30GB</td>
                </tr>
                <tr>
                  <td>Scaled Dot Product</td>
                  <td>9.75</td>
                  <td>4m 6s</td>
                  <td>0.26GB</td>
                  <td>1.16GB</td>
                </tr>

              </tbody>
            </table>

                
          <div class="half-spacer"></div>
          <li>Text Summarization</li>
            <table class="result-table">
              <thead>
                <tr>
                  <th>Attention Type</th>
                  <th>Eval Score</th>
                  <th>Epoch Time</th>
                  <th>Avg GPU</th>
                  <th>Max GPU</th>
                </tr>
              </thead>
              <tbody>
                <tr>
                  <td>Additive</td>
                  <td>2.12</td>
                  <td>3m 58s</td>
                  <td>0.22GB</td>
                  <td>0.82GB</td>
                </tr>
                <tr>
                  <td>Dot Product</td>
                  <td>8.35</td>
                  <td>4m 16s</td>
                  <td>0.27GB</td>
                  <td>1.30GB</td>
                </tr>
                <tr>
                  <td>Scaled Dot Product</td>
                  <td>9.75</td>
                  <td>4m 6s</td>
                  <td>0.26GB</td>
                  <td>1.16GB</td>
                </tr>

              </tbody>
            </table>

          <br class="half-spacer">  
          <li>Result Analysis</li>
            <p> &nbsp; RNN은 다양한 정보를 함유하기에 지나치게 단순하고, LSTM은 GATE구조가 복잡하기에, 그만큼 영향을 받은 요소가 많아 학습 수렴이 어렵습니다. 때문에 모든 학습 결과에서는 GATE를 사용하면서도 단순화된 연산으로 학습의 이점이 큰 GRU가 가장 뛰어난 성능을 보임을 알 수 있습니다. 
            </p>
        </ul>
      </div>
    </div>



<!-- 6. Conclusion -->
    <h2 id="6">6. &hairsp; Conclusion</h2>
    <div class="chapter-box">

      <div class="list-container">
        <ul>
          <li>RNN, LSTM, GRU 을 활용한 seq2seq 모델링 방식의 이해</li>
            <p>&nbsp; 토큰의 벡터 변환을 위한 Embedding Layer와 실제 연산을 위한 순환 신경망 레이어만으로 구성된 단순산 Encoder, Decoder 구조로 이루어져 있으며, 
            </p>

          <br class="half-spacer">
          <li>세 가지 순환 신경망의 성능 비교 검증</li>
            <p>LSTM은 게이트 메커니즘을 통해 장기 의존성 문제를 해결하는 데 강점을 보입니다. 복잡한 문맥을 학습하고 기억하는 데 탁월하며, 긴 시퀀스 데이터에서 뛰어난 성능을 발휘할 수 있습니다. 하지만 LSTM은 더 많은 파라미터를 가지고 있어서 더 많은 데이터와 계산 리소스가 필요할 수 있습니다.
            </p>

          <br class="half-spacer">
          <li>가설 검증
            <ul>
              <li>LSTM, GRU, RNN 순으로 높은 성능</li>
                <p> LSTM의 연산과정이 가장 복잡하고, 다양한 GATE를 사용하기 떄문에, 좋은 성능을 낼 것이라고 예측했으나, 되려 복잡한 연산으로 인해, 학습 시 수렴에 어려움이 생겼습니다.
                오히려 GATE를 사용하면서도, 연산의 단순화를 도모한 GRU의 학습 성능이 가장 좋은 것을 확인했습니다
                </p>

              <li>RNN, GRU, LSTM 순으로 빠른 학습</li>
                <p> 학습 속도 및 GPU 사용량으로 확인해본 효율성은 가설과 마찬가지로 RNN, GRU, LSTM 순으로 나타났습니다.
                </p>
              <li></li>
            </ul>
          </li>
            

          <br class="half-spacer">
          <li>최소 기준치 확립</li>
            <p>앞서 언급했듯이 순환 신경망을 사용한 Seq2Seq모델은 앞으로 다루게 될 다양한 Seq2Seq 모델들의 시작점입니다. 고도화 된 모델일지라도 하이퍼 파라미터 설정이나, 훈련 방식 등의 이유에서 잘못된 성능이 나올수도 있습니다. 이때, 최소한의 동작은 하는 구나 하는 판단의 근거로 다음과 같은 스코어를 활용할 수 있습니다. <br> 
              기계번역 BLEU Score: 00.00, 대화 생성 Rouge Score: 00.00, 문서 요약 Rouge Score: 00.00 이하의 성능ㅇ
            </p>

        </ul>
      </div>
    </div>



<!-- Reference -->    
    <h2>5. Reference</h2>
    <div class="chapter-box">
      <a class="reference" href="https://arxiv.org/abs/1409.3215">&nbsp; Sequence to Sequence Learning with Neural Networks</a> <br> 
      <a class="reference" href="https://arxiv.org/abs/1409.0473">&nbsp; Neural Machine Translation by Jointly Learning to Align and Translate</a> <br> 
      <a class="reference" href="https://arxiv.org/abs/1508.04025">&nbsp; Effective Approaches to Attention-based Neural Machine Translation</a>
    </div>


  </div>
</div>


<div class="pagination">
  <a href="{{ '/rnn_seq2seq' | relative_url }}" class="btn-prev"><span>RNN Seq2Seq</span></a>
  <a href="{{ '/transformer' | relative_url }}" class="btn-next"><span>Transformer</span></a>
</div>