---
layout: default
permalink: /rnn_seq2seq_attn
---


<div id="detail">
  <h1 class="title">RNN Seq2Seq with Attention Analytics</h1>  
  <div class="post">

    <hr>
    <div class="callout">
      <p class="highlighted-text"><span>Project Overview</span></p>
      <div class="small-spacer"></div>
      <p>&nbsp; 이 프로젝트에서는 GRU Seq2Seq 모델의 성능을 개선시키기 위해 세 가지 Attention 기법을 적용시키고, 각 방법론 별의 성능 개선 여부를 확인합니다. Attention 방법론은 Additive, Dot Product, Scaled Dot Product 방식을 사용했습니다. 결과적으로 성능면에서는 ~~~ 순으로, 효율성에서는 ~~~ 순으로 우수하다는 결과를 확인했습니다. </p>
    </div>

      <div class="tblContents">
        <ul>

          <li>
            <a href="#1">1. &hairsp; Introduction</a>
            <ul>
              <li>
                <a href="#1.1">1.1 &hairsp; Motivation</a>
              </li>
              <li>
                <a href="#1.2">1.2 &hairsp; Objective</a>
              </li>
            </ul>
          </li>


          <li>
            <a href="#2">2. &hairsp; Background</a>
            <ul>
              <li>
                <a href="#1.1">1.1 &hairsp; Additive</a>
              </li>
              <li>
                <a href="#1.2">1.2 &hairsp; Dot Product</a>
              </li>
              <li>
                <a href="#1.3">1.3 &hairsp; Scaled Dot Product</a>
              </li>          
            </ul>
          </li>


          <li>
            <a href="#3">3. &hairsp; Experimental Setup</a>
          </li>

          <li>
            <a href="#4">4. &hairsp; Results</a>
          </li>

          <li>
            <a href="#5">5. &hairsp; Conclusion</a>
          </li>

          <li>
            <a href="#6">6. &hairsp; Reference</a>
          </li>

        </ul>
      </div>
      <hr>


<!-- 1. Introduction -->    
    <h2 id="1">1. &hairsp; Introduction</h2>
    <h3 id="1.1">1.1 &hairsp; Motivation</h3>
    <p> &nbsp; Transformer는 Attention 연산을 활용해 기존 RNN의 자리를 성공적으로 대체했습니다. 시기 적으로 보자면 RNN과 Transformer의 변환기 사이 RNN에서 Attention을 추가하는 방식이 사용되었습니다.

    Transformer의 핵심 연산인 Attention에 대한 더 깊은 이해를 위해, 이 과정에서의 모델링 성능을 경험적으로 확인해보는 것이 필요합니다. 

      Transformer 덕분에 Scaled Dot Product Attention은 매우 유명한 어텐션 기법입니다. 하지만 그 이전에 가장 유명했던 어텐션 기법은 Additive Attention과 Dot Product Attention입니다. 각각 바다나우, 루옹 어텐션으로 불리기도함.

      어텐션 기법 간의 장단점을 파악.

      기존 RNN기반 Seq2Seq 모델의 한계를 극복하기 위한 Attention의 태동 이유를 직접 구현을 통해 이해해봅니다. Attention에 왜 더 많은 관심이 모이고, 결과적으로 Transformer까지 이어질 수있었는지 이해해보려 합니다.

      Attention의 시작을 이해함으로써, Transformer의 Scaled Dot Attention부터 효율적인 연산을 위한 Sparse Attention등 다양한 어텐션 기법이해의 토대를 구축하려 합니다.

      # 아니지 구축하려 한다는 목적성은 아래의 objective로 빼야하고, 여기서는 이해하거나 살펴보거나 탐구하거나 좀더 추상적인 원하는 바가 담긴 서술어로 풀어서 사용하는 걸로!

    </p>


    <div class="spacer"></div>    
    <h3 id="1.2">1.2 &hairsp; Objective</h3>
    
    <div class="plain-list">  
      <ul>
        <li>세가지 Attention 기법의 비교분석</li>
        <li>Attention을 직접 구현해보며, 이해도를 증진시킵니다</li>
        <li>Attention 기법의 유용성을 확인</li>
        <li>Attention을 통한 GRU Seq2Seq 모델의 한계를 돌파</li>
      </ul>
    </div>
    <hr>




<!-- Project Desc -->    
    <h2 id="2">2. &hairsp; Background</h2>

    <p>어텐션의 기본 아이디어는 디코더에서 출력 단어를 예측하는 매 시점 (time step) 마다, 인코더에서의 전체 입력 문장을 다시 한 번 참고한다는 것입니다. 단, 전체 입력 문장을 전부 다 동일한 비율로 참고하는 것이 아닌, 해당 시점에서 예측하고자 하는 토큰과 연관이 있는 입력 시퀀스의 부분을 좀 더 집중(attention)적으로 참고합니다. 어텐션을 통해 컨텍스트 벡터에 대한 의존성을 줄이고, 입출력 값 사이의 관계성도 정밀하게 처리할 수 있습니다. 이번 프로젝트에서는 이전 프로젝트에서 좋은 성능과 효율성을 보여줬던 GRU Cell을 활용한 Sequence to Sequnce 구조에 세 가지 Attention 기법을 적용하고, 다양한 자연어 생성 과제에서 성능 변화를 직접 확인해보며, 그 효용성을 검증합니다.
    Attention은 주어진 쿼리에 대해, 키에 대한 값을 이용해, 쿼리의 답을 얻기 위해 어떤 키를 살펴봐야 하는지 계싼해주는 함수입니다. 보다 자세히 말하자면, 쿼리는 어텐션의 주체를 의미하고, 키는 쿼리에 대한 어켄션의 기여도를 계산할 대상, Value는 어텐션의 크기를 계산하기 위한 값을 의미합니다. 최종적으로는 Q, K로 얻어진 Weight을 사용해, Value의 가중합으로 Attention Value(Context Value)를 산출합니다.
    어텐션 산출 과정에 대해 좀더 자세히 보자면, 우선 Attention Score는 쿼리와 키값을 입력으로 받아 둘 사이의 상호작용을 추출하는 과정입니다. 이때 Attention Score는 중요도를 나타내는 벡터라고 볼 수 있습니다.
    </p>

    <div class="small-spacer"></div>
    <h3>1.1 Additive Attention</h3>

    <div class="latex-container">

        <div class="equation-item">
            <p class="latex-equation">\( \mathbf{Attention \space Score} = W^T_a \tanh(W_b s_{t-1} + W_c H) \)</p>
        </div>

        <div class="equation-item">
            <p class="latex-equation">\( \mathbf{Attention \space Distribution} = softmax(Attention \space Score) \)</p>
        </div>


        <div class="equation-item">
            <p class="latex-equation">\( \mathbf{Attention \space Value} = H \cdot Attention \space Distribution \)</p>
        </div>


        <div class="equation-item">
            <p class="latex-equation">\( {\Large s_t} = \text{Decoder}(y_{t-1}, s_{t-1}, c_t) \)</p>
        </div>

    </div>
            
    <p> Bahdanau Attention이라고도 알려져있는 Additive Attention은 글자 그대로 더하는 방식의 Attention을 의미합니다. 
    어텐션 함수는 주어진 Query에 대해 모든 Key와의 유사도를 각각 구합니다.
    그리고 구해낸 유사도를 키와 매핑ㄱ되어있는 각각의 Value에 바영해줍니다, 그리고 유사도가 반영된 Value를 모두 더해서 리턴합니다.

    <br>
    기존의 방법론이 디코더의 모든 Hidden State를 생성하기 위해 동일한 Context Vector를 사용했던 반면, 이 방식에서는 HIdden State를 계산할때만다, 새롭게 만들어진 Context Vector를 사용합니다.. 

    </p><br>
              
    <div class="small-spacer"></div>
    <p><strong>1) Attention Score 연산</strong> <br></p>
    <p style="margin-left: 20px">
    인코더의 Hidden State와, 디코더의 현재 time step, t시점에서의 디코더 은닉상태를 st라고 하면.
    쿼리로는 디코더의 t-1 시점의 은닉상태 s(t-1)을 사용ㅎ합니다.
    즉 바다나우 어텐션의 어텐션 스코어는 s(t-1)와 인코더의 의 i번째 은닉상태의 어텐션 스코어를 계산하는 것이라고 보면 됩니다. </p><br>

    <div class="small-spacer"></div>
    <p><strong>2) 소프트맥스 함수를 통해 어텐션 분포 도출</strong><br></p>
    <p style="margin-left: 20px">
    앞서 획득한 어텐션 스코어 벡터에 softmax함수만 적용해서, 확률분포로 변환 </p><br>

    <div class="small-spacer"></div>
    <p><strong>3) 각 인코더의 어텐션 가중치와 은닉상태를 가중합해서, 최종 어텐션 값 도출</strong></p>
    <p style="margin-left: 20px">
    지금까지 준비해온 정보들을 하나로 합치는 단계입니다. 어텐션은 최종 결과를 얻기 위해 각 인코더의 은닉상태와 어텐션 가중치들을 가중합합니다.
    이 벡터는 인코더의 문맥을 포함하고 있다고 해서 컨텍스트 벡터라고 부릅니다.
    </p><br>

    <div class="small-spacer"></div>
    <p><strong>4) 컨텍스트 벡터로부 st를 구합니다 </strong></p>
    <p style="margin-left: 20px">
    t-1시점의 디코더 임베딩 값과 컨텍스트 벡터를 concat해서 디코더의 입력값으로 활용</p>  


    <div class="spacer"></div>
    <h3>Dot Product Attention</h3>        

    <div class="latex-container">

        <div class="equation-item">
            <p class="latex-equation">\( Attention(Q, K, V) = softmax(\frac{QK^T}{\sqrt{d_k}})V \)</p>
        </div>

        <div class="equation-item">
            <p class="latex-equation">\( Attention \space Value = W^T_a \tanh(W_b s_{t-1} + W_c h_i) \)</p>
        </div>

    </div>


    <p> &nbsp; Dot Product Attention
      두 벡터간의 유사도를 측정하는 가장 간단한 방식은 내적(Dot Product)을 사용하는 것입니다. Query와 Key가 같으면 내적값은 최대가 되고, 반대 방향일 경우 최소 값이 됩니다. 이렇게 내적 함수를 사용하는 어텐션을 Dot Product Attention 혹은 제안자의 이름을 따 Luong Attention이라고도 합니다.

      이방식은 추가적인 레이어가 필요없어서 효율적입니다.

      소프트맥스 함수를 통해 나온 결과값은 입력 시퀀스의 각 토큰별 인코딩 된 값들이 디코더의 출력 단어를 예측하는데 있어 도움이 되는 정도를 수치화해서 표현되는 값입니다. 이 정보를 담아 디코더로 전달됩니다.

      1) Attention Score의 계산
      어텐션 스코어는 현재 디코더의 시점 t에서 단어를 예측하기 위해, 인코더의 모든 은닉상태 각각이 디코더의 현시점의 은닉상st와 얼마나 유사한지 판단하는 스코어 값입니다.

      2)


    </p>  

    <div class="half-spacer"></div>
    <h3>Scaled Dot Product Attention</h3>     

    <div class="latex-container">

        <div class="equation-item">
            <p class="latex-equation">\( Attention(Q, K, V) = softmax(\frac{QK^T}{\sqrt{d_k}})V \)</p>
        </div>

        <div class="equation-item">
            <p class="latex-equation">\( Attention \space Value = W^T_a \tanh(W_b s_{t-1} + W_c h_i) \)</p>
        </div>

    </div>


    <p> &nbsp; Scaled Dot Product Attention은 Dot Product Attention에 정규화를 함으로써 값의 안정화를 도모한 어텐션 기법입니다. 대표적으로 Transformer모델에서 사용되는 가장 대중적인 Attention 기법이라고 할 수 있습니다.

    Dot Product Attention에 Scaling된 형태를 사용합니다,
    Scaling을 하는 이유는 다음과 같습니다.
    1. d_k가 커지면 QK^T의 값 역시 커진다.
    2. QK^T 값이 커지면, softmax함수에서 gradient vanishing이 발생합니다.


    </p>
    <hr>

<!-- Architecture -->    
    <h2 id="3">3. &hairsp; Architecture</h2>
    <h3 id="3.1">3.1 &hairsp; Sequence to Sequence</h3>
    <img src="{{ 'assets/img/research_01/seq2seq.png' | relative_url }}" style="width: 60%; height: 100%; display: block; margin: 0 auto;">
    <div class="dual-container">
      <p>RNN을 활용한 sequence to sequence 모델의 구조는 위의 그림과 같습니다. Encoder와 Decoder로 구성되며, 인코더는 입력값의 정보를 포함하는 Contect Vector를 생성하고, 디코더는 이 벡터값을 사용해, 매 시점마다의 결과값을 만들어 냅니다.
        이러한 구조의 문제점은 주어진 시퀀스의 모든 정보를 한정된 벡터 공간에 투영시켜야 한다는 것입니다.
      </p>
      <img src="{{ 'assets/img/research_01/limit.png' | relative_url }}" style="width: 45%; height: 25%; display: block; margin: 0 auto;">
    </div>
    <p>주어진 시퀀스의 길이가 짧은 경우에는 큰 문제가 아닐수도 있지만, 복잡한 문제이거나 시퀀스의 길이가 길어지면서, 한정된 벡터 공간에 투영하기에는 과도한 정보가 발생하게 됩니다.
       결과적으로 중요한 정보가 누락된 정보벡터를 사용하다보니, 성능적인 하락이 발생하게 됩니다.
    </p>
    <hr>

    <div class="half-spacer"></div>
    <h3 id="3.2">3.2 &hairsp; Sequence to Sequence with Attention</h3>
    <p>한정된 벡터공간의 활용이 문제라면, 이를 한정시키지 않으면 됩니다. 
      입력과 출력의 각 단어들간에 서로 어떤 단어들끼리 연관되어 있는가에 대한 대응관계를 모델에 학습시키는 것이 어텐션 메커니즘의 핵심입니다. 이렇게 단어들 간의 대응관계를 Phrase Alignment라고 부릅니다. 
    </p>
    <hr>


<!-- Experimental Setup -->    
    <h2 id="3">3. &hairsp; Experimental Setup</h2>
    <div class="small-spacer"></div>
    <table class="result-table">
      <thead>
        <tr>
          <th>Data Setup</th>
          <th>Model Setup</th>
          <th>Training Setup</th>
        </tr>
      </thead>
      <tbody>
        <tr>

          <td>
            <ul>
              <li><b>Translation Task</b>: &nbsp; WMT'14 En-De</li>
              <li><b>Dialogue Task</b>: &nbsp; Daily Dialogue</li>
              <li><b>Summarization Task</b>: &nbsp; CNN Daily</li>
              <li><b>Train Data Volumn</b>: &nbsp; 50,000</li>
              <li><b>Valid Data Volumn</b>: &nbsp; 5,000</li>
              <li><b>Test Data Volumn</b>: &nbsp; 100</li>
              <li><b>Tokenizer</b>: &nbsp; Word-Level Tokenizer</li>
              <li><b>Vocab Size</b>: &nbsp; 10,000</li>
            </ul>
          </td>

          <td>
            <ul>
              <li><b>Input Dim</b>: &nbsp; 10,000</li>
              <li><b>Output Dim</b>: &nbsp; 10,000</li>
              <li><b>Embedding Dim</b>: &nbsp; 512</li>
              <li><b>Hidden Dim</b>: &nbsp; 512</li>
              <li><b>Model Params</b>: &nbsp; 000</li>
              <li><b>Model Size</b>: &nbsp; 000</li>
            </ul>
          </td>

          <td>
            <ul>
              <li><b>Num Epochs</b>: &nbsp; 10</li>
              <li><b>Batch Size</b>: &nbsp; 32</li>
              <li><b>Learning Rate</b>: &nbsp; 5e-4</li>
              <li><b>LR Scheduler</b>: &nbsp; pleature</li>
              <li><b>Optimizer</b>: &nbsp; AdamW</li>
              <li><b>Gradient Accumulation Steps</b>: &nbsp; 4</li>
            </ul>
          </td>          
        </tr>

      </tbody>
    </table>
    <div class="half-spacer"></div>
    <hr>


<!-- Result -->    
    <h2>3. Result</h2>
    <div class="small-spacer"></div>
    <table class="result-table">
      
      <thead>
        <tr>
          <th>Attention Strategy</th>
          <th>Machine Translation</th>
          <th>Dialogue Generation</th>
          <th>Text Summarization</th>
        </tr>
      </thead>

      <tbody>
        <tr>
          <td>No Attention Model</td>
          <td>-</td>
          <td>-</td>
          <td>-</td>
        </tr>
      
        <tr>
          <td>Additive Attention Model</td>
          <td>결과2-1</td>
          <td>결과2-2</td>
          <td>결과2-3</td>
        </tr>
      
        <tr>
          <td>Dot Product Attention Model</td>
          <td>결과2-1</td>
          <td>결과2-2</td>
          <td>결과2-3</td>
        </tr>

        <tr>
          <td>Scaled Dot Product Attention Model</td>
          <td>결과2-1</td>
          <td>결과2-2</td>
          <td>결과2-3</td>
        </tr>

      </tbody>
    </table>
    <hr>

<!-- Conclusion -->    
    <h2>4. Conclusion</h2>
    <p>
    </p>
    <hr>


<!-- Reference -->    
    <h2>5. Reference</h2>

    <a class="reference" href="https://arxiv.org/abs/1409.3215">&nbsp; Sequence to Sequence Learning with Neural Networks</a> <br> 
    <a class="reference" href="https://arxiv.org/abs/1409.0473">&nbsp; Neural Machine Translation by Jointly Learning to Align and Translate</a> <br> 
    <a class="reference" href="https://arxiv.org/abs/1412.3555">&nbsp; Empirical Evaluation of Gated Recurrent Neural Networks on Sequence Modeling</a>

    <hr>

  </div>
</div>


<div class="pagination">
  <a href="{{ '/rnn_seq2seq' | relative_url }}" class="btn-prev"><span>RNN Seq2Seq</span></a>
  <a href="{{ '/transformer' | relative_url }}" class="btn-next"><span>Transformer</span></a>
</div>