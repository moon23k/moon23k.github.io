---
layout: default
permalink: /rnn_seq2seq_attn
---


<div id="detail">
  <h1 class="title">RNN Seq2Seq with Attention Analytics</h1>  
  <div class="post">


<!-- Project Desc -->    
    <h2>1. &hairsp; Project Desc</h2>

      <div class="half-spacer"></div>

      <h3>RNN Sequence to Sequence</h3>
      <div class="small-spacer"></div>

      <p>
        &nbsp; 현재 다양한 딥러닝 모델들이 다양한 분야에서 좋은 성능을 보여주고 있습니다. 
        성능이 뛰어난, 모델을 사용하기 위해서는 사용자의 의도에 맞는 Down Stream Task Dataset에 맞춰 파인튜닝이 이뤄져야 합니다.
        대규모 모델의 경우, 파인튜닝을 위한 자원도 많이 필요합니다.
        모델의 성능을 포기하는 방법대신, 효율적인 학습방식으로 이 한계를 극복할수 있습니다.
        이 프로젝트에서는 네가지 대표적인 효율적 모델 학습 방법론을 살펴보고, 실제 Sequence Classification Task에서의 효용성을 통해
        각 방법론 별 효용성을 비교 분석해봅니다.
      </p>


      <div class="spacer"></div>
      <h3>Attention Methodology</h3>
      <div class="img-container">
        <ul>
          <li>Additive Attention</li>        
            <p> &nbsp; 기울기 누적 방법은 전체 배치를 한 번에 계산하는 대신 작은 증분으로 기울기를 계산하는 방법을 지향합니다. 
              이 방법은 모델을 통해 순전파와 역전파를 수행하면서 작은 배치에서 반복적으로 기울기를 계산하고 이 과정 동안 기울기를 누적하는 방식으로 이루어집니다. 
              충분한 수의 기울기가 누적되면 모델의 최적화 단계가 실행됩니다. 
              기울기 누적을 사용하면 GPU 메모리 용량의 제한을 초과하여 효과적인 배치 크기를 증가시킬 수 있습니다. 
              그러나 기울기 누적에 의해 도입된 추가적인 순전파와 역전파는 훈련 과정을 느리게 만들 수 있다는 점을 주의해야 합니다.
            </p>  

          <div class="half-spacer"></div>
          <li>Dot Product Attention</li>        
            <p> &nbsp; 기울기 누적 방법은 전체 배치를 한 번에 계산하는 대신 작은 증분으로 기울기를 계산하는 방법을 지향합니다. 
              이 방법은 모델을 통해 순전파와 역전파를 수행하면서 작은 배치에서 반복적으로 기울기를 계산하고 이 과정 동안 기울기를 누적하는 방식으로 이루어집니다. 
              충분한 수의 기울기가 누적되면 모델의 최적화 단계가 실행됩니다. 
              기울기 누적을 사용하면 GPU 메모리 용량의 제한을 초과하여 효과적인 배치 크기를 증가시킬 수 있습니다. 
              그러나 기울기 누적에 의해 도입된 추가적인 순전파와 역전파는 훈련 과정을 느리게 만들 수 있다는 점을 주의해야 합니다.
            </p>  

          <div class="half-spacer"></div>
          <li>Scaled Dot Product Attention</li>        
            <p> &nbsp; 기울기 누적 방법은 전체 배치를 한 번에 계산하는 대신 작은 증분으로 기울기를 계산하는 방법을 지향합니다. 
              이 방법은 모델을 통해 순전파와 역전파를 수행하면서 작은 배치에서 반복적으로 기울기를 계산하고 이 과정 동안 기울기를 누적하는 방식으로 이루어집니다. 
              충분한 수의 기울기가 누적되면 모델의 최적화 단계가 실행됩니다. 
              기울기 누적을 사용하면 GPU 메모리 용량의 제한을 초과하여 효과적인 배치 크기를 증가시킬 수 있습니다. 
              그러나 기울기 누적에 의해 도입된 추가적인 순전파와 역전파는 훈련 과정을 느리게 만들 수 있다는 점을 주의해야 합니다.
            </p>  

        </ul>
      </div>



<!-- Experimental Setup -->    
    <h2>2. Experimental Setup</h2>

      <div class="small-spacer"></div>
      <h3>Data Setup</h3>
      <p>Sequence Classification을 위한 대표적 데이터 셋 중하나인 AG_News를 사용.
        데이터의 크기는 Train/ Valid/ Test 1000/ 100/ 100으로 설정
        총 네가지 데이터 Label을 모든 데이터에서 고르게 분배해서 한쪽으로 치우치지 않도록 가공
      </p>

      <div class="spacer"></div>
      <h3>Model Setup</h3>
      <p>대표적인 PreTrained Model인 BERT를 사용.
      </p>


      <div class="spacer"></div>
      <h3>Training Setup</h3>
      <div class="small-spacer"></div>

      <div class="code-container">
        <div class="code-snippet">
          <pre><code class="python">
TrainingArguments(
        output_dir= f'ckpt/{strategy}',
        num_train_epochs= 5,
        learning_rate= 1e-5,
        per_device_train_batch_size= 32,
        per_device_eval_batch_size= 32,
        lr_scheduler_type='reduce_lr_on_plateau',
        load_best_model_at_end= True,

        save_strategy= 'epoch',
        logging_strategy= 'epoch',
        evaluation_strategy= 'epoch',

        fp16= True if config.strategy in ['fp16', 'all'] else False,
        fp16_opt_level= '02' if config.strategy in ['fp16', 'all'] else '01',
        gradient_accumulation_steps = True if config.strategy in ['grad_accumulation', 'all'] else 4,
        gradient_checkpointing= True if config.strategy in ['grad_checkpointing', 'all'] else False,
        optim = 'adafactor' if config.strategy in ['optim', 'all'] else 'adamw_torch'
)
          </code></pre>
        </div>
      </div>


<!-- Result -->    
    <h2>3. Result</h2>
    <div class="small-spacer"></div>
    <table class="result-table">
      
      <thead>
        <tr>
          <th>Attention Strategy</th>
          <th>Machine Translation</th>
          <th>Dialogue Generation</th>
          <th>Text Summarization</th>
        </tr>
      </thead>

      <tbody>
        <tr>
          <td>Vanilla Model</td>
          <td>-</td>
          <td>-</td>
          <td>-</td>
        </tr>
      
        <tr>
          <td>Additive Attention Model</td>
          <td>결과2-1</td>
          <td>결과2-2</td>
          <td>결과2-3</td>
        </tr>
      
        <tr>
          <td>Dot Product Attention Model</td>
          <td>결과2-1</td>
          <td>결과2-2</td>
          <td>결과2-3</td>
        </tr>

        <tr>
          <td>Scaled Dot Product Attention Model</td>
          <td>결과2-1</td>
          <td>결과2-2</td>
          <td>결과2-3</td>
        </tr>

      </tbody>
    </table>


<!-- Conclusion -->    
    <h2>4. Conclusion</h2>



<!-- Reference -->    
    <h2>5. Reference</h2>

    <a class="reference" href="https://arxiv.org/abs/1409.3215">&nbsp; Sequence to Sequence Learning with Neural Networks</a> <br> 
    <a class="reference" href="https://arxiv.org/abs/1409.0473">&nbsp; Neural Machine Translation by Jointly Learning to Align and Translate</a> <br> 
    <a class="reference" href="https://arxiv.org/abs/1412.3555">&nbsp; Empirical Evaluation of Gated Recurrent Neural Networks on Sequence Modeling</a>

  </div>
</div>


<div class="pagination">
  <a href="{{ '/rnn_seq2seq' | relative_url }}" class="btn-prev"></a>
  <a href="{{ '/transformer' | relative_url }}" class="btn-next"></a>
</div>