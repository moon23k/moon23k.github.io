---
layout: default
permalink: /plm_fusion
---


<div id="detail">
  <h1 class="title">How to Fuse Pretrained Encoder on Seq2Seq Model</h1>  
  <div class="post">

    <hr>
    <div class="callout">
      <p class="highlighted-text"><span>Project Objective</span></p>
      <div class="small-spacer"></div>
      <p>&nbsp; RNN은 이전 스텝의 결과값을 다음 스텝의 처리과정에 재귀적으로 반영시키며, 시퀀셜 데이터를 처리하기 용이한 네트워크 구조입니다. 대표적인 RNN Cell 구조로는 RNN, LSTM, GRU
      이 프로젝트에서는 세가지 셀 구조별 Seq2Seq 모델 구조에서 성능이 얼마나 나오는지를 비교 분석해봅니다.</p>
    </div>

    <div class="tblContents">
      <ul>

        <li>
          <a href="#1">1. &hairsp; Encoder & Decoder Connection</a>
          <ul>
            <li>
              <a href="#1.1">1.1 &hairsp; Standard Connection</a>
            </li>
            <li>
              <a href="#1.2">1.2 &hairsp; Fused Connection</a>
            </li>
          </ul>
        </li>

        <li>
          <a href="#2">2. &hairsp; Experimental Setup</a>
        </li>

        <li>
          <a href="#3">3. &hairsp; Results</a>
        </li>

        <li>
          <a href="#4">4. &hairsp; Conclusion</a>
        </li>

        <li>
          <a href="#5">5. &hairsp; Reference</a>
        </li>

      </ul>
    </div>
    <hr>


<!-- Encoder & Decoder Connection -->    
    <h2 id="1">1. &hairsp; Encoder & Decoder Connection</h2>

    <h3 id="1.1">1.1 &hairsp; Standard Connection</h3>
    <div class="dual-container">
        <img src="{{ 'assets/img/research_01/standard_transformer.png' | relative_url }}" style="width: 250px; height: 350px;">
        <p>
        &nbsp; 앞선 프로젝트에서는 RNN을 기반으로한 Sequence to Sequence 모델 구조의 효용성에 대해 면밀히 살펴봤습니다. 
        하지만 RNN 네트워크는 장기기억력이라는 관점에서 고질적인 문제점을 태생적으로 지니고 있으며, 이를 해결하기 위한 Attention 메커니즘의 활용 역시 기대만큼 좋은 성능을 보이지는 못했습니다.
        Transformer는 어텐션 메커니즘 만으로 문맥 전체적인 내용을 포착하는 뛰어난 성능과 더불어 병렬처리로 인한 학습의 효율성까지 제고시킨 뛰어난 모델 구조입니다.
        이 글에서는 Transformer의 구조에 대한 설명과 더불어, 두가지 방식에서의 Code Implementation 방식을 알아봅니다. 또한 이렇게 구현한 Transformer 모델이 세가지 자연어 생성 과제에서 어떤 성능을 보이는지 비교해봅니다.
        </p>
    </div>
    <hr>

    <div class="half-spacer"></div>
    <h3 id="1.2">1.2 &hairsp; Fused Connection</h3>
    <div class="dual-container">
      <img src="{{ 'assets/img/research_01/fusion.png' | relative_url }}" style="width: 350px; height: 250px;">
      <p>
      &nbsp; 앞선 프로젝트에서는 RNN을 기반으로한 Sequence to Sequence 모델 구조의 효용성에 대해 면밀히 살펴봤습니다. 
      하지만 RNN 네트워크는 장기기억력이라는 관점에서 고질적인 문제점을 태생적으로 지니고 있으며, 이를 해결하기 위한 Attention 메커니즘의 활용 역시 기대만큼 좋은 성능을 보이지는 못했습니다.
      Transformer는 어텐션 메커니즘 만으로 문맥 전체적인 내용을 포착하는 뛰어난 성능과 더불어 병렬처리로 인한 학습의 효율성까지 제고시킨 뛰어난 모델 구조입니다.
      </p>
    </div>

    <div class="small-spacer"></div>
    <div class="code-container">
      <div class="code-snippet">
        <pre><code class="python">def generate_signal(length, channels, min_timescale=1.0, max_timescale=1.0e4):
    position = np.arange(length)
    num_timescales = channels // 2
    log_timescale_increment = ( math.log(float(max_timescale) / float(min_timescale)) / (float(num_timescales) - 1))
    inv_timescales = min_timescale * np.exp(np.arange(num_timescales).astype(np.float) * -log_timescale_increment)
    scaled_time = np.expand_dims(position, 1) * np.expand_dims(inv_timescales, 0)

    signal = np.concatenate([np.sin(scaled_time), np.cos(scaled_time)], axis=1)
    signal = np.pad(signal, [[0, 0], [0, channels % 2]], 'constant', constant_values=[0.0, 0.0])
    signal =  signal.reshape([1, length, channels])

    return torch.from_numpy(signal).type(torch.FloatTensor)</code></pre>
      </div>
    </div>    

    <hr>



<!-- Experimental Setup -->    
    <h2>2. Experimental Setup</h2>

    <div class="half-spacer"></div>
    <table class="result-table">
      <thead>
        <tr>
          <th>Data Setup</th>
          <th>Model Setup</th>
          <th>Training Setup</th>
        </tr>
      </thead>
      <tbody>
        <tr>

          <td>
            <ul>
              <li><b>Translation Task</b>: &nbsp; WMT'14 En-De</li>
              <li><b>Dialogue Task</b>: &nbsp; Daily Dialogue</li>
              <li><b>Summarization Task</b>: &nbsp; CNN Daily</li>
              <li><b>Train Data Volumn</b>: &nbsp; 50,000</li>
              <li><b>Valid Data Volumn</b>: &nbsp; 5,000</li>
              <li><b>Test Data Volumn</b>: &nbsp; 100</li>
              <li><b>Tokenizer</b>: &nbsp; Word-Level Tokenizer</li>
              <li><b>Vocab Size</b>: &nbsp; 10,000</li>
            </ul>
          </td>

          <td>
            <ul>
              <li><b>BERT</b>: &nbsp; bert-base-uncased</li>
              <li><b>Input Dim</b>: &nbsp; 10,000</li>
              <li><b>Output Dim</b>: &nbsp; 10,000</li>
              <li><b>Embedding Dim</b>: &nbsp; 512</li>
              <li><b>Hidden Dim</b>: &nbsp; 512</li>
              <li><b>Model Params</b>: &nbsp; 000</li>
              <li><b>Model Size</b>: &nbsp; 000</li>
            </ul>
          </td>

          <td>
            <ul>
              <li><b>Num Epochs</b>: &nbsp; 10</li>
              <li><b>Batch Size</b>: &nbsp; 32</li>
              <li><b>Learning Rate</b>: &nbsp; 5e-4</li>
              <li><b>LR Scheduler</b>: &nbsp; pleature</li>
              <li><b>Optimizer</b>: &nbsp; AdamW</li>
              <li><b>Gradient Accumulation Steps</b>: &nbsp; 4</li>
            </ul>
          </td>          
        </tr>

      </tbody>
    </table>
    <div class="half-spacer"></div>
    <hr>


<!-- Result -->    
    <h2>3. Result</h2>
    <div class="small-spacer"></div>
    <table class="result-table">
      
      <thead>
        <tr>
          <th>Attention Strategy</th>
          <th>Machine Translation</th>
          <th>Dialogue Generation</th>
          <th>Text Summarization</th>
        </tr>
      </thead>

      <tbody>
        <tr>
          <td>Vanilla Transformer</td>
          <td>-</td>
          <td>-</td>
          <td>-</td>
        </tr>
      
        <tr>
          <td>Simple Model</td>
          <td>결과2-1</td>
          <td>결과2-2</td>
          <td>결과2-3</td>
        </tr>
      
        <tr>
          <td>Fused Model</td>
          <td>결과2-1</td>
          <td>결과2-2</td>
          <td>결과2-3</td>
        </tr>

      </tbody>
    </table>


<!-- Conclusion -->    
    <h2>4. Conclusion</h2>



<!-- Reference -->    
    <h2>5. Reference</h2>

    <a class="reference" href="https://arxiv.org/abs/1706.03762">&nbsp; Attention Is All You Need</a> <br> 
    <a class="reference" href="https://arxiv.org/abs/2002.06823">&nbsp; Incorporating BERT into Neural Machine Translation</a>


  </div>
</div>


<div class="pagination">
  <a href="{{ '/balance' | relative_url }}" class="btn-prev"><span>Transformer Balance</span></a>
  <a href="{{ '/eff_train' | relative_url }}" class="btn-next"><span>Efficient Training</span></a>
</div>