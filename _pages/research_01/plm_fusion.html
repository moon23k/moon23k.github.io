---
layout: default
permalink: /plm_fusion
---


<div id="detail">
  <h3 class="title">사전학습 된 인코더의 결합 방식에 따른 자연어 생성 능력 성능 비교</h3>

  <div class="tblContents">
    <ul>
      <li>
        <a href="#1">1. Introduction</a>
      </li>

      <li>
        <a href="#2">2. Background</a>
        <ul>
          <li>
            <a href="#2.1">2.1 Neural Machine Translation</a>
          </li>
          <li>
            <a href="#2.2">2.2 Back Translation</a>
          </li>
          <li>
            <a href="#2.3">2.3 Generative Strategy</a>
          </li>
          <li>
            <a href="#2.4">2.4 Data Noise</a>
          </li>          
        </ul>
      </li>
      
      <li>
        <a href="#3">3. Strategies</a>
        <ul>
          <li>
            <a href="#3.1">3.1 Data Volumn</a>
          </li>
          <li>
            <a href="#3.2">3.2 Generative Thing</a>
          </li>
          <li>
            <a href="#3.2">3.3 Noise</a>
          </li>
        </ul>
      </li>

      <li>
        <a href="#4">4. Experimental Setup</a>
        <ul>
          <li>
            <a href="#4.1">4.1 Data Setups</a>
          </li>
          <li>
            <a href="#4.2">4.2 Model Setups</a>
          </li>
          <li>
            <a href="#4.3">4.3 Training Setups</a>
          </li>
          <li>
            <a href="#4.4">4.4 Model desc</a>
          </li>
        </ul>
      </li>

      <li>
        <a href="#5">5. Results</a>
      </li>

      <li>
        <a href="#6">6. Conclusion</a>
      </li>

      <li>
        <a href="#7">7. Reference</a>
      </li>

    </ul>
  </div>


  
  <div class="post">
    <h2 id="1">1. Introduction</h2>
      <p>
        &nbsp; 사전 학습 언어 모델은 다양한 자연어 처리에서 좋은 성능을 보이고 있습니다. 특히 BERT를 필두로 한 NLU Task에 강점을 보이는 사전학습 Encoder 모델들이 뛰어난 모습을 보이고 있습니다.
        이 글에서는 Sequence to Sequence 모델을 구성함에 있어 필수적인 Encoder와 Decoder의 결합 방식에서, 사전학습된 인코더를 활용하는 두가지 결합 방법론에 대해 살펴보고, 직접 구현후 세가지 자연어 생성과제에서의 성능을 비교 분석해봅니다. 
      </p>


    <h2 id="2">2. Background</h2>

      <h3 id="2.1">2.1 Pre Trained Encoder Model</h3>
        <p>
          &nbsp; BERT와 같이 Transformer의 Encoder만으로 구성되어, 사전학습되고, 일반적으로는 NLU Task에서 활용되는 모델들을 Pre Trained Encoder라고 할 수 있습니다.
          이러한 사전학습 인코더 모델들을 

        </p>

      <h3 id="2.2" class="h3-mt">2.2 Transf</h3>
        <p>
          &nbsp; Inductive Bias는 기계 학습 모델이 주어진 학습 데이터로부터 일반화하는 데에 어떤 가정이나 제한사항을 가지고 있는 것을 말합니다. 이러한 가정이나 제한사항은 모델이 어떤 가설 공간을 탐색하고 어떤 가설을 선호하는지에 영향을 미칩니다.nductive Bias는 모델이 일반화 능력과 학습 데이터에 대한 의존성을 조절하는 역할을 합니다. 적절한 nductive Bias는 모델이 주어진 문제에 적합하고 일반화하기 쉽도록 도와줍니다.
        </p>
        <p>
          하지만 Transformer의 경우, 복잡한 모델 구조 때문에 Inductive Bias가 부족하기 쉽습니다. 때문에 간단한 과제 해결과정에서 RNN보다 못한 성능을 보이는 경우도 종종 발생합니다. Universal Transformer 논문에서는 Transformer 레이어 연결과정에서 RNN의 핵심개념이었던 재귀적인 연결 방식을 차용함으로써 Inductive Bias의 향상을 도모할 수 있다고 주장합니다.
        </p>

      <h3 id="2.3" class="h3-mt">2.3 Generative Strategy</h3>
        <p>
          &nbsp; 시퀀스 생성을 위한 Decoding Strategy가 다양하게 존재합니다. 
          <br>
          <strong>Greedy</strong>
          <br>
          <strong>Beam</strong>
          <br>
          <strong>Sampling</strong>
          <br>
          <strong>Sample P</strong>
          <br>

        </p>

      <h3 id="2.3" class="h3-mt">2.4 Data Noise</h3>
        <p>
          &nbsp; 데이터에 의도적인 노이즈를 추가하기 위한 전략을 서술
          MLM
        </p>

    <h2 id="3">3. Strategy</h2>
      <h3>3.1 Data Volumn Ratio</h3>
        <div class="img-container">
          <ul>
            <li>Standard Transformer
              <ul>
                <li>Encoder</li>
                <li>Decoder</li>
              </ul>
            </li>

            <li>Encoder Layer
              <ul>
                <li>Sub-Layer 1</li>
                <li>Sub-Layer 2</li>
              </ul>
            </li>

            <li>Decoder Layer
              <ul>
                <li>Sub-Layer 1</li>
                <li>Sub-Layer 2</li>
                <li>Sub-Layer 3</li>
              </ul>
            </li>

          </ul>
        </div>

      <h3 class="h3-mt">3.2 Standard Transfomrer</h3>
        <p>
          
        </p>

      <h3 id="3.2" class="h3-mt">3.3 Recurrent Transformer</h3>
        <p>
          
        </p>

      <h3 id="3.3" class="h3-mt">3.4 Evolved Transformer</h3>
        <p>
          
        </p>

    <h2 id="4" class="h3-mt">4. Experimental Setup</h2>
    
      <h3 id="4.1">4.1 Data Setups</h3>

      <h3 id="4.2" class="h3-mt">4.2 Model Setups</h3>
      
      <h3 id="4.3" class="h3-mt">4.3 Training Setups</h3>
      
      <h3 id="4.4" class="h3-mt">4.4 Model desc</h3>


    <h2 id="5" class="h3-mt">5. Result</h2>


    <h2 id="6" class="h3-mt">6. Conclusion</h2>


    <h2 id="7" class="h3-mt">7. Reference</h2>


  </div>
</div>

<div class="pagination">
  <a href="{{ '/project_05' | relative_url }}" class="btn-prev"></a>
  <a href="{{ '/project_01' | relative_url }}" class="btn-next"></a>
</div>