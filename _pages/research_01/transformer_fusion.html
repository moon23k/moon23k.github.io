---
layout: default
permalink: /sample
---


<div id="detail">
  <h1 class="title">Transformer Fusion</h1>
    
  <div class="post">




    <hr>
    <div class="callout">
      <p class="highlighted-text"><span>Project Objective</span></p>
      <div class="small-spacer"></div>
      <p>&nbsp; 이 프로젝트에서는 사전 학습된 언어 인코더 모델을 Seq2Seq 아키텍처에서 활용하기 위한 두 가지 방법론을 구현하고, 각 방법론의 성능을 세 가지 자연어 생성 과제에서 비교합니다. 변인이 되는 세 가지 모델구조는 각각 Simple, Fusion Model이라고 명명하며, 모델 구조라는 변인을 제외한 모든 실험 변수들은 동일하게 적용합니다. 자연어 생성 능력 평가를 위해 기계번역, 대화생성, 문서요약이라는 세 가지 과제를 선정했습니다.
      </p>
    </div>

    <div class="tblContents">
      <ul>

        <li>
          <a href="#1">1. &hairsp; Introduction</a>
        </li>

        <li>
          <a href="#2">2. &hairsp; Architecture</a>
        </li>

        <li>
          <a href="#3">3. &hairsp; Experimental Setup</a>
        </li>

        <li>
          <a href="#4">4. &hairsp; Results</a>
        </li>

        <li>
          <a href="#5">5. &hairsp; Conclusion</a>
        </li>

        <li>
          <a href="#6">6. &hairsp; Reference</a>
        </li>

      </ul>
    </div>
    <hr>


<!-- 1. Introduction -->    
    <h2 id="1">&hairsp; 1. &hairsp; Introduction</h2>
    <div class="chapter-box">

      <div class="list-container">
        <ul>
          <li>Description</li>
            <p> &nbsp; 최근 BERT를 필두로 한, 다양한 사전학습 인코더 모델들이 다양한 자연어 처리 영역에서 좋은 성과를 보이고 있습니다. 사전학습 인코더 모델의 경우, 그 적용 범위가 더 넓은 덕에 더 다양하게 연구되어지고 있는 반면, 상대적으로 사전학습 Seq2Seq 모델의 수는 적습니다. 때문에 이 프로젝트에서는 다양한 사전학습 인코더 모델을 Transformer Seq2Seq 구조안에서 사용하기 위한 두 가지 모델링 방법론을 직접 구현하고, 각 방법론 간의 성능 차이를 직접 확인합니다. <br>

            실험 모델의 Backbone 모델은 Transformer이며, 사전학습 인코더 모델로는 하드웨어의 한계를 고려해 비교적 가벼운 AlBERT 사전학습 모델을 선정했습니다. 가장 간단한 방식의 사전학습 인코더의 적용 모델을 Simple Model, 그리고 기본 Transformer 골조 안에서 사전학습 모델의 결과값을 적절히 혼합해서 사용하기 위한 모델구조를 Fusion Model이라고 명명합니다. 그리고 각 모델의 성능은 기계번역, 대화생성, 문서요약이라는 세 가지 자연어 생성 과제에서 진행합니다. 모델 구조를 제외한 모든 실험 변인은 동일하게 적용합니다.

            </p>

          <div class="half-spacer"></div>
          <li>Objective
            <ul>
              <li style="font-weight: normal;">사전학습 인코더 모델을 Transformer Seq2Seq에서 활용하기 위한 방법론 별 자연어 생성 능력 비교</li>
              <li style="font-weight: normal;">추후 다양한 사전학습 모델 사용을 통한 다양한 연구로의 발전 가능성 제고</li>
            </ul>
          </li>

        </ul>
      </div>

    </div>




<!-- 2. Architecture -->    
    <h2 id="2">&hairsp; 2. &hairsp; Architecture</h2>
    <div class="chapter-box">

      <div class="list-container">
        <ul>
          <li>Simple Model</li>
            <div class="dual-container">
              <img src="{{ 'assets/img/research_01/standard_transformer.png' | relative_url }}" style="width: 27%; height: 25%;">
              <p>&nbsp; Transformer Seq2Seq 모델 구조에서, Encoder를 Pretrained Language Encoder Model로 대체한 모델입니다. 연산은 기존 Transformer Seq2Seq의 연산 과정과 동일하게 적용됩니다. Encoder는 Context Vector를 생성하고, Decoder는 Context Vector와 Teacher Forcing을 활용해 예측값을 생성하는 방식으로 학습이 진행됩니다..
              </p>
            </div>      
            <div class="code-container">
              <div class="code-snippet">
                <pre><code class="python">class Decoder(nn.Module):
    def __init__(self, config, ple_embeddings):
        super(Decoder, self).__init__()

        self.embeddings = ple_embeddings
        self.mapping = Mapping(config.emb_dim, config.hidden_dim)

        layer = nn.TransformerDecoderLayer(
            d_model=config.hidden_dim,
            nhead=config.n_heads,
            dim_feedforward=config.pff_dim,
            dropout=config.dropout_ratio,
            activation='gelu',
            batch_first=True,
            norm_first=True
        )
        self.layers = clones(layer, config.n_layers)


    def forward(self, x, memory, e_mask=None, d_mask=None):   
        x = self.mapping(self.embeddings(x))
        for layer in self.layers:
            x = layer(
                x, memory, 
                memory_key_padding_mask=e_mask,
                tgt_mask=d_mask,
            )

        return x



class SimpleModel(nn.Module):
    def __init__(self, config):
        super(SimpleModel, self).__init__()

        self.pad_id = config.pad_id
        self.device = config.device
        self.vocab_size = config.vocab_size

        self.ple = load_ple(config)
        self.mapping = Mapping(self.ple.config.hidden_size, config.hidden_dim)

        self.decoder = Decoder(config, self.ple.embeddings)
        self.generator = nn.Linear(config.hidden_dim, self.vocab_size)

        self.criterion = nn.CrossEntropyLoss()
        self.out = namedtuple('Out', 'logit loss')


    @staticmethod    
    def shift_y(x):
        return x[:, :-1], x[:, 1:]    


    def mask(self, x, y=None):
        x_mask = x == self.pad_id

        if y is None:
            return x_mask

        sz = y.size(1)
        y_mask = torch.triu(torch.full((sz, sz), float('-inf')), diagonal=1).to(self.device)

        return x_mask, y_mask


    def encode(self, x, ple_mask):
        x = self.ple(input_ids=x, attention_mask=ple_mask).last_hidden_state
        return self.mapping(x)
        

    def decode(self, x, memory, e_mask, d_mask):
        return self.decoder(x, memory, e_mask, d_mask)


    def forward(self, input_ids, attention_mask, labels):
        x = input_ids
        y, label = self.shift_y(labels)
        e_mask, d_mask = self.mask(x, y)

        memory = self.encode(x, attention_mask)
        dec_out = self.decode(y, memory, e_mask, d_mask)
        logit = self.generator(dec_out)

        self.out.logit = logit
        self.out.loss = self.criterion(
            logit.contiguous().view(-1, self.vocab_size), 
            label.contiguous().view(-1)
        )

        return self.out</code></pre>
              </div>
            </div>


          <div class="spacer"></div>
          <li>Fusion Model</li>
          <div class="dual-container">
            <img src="{{ 'assets/img/research_01/fusion.png' | relative_url }}" style="width: 40%; height: 40%;">
            <p>&nbsp; PLE Fused Model은 "Incorporating BERT into Neural Machine Translation" 논문에서 제시한 모델 구조를 사용합니다. 사전학습된 인코더 모델을 그대로 인코더로만 사용하는것이 아니라, 인코딩과 디코딩 과정에 추가적인 정보전달을 위한 모델로 활용합니다.
            </p>
          </div>

          <div class="code-container">
            <div class="code-snippet">
              <pre><code class="python">class EncoderLayer(nn.Module):
    def __init__(self, config):
        super(EncoderLayer, self).__init__()

        self.self_attn = MultiHeadAttention(config)
        self.ple_attn = MultiHeadAttention(config)
        self.pff = PositionwiseFeedForward(config)


    def forward(self, x, ple_out, e_mask):

        s_out = self.self_attn(x, key_padding_mask=e_mask)
        p_out = self.ple_attn(x, ple_out, key_padding_mask=e_mask)
        out = x + s_out * 0.5 + p_out * 0.5

        return out + self.pff(out)




class DecoderLayer(nn.Module):
    def __init__(self, config):
        super(DecoderLayer, self).__init__()

        self.self_attn = MultiHeadAttention(config)
        self.ple_attn = MultiHeadAttention(config)
        self.enc_attn = MultiHeadAttention(config)
        self.pff = PositionwiseFeedForward(config)


    def forward(self, x, memory, ple_out, e_mask=None, d_mask=None):
        
        m, p = memory, ple_out

        s_out = self.self_attn(x, attn_mask=d_mask)
        x = x + s_out

        p_out = self.ple_attn(x, p, key_padding_mask=e_mask)
        e_out = self.enc_attn(x, m, key_padding_mask=e_mask)
        out = x + p_out * 0.5 + e_out * 0.5

        return out + self.pff(out)



class Encoder(nn.Module):
    def __init__(self, config, ple_embeddings):
        super(Encoder, self).__init__()

        self.embeddings = ple_embeddings
        self.mapping = Mapping(config.emb_dim, config.hidden_dim)

        self.layers = clones(EncoderLayer(config), config.n_layers)
        self.norm = nn.LayerNorm(config.hidden_dim)


    def forward(self, x, ple_out, e_mask):
        x = self.mapping(self.embeddings(x))
        for layer in self.layers:
            x = layer(x, ple_out, e_mask)
        return self.norm(x)



class Decoder(nn.Module):
    def __init__(self, config, ple_embeddings):
        super(Decoder, self).__init__()

        self.embeddings = ple_embeddings
        self.mapping = Mapping(config.emb_dim, config.hidden_dim)
        
        self.layers = clones(DecoderLayer(config), config.n_layers)
        self.norm = nn.LayerNorm(config.hidden_dim)
        

    def forward(self, x, memory, ple_out, e_mask=None, d_mask=None):
        x = self.mapping(self.embeddings(x))
        for layer in self.layers:
            x = layer(x, memory, ple_out, e_mask, d_mask)
        return self.norm(x)



class FusionModel(nn.Module):
    def __init__(self, config):
        super(FusionModel, self).__init__()

        self.device = config.device
        self.pad_id = config.pad_id
        self.vocab_size = config.vocab_size

        self.ple = load_ple(config)
        self.mapping = Mapping(self.ple.config.hidden_size, config.hidden_dim)
        
        self.encoder = Encoder(config, self.ple.embeddings)
        self.decoder = Decoder(config, self.ple.embeddings)
        self.generator = nn.Linear(config.hidden_dim, self.vocab_size)

        self.criterion = nn.CrossEntropyLoss()
        self.out = namedtuple('Out', 'logit loss')


    @staticmethod    
    def shift_y(x):
        return x[:, :-1], x[:, 1:]    


    def mask(self, x, y=None):
        x_mask = x == self.pad_id

        if y is None:
            return x_mask

        sz = y.size(1)
        y_mask = torch.triu(torch.full((sz, sz), float('-inf')), diagonal=1).to(self.device)

        return x_mask, y_mask


    def encode(self, x, ple_out, e_mask):        
        return self.encoder(x, ple_out, e_mask)
    

    def decode(self, x, m, ple_out, e_mask, d_mask=None):
        return self.decoder(x, m, ple_out, e_mask, d_mask)


    def forward(self, input_ids, attention_mask, labels):        
        x = input_ids
        y, label = self.shift_y(labels)
        e_mask, d_mask = self.mask(x, y)

        ple_out = self.ple(input_ids=x, attention_mask=attention_mask).last_hidden_state
        ple_out = self.mapping(ple_out)

        memory = self.encode(x, ple_out, e_mask)
        d_out = self.decode(y, memory, ple_out, e_mask, d_mask)
        
        logit = self.generator(d_out)
        self.out.logit = logit
        self.out.loss = self.criterion(
            logit.contiguous().view(-1, self.vocab_size), 
            label.contiguous().view(-1)
        )

        return self.out</code></pre>
            </div>
          </div>
          
        </ul>
      </div>
    </div>




<!-- 3. Experimental Setup -->
    <h2 id="3">&hairsp; 3. &hairsp; Experimental Setup</h2>
    <div class="chapter-box">

      <div class="list-container">
        <ul>
          <li>Data Setup
            <div class="dual-container">
              <ul>
                <li><b>Translation Task:</b> &nbsp; WMT'14 En-De</li>
                <li><b>Dialogue Task:</b> &nbsp; Daily Dialogue</li>
                <li><b>Summarization Task:</b> &nbsp; CNN Daily</li>
                <li><b>Tokenizer:</b> &nbsp; AlBERT Tokenizer</li>
              </ul>
              <ul>
                <li><b>Train Data Volumn:</b> &nbsp; 50,000</li>
                <li><b>Valid Data Volumn:</b> &nbsp; 5,000</li>
                <li><b>Test Data Volumn:</b> &nbsp; 100</li>
                <li><b>Vocab Size:</b> &nbsp; 15,000</li>                
              </ul>
            </div>
          </li>

          <div class="half-spacer"></div>
          <li>Model Setup
            <div class="dual-container">
              <ul>
                <li><b>PLE Architecture:</b> &nbsp; AlBERT</li>
                <li><b>Input & Output Dim:</b> &nbsp; 15,000</li>
                <li><b>Hidden Dim:</b> &nbsp; 256</li>
                <li><b>Pff Dim:</b> &nbsp; 512</li>
                <li><b>Num Layers:</b> &nbsp; 3</li>
                <li><b>Num Heads:</b> &nbsp; 8</li>
              </ul>
              <ul>
                <li><b>Simple Model Total Params:</b> &nbsp; 21,995,824</li>
                <li><b>Simple Model Trainable Params:</b> &nbsp; 10,312,240</li>
                <li><b>Simple Model Size:</b> &nbsp; 83.915 MB</li>              
                <li><b>Fusion Model Total Params:</b> &nbsp; 25,193,264</li>
                <li><b>Fusion Model Trainable Params:</b> &nbsp; 13,509,680</li>
                <li><b>Fusion Model Size:</b> &nbsp; 96.112 MB</li>                              
              </ul>
            </div>
          </li>

          <div class="half-spacer"></div>
          <li>Training Setup
            <div class="dual-container">
              <ul>
                <li><b>Num Epochs:</b> &nbsp; 10</li>
                <li><b>Batch Size:</b> &nbsp; 32</li>
                <li><b>Learning Rate:</b> &nbsp; 5e-4</li>
                <li><b>Learning Rate for PLE:</b> &nbsp; 5e-5</li>                
              </ul>
              <ul>
                <li><b>LR Scheduler:</b> &nbsp; ReduceLROnPlateau</li>                
                <li><b>Optimizer:</b> &nbsp; AdamW</li>
                <li><b>Gradient Accumulation Steps:</b> &nbsp; 4
                <li><b>Early Stop Patience:</b> &nbsp; 3</li>
              </ul>
            </div>
          </li>
        </ul>
      </div>
    </div>


<!-- 4. Result -->
    <h2 id="4">&hairsp; 4. &hairsp; Result</h2>
    <div class="chapter-box">

      <div class="list-container">
        <ul>
          <li>Machine Translation</li>

            <table class="result-table">
              <thead>
                <tr>
                  <th>Model Type</th>
                  <th>Eval Score</th>
                  <th>Epoch Time</th>
                  <th>Avg GPU</th>
                  <th>Max GPU</th>
                </tr>
              </thead>
              <tbody>
                <tr>
                  <td>Transformer</td>
                  <td>13.72</td>
                  <td>53s</td>
                  <td>0.20GB</td>
                  <td>0.95GB</td>
                </tr>
                <tr>
                  <td>Simple</td>
                  <td>9.62</td>
                  <td>2m 13s</td>
                  <td>0.22GB</td>
                  <td>1.75GB</td>
                </tr>
                <tr>
                  <td>Fusion</td>
                  <td>9.99</td>
                  <td>2m 18s</td>
                  <td>0.22GB</td>
                  <td>1.64GB</td>
                </tr>

              </tbody>
            </table>
          
          <div class="spacer"></div>
          <li>Dialogue Generation</li>
            <table class="result-table">
              <thead>
                <tr>
                  <th>Model Type</th>
                  <th>Eval Score</th>
                  <th>Epoch Time</th>
                  <th>Avg GPU</th>
                  <th>Max GPU</th>
                </tr>
              </thead>
              <tbody>
                <tr>
                  <td>Transformer</td>
                  <td>1.34</td>
                  <td>42s</td>
                  <td>0.20GB</td>
                  <td>0.85GB</td>
                </tr>                
                <tr>
                  <td>Simple</td>
                  <td>6.56</td>
                  <td>1m 36s</td>
                  <td>0.20GB</td>
                  <td>1.27GB</td>
                </tr>
                <tr>
                  <td>Fusion</td>
                  <td>6.44</td>
                  <td>1m 40s</td>
                  <td>0.20GB</td>
                  <td>1.19GB</td>
                </tr>

              </tbody>
            </table>

                
          <div class="spacer"></div>
          <li>Text Summarization</li>
            <table class="result-table">
              <thead>
                <tr>
                  <th>Model Type</th>
                  <th>Eval Score</th>
                  <th>Epoch Time</th>
                  <th>Avg GPU</th>
                  <th>Max GPU</th>
                </tr>
              </thead>
              <tbody>
                <tr>
                  <td>Transformer</td>
                  <td>7.68</td>
                  <td>2m 41s</td>
                  <td>0.21GB</td>
                  <td>2.89GB</td>
                </tr>                
                <tr>
                  <td>Simple</td>
                  <td>13.80</td>
                  <td>16m 29s</td>
                  <td>0.22GB</td>
                  <td>2.30GB</td>
                </tr>
                <tr>
                  <td>Fusion</td>
                  <td>12.42</td>
                  <td>18m 23s</td>
                  <td>0.22GB</td>
                  <td>2.08GB</td>
                </tr>

              </tbody>
            </table>


          <div class="spacer"></div>
          <li>Result Analysis</li>
            <p>&nbsp; Simple Model이 Fusion Model에 비해 좋은 성능을 보입니다. 하나 특이한 결과는 최대 GPU 사용량에서 더 크고, 연산과정이 복잡한 Fusion모델의 효율성이 더 높게 나온다는 점입니다. 이를 이전 실험에서 Transformer의 성능과 비교해보
            </p>
        </ul>
      </div>
    </div>



<!-- 5. Conclusion -->
    <h2 id="5">&hairsp; 5. &hairsp; Conclusion</h2>
    <div class="chapter-box">

      <div class="list-container">
        <ul>
          <li>Transformer의 틀 안에서 사전학습 언어 인코더 모델을 활용할 수 있는 아키텍처 구현</li>
            <p>&nbsp; 가장 
            </p>

          <div class="half-spacer"></div>
          <li>세 가지 순환 신경망의 성능 비교 검증</li>
            <p>LSTM은 게이트 메커니즘을 통해 장기 의존성 문제를 해결하는 데 강점을 보입니다. 복잡한 문맥을 학습하고 기억하는 데 탁월하며, 긴 시퀀스 데이터에서 뛰어난 성능을 발휘할 수 있습니다. 하지만 LSTM은 더 많은 파라미터를 가지고 있어서 더 많은 데이터와 계산 리소스가 필요할 수 있습니다.
            </p>

        </ul>
      </div>
    </div>



<!-- Reference -->    
    <h2 id="6">&hairsp; 6. Reference</h2>
    <div class="chapter-box">
      <a class="reference" href="https://arxiv.org/abs/1706.03762">&nbsp; Attention Is All You Need</a> <br> 
      <a class="reference" href="https://arxiv.org/abs/2002.06823">&nbsp; Incorporating BERT into Neural Machine Translation</a> <br>
    </div>


  </div>
</div>


<div class="pagination">
  <a href="{{ '/balance' | relative_url }}" class="btn-prev"><span>Transformer Balance</span></a>
  <a href="{{ '/eff_train' | relative_url }}" class="btn-next"><span>Efficient Training</span></a>
</div>