---
layout: default
permalink: /transformer_fusion
---


<div id="detail">
  <h1 class="title">Transformer&hairsp; Fusion</h1>
    
  <div class="post">
    <hr>
    <div class="callout">
      <p class="highlighted-text"><span>Project Objective</span></p>
      <div class="small-spacer"></div>
      <p>&nbsp; 이 프로젝트에서는 사전학습된 언어 인코더 모델을 Seq2Seq 아키텍처에서 활용하기 위한 7개의 모델 구조를 제시하고, 각 모델의 자연어 생성 능력을 검증합니다. 변인이 되는 세 가지 모델 구조는 PLE와의 결합 방식에 따라 Simple, Parallel, Sequential 모델이라고 명명하며, Parallel, Sequential 모델은 Encoder, Decoder 그리고 Encoder & Decoder 세가지 방식으로 적용하며 PLE의 결합 시너지를 면밀하게 살펴봅니다.

      </p>
    </div>

    <div class="tblContents">
      <ul>

        <li>
          <a href="#1">1. &hairsp; Introduction</a>
        </li>

        <li>
          <a href="#2">2. &hairsp; Architecture</a>
        </li>

        <li>
          <a href="#3">3. &hairsp; Experimental Setup</a>
        </li>

        <li>
          <a href="#4">4. &hairsp; Results</a>
        </li>

        <li>
          <a href="#5">5. &hairsp; Conclusion</a>
        </li>

        <li>
          <a href="#6">6. &hairsp; Reference</a>
        </li>

      </ul>
    </div>
    <hr>


<!-- 1. Introduction -->    
    <h2 id="1">&hairsp; 1. Introduction</h2>
    <div class="chapter-box">

      <div class="list-container">
        <ul>
          <li>Description</li>
            <p> &nbsp; 최근 BERT를 필두로 한, 다양한 사전학습 인코더 모델들이 다양한 자연어 처리 영역에서 좋은 성과를 보이고 있습니다. 사전학습 인코더 모델의 경우, 그 적용 범위가 더 넓은 덕에 더 다양하게 연구되어지고 있는 반면, 상대적으로 사전학습 Seq2Seq 모델의 수는 적습니다. 때문에 이 프로젝트에서는 다양한 사전학습 인코더 모델을 Transformer Seq2Seq 구조안에서 사용하기 위한 세 가지 구조 방법론 안에서 적용범위에 차별을 두며 총 7개의 모델을 구현하고, 각 모델의 성능을 직접 확인합니다. <br>

            실험 모델의 Backbone 모델은 Transformer이며, 사전학습 인코더 모델로는 하드웨어의 한계를 고려해 비교적 가벼운 AlBERT 사전학습 모델을 선정했습니다. 가장 간단한 방식의 사전학습 인코더의 적용 모델을 Simple Model, 그리고 기본 Transformer 골조 안에서 사전학습 모델의 결과값을 적절히 혼합해서 사용하기 위한 모델구조를 Fusion Model이라고 명명합니다. 그리고 각 모델의 성능은 기계번역, 대화생성, 문서요약이라는 세 가지 자연어 생성 과제에서 진행합니다. 모델 구조를 제외한 모든 실험 변인은 동일하게 적용합니다.

            </p>

          <div class="half-spacer"></div>
          <li>Objective
            <ul>
              <li style="font-weight: normal;">사전학습 인코더 모델을 Transformer Seq2Seq에서 활용하기 위한 방법론 별 자연어 생성 능력 비교</li>
              <li style="font-weight: normal;">추후 다양한 사전학습 모델 사용을 통한 다양한 연구로의 발전 가능성 제고</li>
            </ul>
          </li>

        </ul>
      </div>

    </div>



<!-- 2. Background -->    
    <h2 id="2">&hairsp; 2.&hairsp; Background</h2>
    <div class="chapter-box">

      <div class="list-container">
        <ul>
          <li>Pretrained Encoder Model
            <div class="small-spacer"></div>
            <ul>
              <li>트랜스포머의 인코더는 입력 시퀀스의 단어들을 임베딩하고, 다중 헤드 어텐션과 피드포워드 신경망을 통해 정보를 추상화. 각 인코더 레이어는 잔여 연결과 층 정규화를 활용하여 안정성을 유지하며, 입력 문장의 특징을 계층적으로 추출</li>
              <li>디코더는 인코더의 출력을 기반으로 출력 시퀀스를 생성. 각 디코더 레이어는 다중 헤드 어텐션을 사용하여 인코더의 출력과 현재까지의 디코더 입력에 대한 어텐션을 계산하고, 피드포워드 신경망을 통해 출력을 생성. 잔여 연결과 층 정규화는 안정성을 제공하며 디코딩 과정을 안정화.</li>
              <li>인코더와 디코더는 각각의 레이어를 통해 상호작용하며, 어텐션 메커니즘을 활용해 입력 문장의 문맥 정보를 고려하고 출력 문장을 생성. 트랜스포머의 인코더-디코더 구조는 기존의 번역 모델보다 효과적인 학습과 예측을 가능케 함.</li>
            </ul>
          </li>

          <div class="spacer"></div>
          <li>Lack of Pretrained Seq2Seq Model
            <div class="small-spacer"></div>
            <ul>
              <li>트랜스포머의 인코더는 입력 시퀀스의 단어들을 임베딩하고, 다중 헤드 어텐션과 피드포워드 신경망을 통해 정보를 추상화. 각 인코더 레이어는 잔여 연결과 층 정규화를 활용하여 안정성을 유지하며, 입력 문장의 특징을 계층적으로 추출</li>
              <li>디코더는 인코더의 출력을 기반으로 출력 시퀀스를 생성. 각 디코더 레이어는 다중 헤드 어텐션을 사용하여 인코더의 출력과 현재까지의 디코더 입력에 대한 어텐션을 계산하고, 피드포워드 신경망을 통해 출력을 생성. 잔여 연결과 층 정규화는 안정성을 제공하며 디코딩 과정을 안정화.</li>
              <li>인코더와 디코더는 각각의 레이어를 통해 상호작용하며, 어텐션 메커니즘을 활용해 입력 문장의 문맥 정보를 고려하고 출력 문장을 생성. 트랜스포머의 인코더-디코더 구조는 기존의 번역 모델보다 효과적인 학습과 예측을 가능케 함.</li>
            </ul>
          </li>


          <div class="spacer"></div>
          <li>Transformer Fusion
            <div class="small-spacer"></div>
            <ul>
              <div class="dual-container" style="display: flex; justify-content: center; align-items: center;">
                <img src="{{ 'assets/img/research_01/standard_transformer.png' | relative_url }}" style="width: 21%; height: 50%;">
                <img src="{{ 'assets/img/research_01/fusion.png' | relative_url }}" style="width: 43%; height: 50%;">              
              </div>
              <div class="small-spacer"></div>
              <li>트랜스포머의 인코더는 입력 시퀀스의 단어들을 임베딩하고, 다중 헤드 어텐션과 피드포워드 신경망을 통해 정보를 추상화. 각 인코더 레이어는 잔여 연결과 층 정규화를 활용하여 안정성을 유지하며, 입력 문장의 특징을 계층적으로 추출</li>
              <li>디코더는 인코더의 출력을 기반으로 출력 시퀀스를 생성. 각 디코더 레이어는 다중 헤드 어텐션을 사용하여 인코더의 출력과 현재까지의 디코더 입력에 대한 어텐션을 계산하고, 피드포워드 신경망을 통해 출력을 생성. 잔여 연결과 층 정규화는 안정성을 제공하며 디코딩 과정을 안정화.</li>
              <li>인코더와 디코더는 각각의 레이어를 통해 상호작용하며, 어텐션 메커니즘을 활용해 입력 문장의 문맥 정보를 고려하고 출력 문장을 생성. 트랜스포머의 인코더-디코더 구조는 기존의 번역 모델보다 효과적인 학습과 예측을 가능케 함.</li>
            </ul>
          </li>


        </ul>
      </div>
    </div>



<!-- 3. Architecture -->    
    <h2 id="3">&hairsp; 3.&hairsp; Architecture</h2>
    <div class="chapter-box">

      <div class="list-container">
        <ul>
          <li>Simple&hairsp; Fusion&hairsp; Model
            <ul>
              <li>표준 트랜스포머의 인코더를 PLE로 대체한 모델 구조</li>
              <li>가장 직관적이고, 단순한 Fusion방식으로, 구현이 간단하다는 장점이 있음</li>
              <li>인코더를 제외한 모든 모델 구조는 표준 트랜스포머와 동일</li>
            </ul>
          </li>


          <div class="spacer"></div>
          <li>Parallel&hairsp; Fusion&hairsp; Model
            <ul>
              <li>"Incorporating BERT into Neural Machine Translation" 논문에서 제시한 모델 구조</li>
              <li>PLE를 인코더로만 사용하는것이 아니라, 인코딩과 디코딩 과정에 추가적인 정보전달을 위한 모델로 활용</li>
            </ul>
          </li>


          <div class="spacer"></div>
          <li>Sequential&hairsp; Fusion&hairsp; Model
            <ul>
              <li>어텐션이 순차적으로 이어지도록 유도한 모델 구조</li>
              <li>단순한 덧셈 연산으로는 서로 상이한 기저차원의 값들의 요소들을 제대로 반영하기 어려울것 같다는 가정에서 출발한 모델</li>
              <li></li>
            </ul>
          </li>

        </ul>
      </div>
    </div>




<!-- 4. Experimental Setup -->
    <h2 id="4">&hairsp; 4.&hairsp; Experimental Setup</h2>
    <div class="chapter-box">

      <div class="list-container">
        <ul>
          <li>Data Setup
            <div class="small-spacer"></div>
            <div class="dual-container">
              <ul>
                <li><b>Translation Task:</b> &nbsp; WMT'14 En-De</li>
                <li><b>Dialogue Task:</b> &nbsp; Daily Dialogue</li>
                <li><b>Summarization Task:</b> &nbsp; CNN Daily</li>
                <li><b>Tokenizer:</b> &nbsp; AlBERT Pretrained Tokenizer</li>
              </ul>
              <ul>
                <li><b>Train Data Volumn:</b> &nbsp; 50,000</li>
                <li><b>Valid Data Volumn:</b> &nbsp; 5,000</li>
                <li><b>Test Data Volumn:</b> &nbsp; 100</li>
                <li><b>Vocab Size:</b> &nbsp; 15,000</li>                
              </ul>
            </div>
          </li>

          <div class="spacer"></div>
          <li>Training Setup
            <div class="small-spacer"></div>
            <div class="dual-container">
              <ul>
                <li><b>Hardware</b>: &nbsp; Google Colab TPU</li>
                <li><b>Num Epochs:</b> &nbsp; 10</li>
                <li><b>Batch Size:</b> &nbsp; 32</li>
                <li><b>Optimizer:</b> &nbsp; AdamW</li>
              </ul>
              <ul>
                <li><b>Learning Rate:</b> &nbsp; 5e-4</li>
                <li><b>Learning Rate for PLE:</b> &nbsp; 5e-5</li>                            
                <li><b>LR Scheduler:</b> &nbsp; ReduceLROnPlateau</li>                
                <li><b>Gradient Accumulation Steps:</b> &nbsp; 4
              </ul>
            </div>
          </li>
        

          <div class="spacer"></div>
          <li>Model Setup
            <div class="small-spacer"></div>
            <div class="dual-container">
              <ul>
                <li><b>PLE Architecture:</b> &nbsp; AlBERT</li>
                <li><b>Input & Output Dim:</b> &nbsp; 30,000</li>
                <li><b>Hidden Dim:</b> &nbsp; 256</li>
              </ul>
              <ul>
                <li><b>Pff Dim:</b> &nbsp; 512</li>
                <li><b>Num Layers:</b> &nbsp; 3</li>
                <li><b>Num Heads:</b> &nbsp; 8</li>                              
              </ul>
            </div>
          </li>


          <div class="spacer"></div>
          <li>Model Size</li>
          <table class="result-table">
            <thead>
              <tr>
                <th>&nbsp; Model Type&nbsp; </th>
                <th>&nbsp; Total Params&nbsp; </th>
                <th>&nbsp; Trainable Params&nbsp; </th>
                <th>&nbsp; Model Size&nbsp; </th>
              </tr>
            </thead>

            <tbody>
              <tr>
                <td>Simple Model</td>
                <td>22,192,688</td>
                <td>10,509,104</td>
                <td>84.666 MB</td>
              </tr>

              <tr>
                <td>Parallel Model</td>
                <td>25,193,264</td>
                <td>13,509,680</td>
                <td>96.112 MB</td>
              </tr>

              <tr>
                <td>Sequential Model</td>
                <td>25,199,408</td>
                <td>13,515,824</td>
                <td>96.136 MB</td>
              </tr>              

            </tbody>
          </table>

        </ul>
      </div>
    </div>


<!-- 5. Result -->
    <h2 id="5">&hairsp; 5.&hairsp; Result</h2>
    <div class="chapter-box">

      <div class="list-container">
        <ul>
          <li>Machine Translation</li>

            <table class="result-table">
              <thead>
                <tr>
                  <th>Model Type</th>
                  <th>Eval Score</th>
                  <th>Epoch Time</th>
                  <th>Avg GPU</th>
                  <th>Max GPU</th>
                </tr>
              </thead>
              <tbody>
                <tr>
                  <td>Transformer</td>
                  <td>13.72</td>
                  <td>53s</td>
                  <td>0.20GB</td>
                  <td>0.95GB</td>
                </tr>
                
                <tr>
                  <td>Simple Fusion</td>
                  <td>9.58</td>
                  <td>2m 13s</td>
                  <td>0.22GB</td>
                  <td>1.72GB</td>
                </tr>

                <tr>
                  <td>Parallel Fusion</td>
                  <td>9.58</td>
                  <td>2m 31s</td>
                  <td>0.26GB</td>
                  <td>1.92GB</td>
                </tr>

                <tr>
                  <td>Sequential Fusion</td>
                  <td>9.57</td>
                  <td>2m 32s</td>
                  <td>0.26GB</td>
                  <td>1.94GB</td>
                </tr>

              </tbody>
            </table>
          
          <div class="spacer"></div>
          <li>Dialogue Generation</li>
            <table class="result-table">
              <thead>
                <tr>
                  <th>Model Type</th>
                  <th>Eval Score</th>
                  <th>Epoch Time</th>
                  <th>Avg GPU</th>
                  <th>Max GPU</th>
                </tr>
              </thead>
              <tbody>
                <tr>
                  <td>Transformer</td>
                  <td>1.34</td>
                  <td>42s</td>
                  <td>0.20GB</td>
                  <td>0.85GB</td>
                </tr>                
                
                <tr>
                  <td>Simple Fusion</td>
                  <td>6.55</td>
                  <td>1m 38s</td>
                  <td>0.21GB</td>
                  <td>1.25GB</td>
                </tr>

                <tr>
                  <td>Parallel Fusion</td>
                  <td>6.59</td>
                  <td>1m 54s</td>
                  <td>0.24GB</td>
                  <td>1.42GB</td>
                </tr>

                <tr>
                  <td>Sequential Fusion</td>
                  <td>6.55</td>
                  <td>1m 53s</td>
                  <td>0.24GB</td>
                  <td>1.45GB</td>
                </tr>

              </tbody>
            </table>


          <div class="spacer"></div>
          <li>Text Summarization</li>
            <table class="result-table">
              <thead>
                <tr>
                  <th>Model Type</th>
                  <th>Eval Score</th>
                  <th>Epoch Time</th>
                  <th>Avg GPU</th>
                  <th>Max GPU</th>
                </tr>
              </thead>
              <tbody>
                <tr>
                  <td>Transformer</td>
                  <td>1.34</td>
                  <td>3m 42s</td>
                  <td>0.20GB</td>
                  <td>0.85GB</td>
                </tr>                
                
                <tr>
                  <td>Simple Fusion</td>
                  <td>7.33</td>
                  <td>18m 47s</td>
                  <td>0.22GB</td>
                  <td>3.01GB</td>
                </tr>

                <tr>
                  <td>Parallel Fusion</td>
                  <td>7.36</td>
                  <td>22m 9s</td>
                  <td>0.25GB</td>
                  <td>4.01GB</td>
                </tr>

                <tr>
                  <td>Sequential Fusion</td>
                  <td>7.30</td>
                  <td>22m 30s</td>
                  <td>0.25GB</td>
                  <td>4.01GB</td>
                </tr>

              </tbody>
            </table>


          <div class="spacer"></div>
          <li>Ablation Study</li>

            <table class="result-table">
              <thead>
                <tr>
                  <th>Model Type</th>
                  <th>Translation Score</th>
                  <th>Epoch Time</th>
                  <th>Avg GPU</th>
                  <th>Max GPU</th>
                </tr>
              </thead>
              <tbody>

                <tr>
                  <td>Parallel Encoder Fusion</td>
                  <td>9.61</td>
                  <td>2m 27s</td>
                  <td>0.25GB</td>
                  <td>1.87GB</td>
                </tr>

                <tr>
                  <td>Parallel Decoder Fusion</td>
                  <td>9.60</td>
                  <td>2m 29s</td>
                  <td>0.25GB</td>
                  <td>1.89GB</td>
                </tr>

                <tr>
                  <td>Sequential Encoder Fusion</td>
                  <td>9.62</td>
                  <td>2m 26s</td>
                  <td>0.25GB</td>
                  <td>1.88GB</td>
                </tr>

                <tr>
                  <td>Sequential Decoder Fusion</td>
                  <td>9.58</td>
                  <td>2m 27s</td>
                  <td>0.25GB</td>
                  <td>1.90GB</td>
                </tr>

              </tbody>
            </table>

            <table class="result-table">
              <thead>
                <tr>
                  <th>Model Type</th>
                  <th>Dialogue Score</th>
                  <th>Epoch Time</th>
                  <th>Avg GPU</th>
                  <th>Max GPU</th>
                </tr>
              </thead>
              <tbody>

                <tr>
                  <td>Parallel Encoder Fusion</td>
                  <td>6.50</td>
                  <td>1m 51s</td>
                  <td>0.23GB</td>
                  <td>1.39GB</td>
                </tr>

                <tr>
                  <td>Parallel Decoder Fusion</td>
                  <td>6.57</td>
                  <td>1m 48s</td>
                  <td>0.23GB</td>
                  <td>1.40GB</td>
                </tr>

                <tr>
                  <td>Sequential Encoder Fusion</td>
                  <td>6.56</td>
                  <td>1m 49s</td>
                  <td>0.23GB</td>
                  <td>1.40GB</td>
                </tr>

                <tr>
                  <td>Sequential Decoder Fusion</td>
                  <td>6.54</td>
                  <td>1m 50s</td>
                  <td>0.23GB</td>
                  <td>1.41GB</td>
                </tr>

              </tbody>
            </table>


          <div class="spacer"></div>
          <li>Result Analysis</li>
            <p>&nbsp; Simple Model이 Fusion Model에 비해 좋은 성능을 보입니다. 하나 특이한 결과는 최대 GPU 사용량에서 더 크고, 연산과정이 복잡한 Fusion모델의 효율성이 더 높게 나온다는 점입니다. 이를 이전 실험에서 Transformer의 성능과 비교해보
            </p>
        </ul>
      </div>
    </div>



<!-- 6. Conclusion -->
    <h2 id="6">&hairsp; 6.&hairsp; Conclusion</h2>
    <div class="chapter-box">

      <div class="list-container">
        <ul>
          <li>Transformer의 틀 안에서 사전학습 언어 인코더 모델을 활용할 수 있는 아키텍처 구현</li>
            <p>&nbsp; 가장 
            </p>

          <div class="half-spacer"></div>
          <li>세 가지 순환 신경망의 성능 비교 검증</li>
            <p>LSTM은 게이트 메커니즘을 통해 장기 의존성 문제를 해결하는 데 강점을 보입니다. 복잡한 문맥을 학습하고 기억하는 데 탁월하며, 긴 시퀀스 데이터에서 뛰어난 성능을 발휘할 수 있습니다. 하지만 LSTM은 더 많은 파라미터를 가지고 있어서 더 많은 데이터와 계산 리소스가 필요할 수 있습니다.
            </p>

        </ul>
      </div>
    </div>



<!-- Reference -->    
    <h2 id="7">&hairsp; 7.&hairsp; Reference</h2>
    <div class="chapter-box">
      <a class="reference" href="https://arxiv.org/abs/1706.03762">&nbsp; Attention Is All You Need</a> <br> 
      <a class="reference" href="https://arxiv.org/abs/2002.06823">&nbsp; Incorporating BERT into Neural Machine Translation</a> <br>
    </div>


  </div>
</div>


<div class="pagination">
  <a href="{{ '/transformer_variants' | relative_url }}" class="btn-prev"><span>Transformer Variants</span></a>
  <a href="{{ '/cpt_train' | relative_url }}" class="btn-next"><span>Customized Pre-Training</span></a>
</div>