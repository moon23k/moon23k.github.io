---
layout: default
permalink: /balance
---


<div id="detail">
  <div class="title-line"></div>
  <h3 class="title">Transformer Encoder & Decoder Balance:<br>자연어 생성을 위한 Transformer 모델 구조에서의  성능 비교</h3>
  <div class="title-line"></div>

  <div class="tblContents">
    <ul>
      <li>
        <a href="#1">1. Introduction</a>
      </li>

      <li>
        <a href="#2">2. Background</a>
        <ul>
          <li>
            <a href="#2.1">2.1 Natural Language Generation Tasks</a>
          </li>
          <li>
            <a href="#2.2">2.2 Encoder & Decoder</a>
          </li>
          <li>
            <a href="#2.3">2.3 Transformer</a>
          </li>
        </ul>
      </li>
      
      <li>
        <a href="#3">3. Architecure</a>
        <ul>
          <li>
            <a href="#3.1">3.1 Common Encoder</a>
          </li>
          <li>
            <a href="#3.2">3.2 Simple Model</a>
          </li>
          <li>
            <a href="#3.2">3.3 Fused Model</a>
          </li>
          <li>
            <a href="#3.3">3.4 Lever Model</a>
          </li>
        </ul>
      </li>

      <li>
        <a href="#4">4. Experimental Setup</a>
        <ul>
          <li>
            <a href="#4.1">4.1 Data Setups</a>
          </li>
          <li>
            <a href="#4.2">4.2 Model Setups</a>
          </li>
          <li>
            <a href="#4.3">4.3 Training Setups</a>
          </li>
          <li>
            <a href="#4.4">4.4 Model desc</a>
          </li>
        </ul>
      </li>

      <li>
        <a href="#5">5. Results</a>
      </li>

      <li>
        <a href="#6">6. Conclusion</a>
      </li>

      <li>
        <a href="#7">7. Reference</a>
      </li>

    </ul>
  </div>


  
  <div class="post">
    <h2 id="1">1. Introduction</h2>
      <p>
        &nbsp; 자연어 생성은 Encoder와 Decoder 구조가 필수적입니다. 
        하지만 Transformer의 Encoder와 Decoder사이 밸런스를 어떤식으로 조절하느냐에 따른 비교 연구는 부족합니다.
        이를 해결하기 위해, Encoder와 Decoder의 사이의 밸런스를 Depth와 Width관점에서 다르게 하며, 세가지 자연어 생성과제에서
        실제 결과를 통해 비교해봅니다.

        하지만 모든 자연어 생성과제에서 균형잡힌 모델 구조가 필요하지 않을 수도 있습니다.
        직관적으로 생각해봤을때, Translation의 경우, 균형잡힌 모델이 더 좋을 테고
        Dialogue Generation의 경우, Decoder에 무게추가 조금은 기울은 형태가 좋을 것 같고,
        Text Summarization의 경우는, Encoder에 초점을 주는게 좋을 것이라는 가정을 세우고,
        실제 실험 결과가 가정과 같을지 확인해봅니다.
      </p>

    <h2 id="2">2. Background</h2>

      <h3 class="h3-mt" id="2.1">2.1 Natural Language Generation Tasks</h3>
        <p>
          &nbsp; Transformer의 도래 이전, 시퀀스 데이터의 처리에는 RNN 기반의 모델들이 지배적 위치를 점하고 있었습니다. RNN은 순서 정보의 의존성을 잘 포착할 수 있다는 장점이 있지만, 순차적인 연산과정에서 발생하는 병렬화의 어려움과 장거리 데이터간의 의존성 처리의 어려움이라는 단점이 존재했습니다. Transformer는 이러한 한계를 극복하기 위해 self-attention 메커니즘을 통해 모든 입력 토큰 간의 관계를 동시에 계산하는 방법을 선택했으며, 덕분에 병렬 처리가 가능해져 RNN보다 훨씬 빠른 학습과 추론 속도를 제공했습다. 뿐만 아니라 Transformer는 self-attention 메커니즘을 통해 임의 위치 간 의존성을 효과적으로 포착하며 장기 의존성을 보다 잘 처리했습니다. 결과적으로 Transformer는 문장 전체의 의미를 파악하거나, 긴 문맥을 이해하는 등 어려운 환경에서도 우수한 성능을 보이며, 성공적으로 RNN 구조를 대체하게 되었습니다.
        </p>

      <h3 id="2.2" class="h3-mt">2.2 Encoder & Decoder</h3>
        <p>
          &nbsp; Inductive Bias는 기계 학습 모델이 주어진 학습 데이터로부터 일반화하는 데에 어떤 가정이나 제한사항을 가지고 있는 것을 말합니다. 이러한 가정이나 제한사항은 모델이 어떤 가설 공간을 탐색하고 어떤 가설을 선호하는지에 영향을 미칩니다.nductive Bias는 모델이 일반화 능력과 학습 데이터에 대한 의존성을 조절하는 역할을 합니다. 적절한 nductive Bias는 모델이 주어진 문제에 적합하고 일반화하기 쉽도록 도와줍니다.
        </p>
        <p>
          하지만 Transformer의 경우, 복잡한 모델 구조 때문에 Inductive Bias가 부족하기 쉽습니다. 때문에 간단한 과제 해결과정에서 RNN보다 못한 성능을 보이는 경우도 종종 발생합니다. Universal Transformer 논문에서는 Transformer 레이어 연결과정에서 RNN의 핵심개념이었던 재귀적인 연결 방식을 차용함으로써 Inductive Bias의 향상을 도모할 수 있다고 주장합니다.
        </p>

      <h3 id="2.3" class="h3-mt">2.3 Transformer</h3>
        <p>
          &nbsp; Neural Architecture Search (NAS)는 인공 신경망 구조를 자동으로 탐색하는 알고리즘으로, 기계 학습과 딥러닝 모델의 설계 프로세스를 자동화하여 최적의 신경망 구조를 찾는 데 사용됩니다. 기존의 딥러닝 모델링은 엔지니어가 손수 설계하고 최적화하는 일련의 과정을 도맡아 했습니다. 하지만 이러한 과정은 많은 시간과 노력을 필요로 하며, 최적의 구조를 찾는 것이 어려운 경우도 있습니다. NAS는 이러한 문제를 해결하기 위해 컴퓨터가 신경망 구조를 자동으로 발견하고 최적화할 수 있도록 돕습니다.
        </p>
        <p>
          NAS는 주어진 문제에 대한 최적의 신경망 구조를 찾기 위해 다양한 탐색 공간에서 구조를 생성하고 평가합니다. 이를 위해 NAS는 다양한 기법을 사용합니다. 예를 들어, 구조적인 변형이나 파라미터 조정을 통해 새로운 구조를 생성하고, 각 구조를 평가하기 위해 검증 세트나 교차 검증을 사용합니다. 이러한 과정을 반복하여 가장 성능이 우수한 신경망 구조를 찾습니다. 이런 NAS를 활용해 보다 고도화된 Transformer 모델을 구축한 결과물이 바로 Evolved Transformer입니다.
        </p>


    <h2 id="3">3. Architecture</h2>
      <h3 class="h3-mt">3.1 Transfomrer</h3>
        <div class="img-container">
          <img  src="{{ 'assets/img/project_01/standard_transformer.png' | relative_url }}">
          <ul>
            <li>Equal Model
              <ul>
                <li>Encoder</li>
                <li>Decoder</li>
              </ul>
            </li>

            <li>Wide Model
              <ul>
                <li>Sub-Layer 1</li>
                <li>Sub-Layer 2</li>
              </ul>
            </li>

            <li>Deep Model
              <ul>
                <li>Sub-Layer 1</li>
                <li>Sub-Layer 2</li>
                <li>Sub-Layer 3</li>
              </ul>
            </li>

          </ul>
        </div>

    <h2 id="4" class="h3-mt">4. Experimental Setup</h2>
      <h3 id="4.1">4.1 Data Setups</h3>

      <h3 id="4.2" class="h3-mt">4.2 Model Setups</h3>
      
      <h3 id="4.3" class="h3-mt">4.3 Training Setups</h3>
      
      <h3 id="4.4" class="h3-mt">4.4 Model desc</h3>


    <h2 id="5" class="h3-mt">5. Result</h2>


    <h2 id="6" class="h3-mt">6. Conclusion</h2>


    <h2 id="7" class="h3-mt">7. Reference</h2>


  </div>
</div>

<div class="pagination">
  <a href="{{ '/project_04' | relative_url }}" class="btn-prev"></a>
  <a href="{{ '/project_06' | relative_url }}" class="btn-next"></a>
</div>