---
layout: default
permalink: /balance
---


<div id="detail">
  <h1 class="title">Transformer Balance Research on NLG Tasks</h1>  
  <div class="post">

    <div class="callout">
      <p class="highlighted-text"><span>Project Objective</span></p>
      <div class="small-spacer"></div>
      <p>&nbsp; Transformer Seq2Seq 모델은 Encoder와 Decoder로 구성됩니다. 일반적으로는 Encoder의 설정과 Decoder의 설정을 거의 균일하게 맞춥니다. 하지만 특정 태스크의 경우에는 Encoder의 중요성이 더욱 클수도 있고, 다른 경우에는 Deocder의 역할이 더욱 중요할 수 도 있습니다.

      중요한 역할을 강조시키기 위해, 모델의 크기를 키우는것이 도움이 되는데,
      Transformer의 경우에는 Hidden Dimension의 크기를 키우는 것 뿐만아니라, Layer의 개수를 늘리는 방법도 존재합니다.

      전자의 경우에는 은닉층에 보유할 수 있는 정보의 양이 많아지면서, 더 많은 정보를 다룰수 있습니다.
      그리고 후자의 경우에는, 더 많이 레이어를 반복시키면서 더 깊은 이해를 도모할 수 있습니다.

      이 프로젝트에서는 

      </p>
    </div>

    <div class="tblContents">
      <ul>

        <li>
          <a href="#1">1. Balance</a>
          <ul>
            <li>
              <a href="#1.1">1.1 Task Balance</a>
            </li>
            <li>
              <a href="#1.2">1.2 Transformer Balance</a>
            </li>
          </ul>        
        </li>
        
        <li>
          <a href="#2">2. Architecure</a>
          <ul>
            <li>
              <a href="#2.1">2.1 Base Model</a>
            </li>
            <li>
              <a href="#2.2">2.2 Wide Model</a>
            </li>
            <li>
              <a href="#2.3">2.3 Deep Model</a>
            </li>            
          </ul>        

        </li>

        <li>
          <a href="#3">3. Experimental Setup</a>
        </li>

        <li>
          <a href="#4">4. Results</a>
        </li>

        <li>
          <a href="#5">5. Conclusion</a>
        </li>

        <li>
          <a href="#6">6. Reference</a>
        </li>

      </ul>
    </div>
    <hr>


<!-- 2. Architecture -->
    <h2 id="1">1. &hairsp; Balance</h2>

    <h3 id="1.1">1.1 &hairsp; Task Balance</h3>
    <p>
      &nbsp; 해결하고자 하는 자연어 생성 과제 별로 인코딩 및 디코딩의 중요성이 상이합니다. 대표적인 자연어 생성과제 세가지를 예시로 살펴보면 다음과 같습니다.

      1. Machine Translation
      Machine Translation은 특정 언어의 시퀀스를 동일한 의미의 다른 언어 시퀀스로 변환해서 반환하는 과제를 의미합니다.
      상대적으로 인코딩과 디코딩의 중요성 비중이 반반정도라고 볼 수 있습니다.

      2. Dialogue Generation
      Dialogue Generation은 다양한 입력 시퀀스에 따라, 적절한 대답을 반환하는 과제입니다. 
      GPT와 같은 모델들이 대화 생성과제에서 좋은 성능을 내는 것들 역시 디코딩과정의 중요성을 시사합니다.

      3. Text Summarization
      Text Summarization은 긴 시퀀스를 입력값으로 받아, 입력값에서 중요한 내용들을 포함하는 짧은 요약 시퀀스를 출력값으로 반환하는 과제입니다.
      이때 입력값이 출력값에 비해 길기 때문에, 입력값을 제대로 다루는 것이 중요합니다. 이를 위해서는 필연적으로 인코딩의 중요성이 상대적으로 강조됩니다.
    </p>

    <div class="spacer"></div>
    <h3 id="1.2">1.2 &hairsp; Transformer Balance</h3>
    <p>
      &nbsp; 일반적으로 Transformer Seq2Seq 모델을 구성할때 Encoder와 Decoder의 너비나 깊이를 동일하게 구성합니다.
      균형을 깨뜨리는 경우 정보가 왜곡되거나, 전달 과정이 어려워지는 등의 부작용이 발생할 가능성이 크기 때문입니다.

      하지만 특정 요소에 더 집중하고 싶은 경우, 이러한 균형을 깨뜨리는 것도 가능합니다. 대표적인 방법으로는 

      1. Hidden Dimension의 크기를 키운다
        다룰수 있는 정보의 양이 증가하므로, 더 많은 정보를 함유 할 수 있게 됩니다.

      2. Layer의 개수를 증가 시킨다.
        연산이 반복되는 레이어의 개수를 증가시킴으로써, 증가된 레이어 개수만큼 더 깊은 이해가 가능하도록 유도할 수 있습니다.

    </p>


    <div class="spacer"></div>
    <h2 id="2">2. &hairsp; Architecture</h3>

    <h3 id="2.1">2.1 &hairsp; Base Model</h3>
    <div class="code-container">
      <div class="code-snippet">
        <pre><code class="python">
TrainingArguments(
      output_dir= f'ckpt/{strategy}',
      num_train_epochs= 5,
      learning_rate= 1e-5,
      per_device_train_batch_size= 32,
      per_device_eval_batch_size= 32,
      lr_scheduler_type='reduce_lr_on_plateau',
      load_best_model_at_end= True,

      save_strategy= 'epoch',
      logging_strategy= 'epoch',
      evaluation_strategy= 'epoch',

      fp16= True if config.strategy in ['fp16', 'all'] else False,
      fp16_opt_level= '02' if config.strategy in ['fp16', 'all'] else '01',
      gradient_accumulation_steps = True if config.strategy in ['grad_accumulation', 'all'] else 4,
      gradient_checkpointing= True if config.strategy in ['grad_checkpointing', 'all'] else False,
      optim = 'adafactor' if config.strategy in ['optim', 'all'] else 'adamw_torch'
)
        </code></pre>
      </div>
    </div>


    <h3 id="2.2">2.2 &hairsp; Wide Model</h3>
    <div class="code-container">
      <div class="code-snippet">
        <pre><code class="python">
TrainingArguments(
      output_dir= f'ckpt/{strategy}',
      num_train_epochs= 5,
      learning_rate= 1e-5,
      per_device_train_batch_size= 32,
      per_device_eval_batch_size= 32,
      lr_scheduler_type='reduce_lr_on_plateau',
      load_best_model_at_end= True,

      save_strategy= 'epoch',
      logging_strategy= 'epoch',
      evaluation_strategy= 'epoch',

      fp16= True if config.strategy in ['fp16', 'all'] else False,
      fp16_opt_level= '02' if config.strategy in ['fp16', 'all'] else '01',
      gradient_accumulation_steps = True if config.strategy in ['grad_accumulation', 'all'] else 4,
      gradient_checkpointing= True if config.strategy in ['grad_checkpointing', 'all'] else False,
      optim = 'adafactor' if config.strategy in ['optim', 'all'] else 'adamw_torch'
)
        </code></pre>
      </div>
    </div>

    <h3 id="2.3">2.3 &hairsp; Deep Model</h3>
    <div class="code-container">
      <div class="code-snippet">
        <pre><code class="python">
TrainingArguments(
      output_dir= f'ckpt/{strategy}',
      num_train_epochs= 5,
      learning_rate= 1e-5,
      per_device_train_batch_size= 32,
      per_device_eval_batch_size= 32,
      lr_scheduler_type='reduce_lr_on_plateau',
      load_best_model_at_end= True,

      save_strategy= 'epoch',
      logging_strategy= 'epoch',
      evaluation_strategy= 'epoch',

      fp16= True if config.strategy in ['fp16', 'all'] else False,
      fp16_opt_level= '02' if config.strategy in ['fp16', 'all'] else '01',
      gradient_accumulation_steps = True if config.strategy in ['grad_accumulation', 'all'] else 4,
      gradient_checkpointing= True if config.strategy in ['grad_checkpointing', 'all'] else False,
      optim = 'adafactor' if config.strategy in ['optim', 'all'] else 'adamw_torch'
)
        </code></pre>
      </div>
    </div>



    <h2>2. &hairsp; Experimental Setup</h2>

      <div class="small-spacer"></div>
      <h3>Data Setup</h3>
      <div class="small-spacer"></div>
      <p>Sequence Classification을 위한 대표적 데이터 셋 중하나인 AG_News를 사용.
        데이터의 크기는 Train/ Valid/ Test 1000/ 100/ 100으로 설정
        총 네가지 데이터 Label을 모든 데이터에서 고르게 분배해서 한쪽으로 치우치지 않도록 가공
      </p>

      <div class="spacer"></div>
      <h3>Model Setup</h3>
      <div class="small-spacer"></div>
      <p>대표적인 PreTrained Model인 BERT를 사용.
      </p>


      <div class="spacer"></div>
      <h3>Training Setup</h3>
      <div class="small-spacer"></div>

      <div class="code-container">
        <div class="code-snippet">
          <pre><code class="python">
TrainingArguments(
        output_dir= f'ckpt/{strategy}',
        num_train_epochs= 5,
        learning_rate= 1e-5,
        per_device_train_batch_size= 32,
        per_device_eval_batch_size= 32,
        lr_scheduler_type='reduce_lr_on_plateau',
        load_best_model_at_end= True,

        save_strategy= 'epoch',
        logging_strategy= 'epoch',
        evaluation_strategy= 'epoch',

        fp16= True if config.strategy in ['fp16', 'all'] else False,
        fp16_opt_level= '02' if config.strategy in ['fp16', 'all'] else '01',
        gradient_accumulation_steps = True if config.strategy in ['grad_accumulation', 'all'] else 4,
        gradient_checkpointing= True if config.strategy in ['grad_checkpointing', 'all'] else False,
        optim = 'adafactor' if config.strategy in ['optim', 'all'] else 'adamw_torch'
)
          </code></pre>
        </div>
      </div>



    <h2>3. &hairsp; Result</h2>
    <div class="small-spacer"></div>
    <table class="result-table">
      <thead>
        <tr>
          <th>Model Type</th>
          <th>Machine Translation</th>
          <th>Dialogue Generation</th>
          <th>Text Summarization</th>
        </tr>
      </thead>
      <tbody>
        <tr>
          <td>Baseline Model</td>
          <td>174</td>
          <td>7.00</td>
          <td>79</td>
        </tr>

        <tr>
          <td>Encoder Wide Model</td>
          <td>69</td>
          <td>5.47</td>
          <td>78</td>
        </tr>
        <tr>
          <td>Encoder Deep Model</td>
          <td>182</td>
          <td>6.29</td>
          <td>83</td>
        </tr>

        <tr>
          <td>Decoder Wide Model</td>
          <td>239</td>
          <td>3.54</td>
          <td>79</td>
        </tr>

        <tr>
          <td>Decoder Deep Model</td>
          <td>179</td>
          <td>6.72</td>
          <td>79</td>
        </tr>

        <tr>
          <td>Large Model</td>
          <td>85</td>
          <td>2.91</td>
          <td>80</td>
        </tr>

      </tbody>
    </table>


    <h2>4. &hairsp; Conclusion</h2>
    <div class="small-spacer"></div>
    <p> &nbsp; 균형이 잘 맞는 경우가 가장 이상적. 불균형은 자연스레 성능 하락을 야기
    </p>  


  </div>
</div>


<div class="pagination">
  <a href="{{ '/transformer_variants' | relative_url }}" class="btn-prev"><span>Transformer Variants</span></a>
  <a href="{{ '/plm_fusion' | relative_url }}" class="btn-next"><span>PLM Encoder Fusion</span></a>
</div>