---
layout: default
permalink: /balance
---


<div id="detail">
  <h1 class="title">Transformer Encoder & Decoder Balance Analystics</h1>  
  <div class="post">

    <div class="callout">
      <p class="highlighted-text"><span>Project Objective</span></p>
      <div class="small-spacer"></div>
      <p>&nbsp; RNN은 이전 스텝의 결과값을 다음 스텝의 처리과정에 재귀적으로 반영시키며, 시퀀셜 데이터를 처리하기 용이한 네트워크 구조입니다. 대표적인 RNN Cell 구조로는 RNN, LSTM, GRU
      이 프로젝트에서는 세가지 셀 구조별 Seq2Seq 모델 구조에서 성능이 얼마나 나오는지를 비교 분석해봅니다.</p>
    </div>

    <div class="tblContents">
      <ul>

        <li>
          <a href="#1">1. RNN Cells</a>
          <ul>
            <li>
              <a href="#1.1">1.1 RNN</a>
            </li>
            <li>
              <a href="#1.2">1.2 LSTM</a>
            </li>
            <li>
              <a href="#1.3">1.3 GRU</a>
            </li>          
          </ul>
        </li>
        
        <li>
          <a href="#2">2. Architecure</a>
        </li>

        <li>
          <a href="#3">3. Experimental Setup</a>
        </li>

        <li>
          <a href="#4">4. Results</a>
        </li>

        <li>
          <a href="#5">5. Conclusion</a>
        </li>

        <li>
          <a href="#6">6. Reference</a>
        </li>

      </ul>
    </div>
    <hr>



    <h2>1. &hairsp; Project Desc</h2>

      <div class="half-spacer"></div>
      <h3>Transformer Encoder & Decoder</h3>
      <div class="small-spacer"></div>
      <p>
        &nbsp; 현재 다양한 사전학습 딥러닝 모델들이 다양한 분야에서 좋은 성능을 보여주고 있습니다. 
        대부분의 사전학습 모델들은 성능을 극대화 하기 위해, 더 큰 모델 파라미터를 보유하는 경향을 지니고 개발되어 왔습니다.
        모델의 크기가 커지면서 성능이 좋아지는 경향성이 있기는 하지만, 그만큼 
      </p>


      <div class="spacer"></div>
      <h3>Balancing Strategy</h3>

      <div class="img-container">
        <ul>
          <li>Base Model</li>        
            <p> &nbsp; 기울기 누적 방법은 전체 배치를 한 번에 계산하는 대신 작은 증분으로 기울기를 계산하는 방법을 지향합니다. 
              이 방법은 모델을 통해 순전파와 역전파를 수행하면서 작은 배치에서 반복적으로 기울기를 계산하고 이 과정 동안 기울기를 누적하는 방식으로 이루어집니다. 
              충분한 수의 기울기가 누적되면 모델의 최적화 단계가 실행됩니다. 
              기울기 누적을 사용하면 GPU 메모리 용량의 제한을 초과하여 효과적인 배치 크기를 증가시킬 수 있습니다. 
              그러나 기울기 누적에 의해 도입된 추가적인 순전파와 역전파는 훈련 과정을 느리게 만들 수 있다는 점을 주의해야 합니다.
            </p>  

          <div class="half-spacer"></div>
          <li>Deep Model</li>        
            <p> &nbsp; 기울기 누적 방법은 전체 배치를 한 번에 계산하는 대신 작은 증분으로 기울기를 계산하는 방법을 지향합니다. 
              이 방법은 모델을 통해 순전파와 역전파를 수행하면서 작은 배치에서 반복적으로 기울기를 계산하고 이 과정 동안 기울기를 누적하는 방식으로 이루어집니다. 
              충분한 수의 기울기가 누적되면 모델의 최적화 단계가 실행됩니다. 
              기울기 누적을 사용하면 GPU 메모리 용량의 제한을 초과하여 효과적인 배치 크기를 증가시킬 수 있습니다. 
              그러나 기울기 누적에 의해 도입된 추가적인 순전파와 역전파는 훈련 과정을 느리게 만들 수 있다는 점을 주의해야 합니다.
            </p>  

          <div class="half-spacer"></div>
          <li>Wide Model</li>        
            <p> &nbsp; 기울기 누적 방법은 전체 배치를 한 번에 계산하는 대신 작은 증분으로 기울기를 계산하는 방법을 지향합니다. 
              이 방법은 모델을 통해 순전파와 역전파를 수행하면서 작은 배치에서 반복적으로 기울기를 계산하고 이 과정 동안 기울기를 누적하는 방식으로 이루어집니다. 
              충분한 수의 기울기가 누적되면 모델의 최적화 단계가 실행됩니다. 
              기울기 누적을 사용하면 GPU 메모리 용량의 제한을 초과하여 효과적인 배치 크기를 증가시킬 수 있습니다. 
              그러나 기울기 누적에 의해 도입된 추가적인 순전파와 역전파는 훈련 과정을 느리게 만들 수 있다는 점을 주의해야 합니다.
            </p>  

        </ul>
      </div>





    <h2>2. &hairsp; Experimental Setup</h2>

      <div class="small-spacer"></div>
      <h3>Data Setup</h3>
      <div class="small-spacer"></div>
      <p>Sequence Classification을 위한 대표적 데이터 셋 중하나인 AG_News를 사용.
        데이터의 크기는 Train/ Valid/ Test 1000/ 100/ 100으로 설정
        총 네가지 데이터 Label을 모든 데이터에서 고르게 분배해서 한쪽으로 치우치지 않도록 가공
      </p>

      <div class="spacer"></div>
      <h3>Model Setup</h3>
      <div class="small-spacer"></div>
      <p>대표적인 PreTrained Model인 BERT를 사용.
      </p>


      <div class="spacer"></div>
      <h3>Training Setup</h3>
      <div class="small-spacer"></div>

      <div class="code-container">
        <div class="code-snippet">
          <pre><code class="python">
TrainingArguments(
        output_dir= f'ckpt/{strategy}',
        num_train_epochs= 5,
        learning_rate= 1e-5,
        per_device_train_batch_size= 32,
        per_device_eval_batch_size= 32,
        lr_scheduler_type='reduce_lr_on_plateau',
        load_best_model_at_end= True,

        save_strategy= 'epoch',
        logging_strategy= 'epoch',
        evaluation_strategy= 'epoch',

        fp16= True if config.strategy in ['fp16', 'all'] else False,
        fp16_opt_level= '02' if config.strategy in ['fp16', 'all'] else '01',
        gradient_accumulation_steps = True if config.strategy in ['grad_accumulation', 'all'] else 4,
        gradient_checkpointing= True if config.strategy in ['grad_checkpointing', 'all'] else False,
        optim = 'adafactor' if config.strategy in ['optim', 'all'] else 'adamw_torch'
)
          </code></pre>
        </div>
      </div>



    <h2>3. &hairsp; Result</h2>
    <div class="small-spacer"></div>
    <table class="result-table">
      <thead>
        <tr>
          <th>Model Type</th>
          <th>Machine Translation</th>
          <th>Dialogue Generation</th>
          <th>Text Summarization</th>
        </tr>
      </thead>
      <tbody>
        <tr>
          <td>Baseline Model</td>
          <td>174</td>
          <td>7.00</td>
          <td>79</td>
        </tr>

        <tr>
          <td>Encoder Wide Model</td>
          <td>69</td>
          <td>5.47</td>
          <td>78</td>
        </tr>
        <tr>
          <td>Encoder Deep Model</td>
          <td>182</td>
          <td>6.29</td>
          <td>83</td>
        </tr>

        <tr>
          <td>Decoder Wide Model</td>
          <td>239</td>
          <td>3.54</td>
          <td>79</td>
        </tr>

        <tr>
          <td>Decoder Deep Model</td>
          <td>179</td>
          <td>6.72</td>
          <td>79</td>
        </tr>

        <tr>
          <td>Large Model</td>
          <td>85</td>
          <td>2.91</td>
          <td>80</td>
        </tr>

      </tbody>
    </table>


    <h2>4. &hairsp; Conclusion</h2>
    <div class="small-spacer"></div>
    <p> &nbsp; 균형이 잘 맞는 경우가 가장 이상적. 불균형은 자연스레 성능 하락을 야기
    </p>  


  </div>
</div>


<div class="pagination">
  <a href="{{ '/transformer_variants' | relative_url }}" class="btn-prev"><span>Transformer Variants</span></a>
  <a href="{{ '/plm_fusion' | relative_url }}" class="btn-next"><span>PLM Encoder Fusion</span></a>
</div>