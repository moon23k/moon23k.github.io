---
layout: default
permalink: /multiturn_dialogue
---


<div id="detail">
  <h1 class="title">Multi Turn Dialogue Generation</h1>  
  <div class="post">

    <hr>
    <div class="callout">
      <p class="highlighted-text"><span>Project Objective</span></p>
      <div class="small-spacer"></div>
      <p>&nbsp; 이 프로젝트에서는 멀티턴 대화 생성 모델을 위한 효율적 모델 구조를 다각적으로 탐구해봅니다.
      </p>
    </div>

    <div class="tblContents">
      <ul>

        <li>
          <a href="#1">1. &hairsp; Introduction</a>
        </li>

        <li>
          <a href="#2">2. &hairsp; Background</a>
        </li>

        <li>
          <a href="#3">3. &hairsp; Architecure</a>
        </li>

        <li>
          <a href="#4">4. &hairsp; Experimental Setup</a>
        </li>

        <li>
          <a href="#5">5. &hairsp; Result</a>
        </li>

        <li>
          <a href="#6">6. &hairsp; Conclusion</a>
        </li>

        <li>
          <a href="#7">7. &hairsp; Reference</a>
        </li>

      </ul>
    </div>
    <hr>


<!-- 1. Introduction -->    
    <h2 id="1">&hairsp; 1. &hairsp; Introduction</h2>
    <div class="chapter-box">

      <div class="list-container">
        <ul>
          <li>Description</li>
            <p> &nbsp; 자연어 생성을 위한 Sequence to Sequence 모델 구조에서는 Exposure Bias라는 문제점이 빈번하게 발생합니다. 학습과정과 실제 추론과정 사이 괴리에서 기인하는 이 문제를 해결하기 위한 다양한 방법론이 존재하며, 그 방법론 중 하나가 바로 Scheduled Sampling. 하지만 Scheduled Sampling은 학습과정 안에서 생성을 동반하기 때문에 Transformer에 사용하기 적합하지 않습니다. 하지만 이런 문제를 해결하며 새로운 방법론을 제시한 Scheduled Sampling for Transformers라는 논문이 있었습니다. 이 논문에서 아이디어를 얻어, 이 프로젝트에서는 다양한 Sampling Ratio 별 Scheduled Sampling의 효용성을 세가지 자연어 과제에서 검증해봅니다.
            </p>

          <div class="spacer"></div>
          <li>Objective
            <ul>
              <li style="font-weight: normal;">Scheduled Sampling for Transformers 구현</li>
              <li style="font-weight: normal;">Sampling Ratio 별 자연어 생성 능력의 변화 확인</li>
              <li style="font-weight: normal;">Scheduled Sampling의 Exposure Bias 완화 기능 확인</li>
            </ul>
          </li>

        </ul>
      </div>

    </div>


<!-- 2. Background -->    
    <h2 id="2">&hairsp; 2. &hairsp; Background</h2>
    <div class="chapter-box">

      <div class="list-container">
        <ul>
          <li>Multi Turn Dialigue Generation
            <p>&nbsp; Multi Turn Dialigue Generation은 ...
            </p>
          </li>

          <div class="spacer"></div>
          <li>Dialogue History
            <p>&nbsp; Dialogue History는 ...
            </p>
          </li>

        <ul>          
      </div>
    </div>


<!-- 3. Architecture -->    
    <h2 id="3">&hairsp; 3. &hairsp; Architecture</h2>
    <div class="chapter-box">

      <div class="list-container">
        <ul>
          <li>History Concat Model
            <p>&nbsp; History Concat Model ...
            </p>
          </li>


          <div class="spacer"></div>
          <li>History Fusion Model
            <p>&nbsp; ...
            </p>
          </li>

        </ul>          
      </div>
    </div>


<!-- 4. Experimental Setup -->
    <h2 id="4">&hairsp; 4. &hairsp; Experimental Setup</h2>
    <div class="chapter-box">

      <div class="list-container">
        <ul>
          <li>Data Setup
            <div class="dual-container">
              <ul>
                <li>Dataset: &nbsp; Daily Dialogue</li>
                <li>Tokenizer: &nbsp; BPE Tokenizer</li>
                <li>Vocab Size: &nbsp; 15,000</li>
              </ul>
              <ul>
                <li>Train Data Volumn: &nbsp; 50,000</li>
                <li>Valid Data Volumn: &nbsp; 5,000</li>
                <li>Test Data Volumn: &nbsp; 100</li>
                                
              </ul>
            </div>
          </li>

          <div class="half-spacer"></div>
          <li>Model Setup
            <div class="dual-container">
              <ul>
                <li>Input Dim: &nbsp; 15,000</li>
                <li>Output Dim: &nbsp; 15,000</li>
                <li>Embedding Dim: &nbsp; 512</li>
              </ul>
              <ul>
                <li>Hidden Dim: &nbsp; 512</li>
                <li>Model Params: &nbsp; 15,488,664</li>
                <li>Model Size: &nbsp; 60.085 MB</li>                
              </ul>
            </div>
          </li>

          <div class="half-spacer"></div>
          <li>Training Setup
            <div class="dual-container">
              <ul>
                <li>Num Epochs: &nbsp; 10</li>
                <li>Batch Size: &nbsp; 32</li>
                <li>Learning Rate: &nbsp; 5e-4</li>
                <li>LR Scheduler: &nbsp; ReduceLROnPlateau</li>
              </ul>
              <ul>
                <li><b>Hardware</b>: &nbsp; Google Colab A100 GPU</li>
                <li>Optimizer: &nbsp; AdamW</li>
                <li>Apply Gradient Checkpoint: &nbsp; True</li>
                <li>Gradient Accumulation Steps: &nbsp; 4</li>
              </ul>
            </div>
          </li>
        </ul>
      </div>
    </div>


<!-- 5. Result -->
    <h2 id="5">&hairsp; 5. &hairsp; Result</h2>
    <div class="chapter-box">

      <div class="list-container">
        <ul>
          <li>Machine Translation</li>

            <table class="result-table">
              <thead>
                <tr>
                  <th>Sampling Ratio</th>
                  <th>Score</th>
                  <th>Epoch Time</th>
                  <th>Avg GPU</th>
                  <th>Max GPU</th>
                </tr>
              </thead>
              <tbody>
                <tr>
                  <td>0.0</td>
                  <td>12.99</td>
                  <td>0m 44s</td>
                  <td>0.20GB</td>
                  <td>0.95GB</td>
                </tr>
                <tr>
                  <td>0.1</td>
                  <td>2.11</td>
                  <td>1m 00s</td>
                  <td>0.20GB</td>
                  <td>0.95GB</td>
                </tr>
                <tr>
                  <td>0.3</td>
                  <td>7.34</td>
                  <td>1m 00s</td>
                  <td>0.20GB</td>
                  <td>0.95GB</td>
                </tr>
                <tr>
                  <td>0.5</td>
                  <td>6.94</td>
                  <td>1m 00s</td>
                  <td>0.20GB</td>
                  <td>0.95GB</td>
                </tr>

              </tbody>
            </table>
          
          <div class="half-spacer"></div>
          <li>Dialogue Generation</li>
            <table class="result-table">
              <thead>
                <tr>
                  <th>Sampling Ratio</th>
                  <th>Score</th>
                  <th>Epoch Time</th>
                  <th>Avg GPU</th>
                  <th>Max GPU</th>
                </tr>
              </thead>
              <tbody>
                <tr>
                  <td>0.0</td>
                  <td>2.03</td>
                  <td>0m 43s</td>
                  <td>0.20GB</td>
                  <td>0.85GB</td>
                </tr>
                <tr>
                  <td>0.1</td>
                  <td>2.31</td>
                  <td>0m 59s</td>
                  <td>0.20GB</td>
                  <td>0.85GB</td>
                </tr>
                <tr>
                  <td>0.3</td>
                  <td>2.34</td>
                  <td>0m 59s</td>
                  <td>0.20GB</td>
                  <td>0.85GB</td>
                </tr>
                <tr>
                  <td>0.5</td>
                  <td>0.82</td>
                  <td>0m 59s</td>
                  <td>0.20GB</td>
                  <td>0.85GB</td>
                </tr>

              </tbody>
            </table>


          <div class="half-spacer"></div>
          <li>Text Summarization</li>
            <table class="result-table">
              <thead>
                <tr>
                  <th>Sampling Ratio</th>
                  <th>Score</th>
                  <th>Epoch Time</th>
                  <th>Avg GPU</th>
                  <th>Max GPU</th>
                </tr>
              </thead>
              <tbody>
                <tr>
                  <td>0.0</td>
                  <td>7.60</td>
                  <td>2m 44s</td>
                  <td>0.21GB</td>
                  <td>2.78GB</td>
                </tr>
                <tr>
                  <td>0.1</td>
                  <td>2.95</td>
                  <td>3m 8s</td>
                  <td>0.21GB</td>
                  <td>2.78GB</td>
                </tr>
                <tr>
                  <td>0.3</td>
                  <td>4.10</td>
                  <td>3m 8s</td>
                  <td>0.21GB</td>
                  <td>2.78GB</td>
                </tr>
                <tr>
                  <td>0.5</td>
                  <td>5.39</td>
                  <td>3m 8s</td>
                  <td>0.21GB</td>
                  <td>2.78GB</td>
                </tr>

              </tbody>
            </table>


          <div class="half-spacer"></div>  
          <li>Result Analysis</li>
            <p> &nbsp; Scheduled Sampling이 대화 생성에서는 긍정적인 영향을 끼치긴 했지만, 미미한 수준에 그칠 뿐이며, 나머지 과제에서는 모두 성능과 효율성 모두에 악영향을 미쳤습니다. 이를 통해 Scheduled Sampling이 Transformer에서는 맞지 않는 학습 방식임을 알 수 있습니다. 
            </p>
        </ul>
      </div>
    </div>


<!-- 6. Conclusion -->
    <h2 id="6">&hairsp; 6. &hairsp; Conclusion</h2>
    <div class="chapter-box">

      <p>&nbsp; 이 프로젝트에서는 Scheduled Sampling for Transformers 논문에서 소개한 내용을 토대로, 코드를 구성하고, 세가지 자연어 생성 과제에서 그 성능을 검증해봤습니다.
      결과적으로는 Scheduled Sampling for Transformers 방법론이 Transformer에는 맞지 않는 학습 방식이라는 것을 확인했습니다. Exposure Bias라는 문제점을 완화하기 위한 다양한 방법론 중 하나를 직접 검증해보며, 추후 Exposure Bias 완화를 위한 연구에 참고점으로 삼을 수 있을 것 같습니다.
      </p>

    </div>


<!-- 7. Reference -->
    <h2 id="7">&hairsp; 7. &hairsp; Reference</h2>
    <div class="chapter-box">
      <a class="reference" href="https://arxiv.org/abs/1706.03762">&nbsp; Attention Is All You Need</a> <br> 
      <a class="reference" href="https://arxiv.org/abs/1906.07651">&nbsp; Scheduled Sampling for Transformers</a> 

    </div>

  </div>
</div>


<div class="pagination">
  <a href="{{ '/multilingual_translation' | relative_url }}" class="btn-prev"><span>Multi-Lingual Translation</span></a>
  <a href="{{ '/efficient_summarization' | relative_url }}" class="btn-next"><span>Efficient Summarization</span></a>
</div>