---
layout: default
permalink: /peft
---


<div id="detail">
  <h1 class="title">Parameter Efficient Fine Tuning Analystics</h1>  
  <div class="post">

    <hr>
    <div class="callout">
      <p class="highlighted-text"><span>Project Objective</span></p>
      <div class="small-spacer"></div>
      <p>&nbsp; 대규모 사전학습 언어 모델(Large Language Models, LLM)들이 다양한 분야에서 우수한 성능을 보이고 있습니다. 
        일반적으로 LLM은 사용자가 원하는 Down Stream Task에 맞게 Fine Tuning하는 방식을 채택하지만, 대규모 모델의 파라미터를 전부 업데이트 하는 것은 비용과 시간이 많이 소요되는 일입니다. 
        이러한 도전에 대응하여, 파라미터의 효율적인 업데이트에 중점을 둔 새로운 방법론이 등장했는데, 그것이 바로 PEFT(Parameter Efficient Fine Tuning)입니다. 
        PEFT는 모델을 고도로 튜닝하면서도 파라미터 업데이트를 효율적으로 수행함으로써 리소스를 절약하는 방법을 제공합니다.
        
        이 프로젝트에서는 PEFT를 적용한 총 5가지 방식을 직접 실험해보고, 메모리와 시간 측면에서의 효율성과 더불어 실제 NMT(Neural Machine Translation) 작업에서의 성능을 비교 분석하려고 합니다. 각각의 PEFT 방식을 구체적으로 살펴보면서 어떻게 모델의 성능 향상과 효율적인 파라미터 업데이트가 이루어지는지 확인해 보겠습니다.
      </p>
    </div>

    <div class="tblContents">
      <ul>

        <li>
          <a href="#1">1. &hairsp; PEFT Strategy</a>
          <ul>
            <li>
              <a href="#1.1">1.1 &hairsp; Prompt Tuning</a>
            </li>
            <li>
              <a href="#1.2">1.2 &hairsp; Prefix Tuning</a>
            </li>
            <li>
              <a href="#1.3">1.3 &hairsp; P Tuning</a>
            </li>            
            <li>
              <a href="#1.4">1.4 &hairsp; LoRA</a>
            </li>            
            <li>
              <a href="#1.5">1.5 &hairsp; IA3</a>
            </li>            
          </ul>
        </li>        

        <li>
          <a href="#2">2. &hairsp; Experimental Setup</a>
        </li>

        <li>
          <a href="#3">3. &hairsp; Results</a>
        </li>

        <li>
          <a href="#4">4. &hairsp; Conclusion</a>
        </li>

        <li>
          <a href="#5">5. &hairsp; Reference</a>
        </li>

      </ul>
    </div>
    <hr>    


    <h2 id="1">1. &hairsp; PEFT Strategy</h2>
    <h3 id="1.1">1.1 &hairsp; Prompt Tuning</h3>

    <div style="text-align: center;">
      <img src="{{ 'assets/img/research_03/prompt_tuning.png' | relative_url }}" style="width:530px; display: block; margin: 0 auto;">
    </div>
    <div class="small-spacer"></div>
    <p>
      &nbsp; 
      prompt라는 단어에서 드러나듯이, 해결하고자 하는 과제를 잘 표현하는 특정 프롬프트를 모델에 추가하는 방식을 사용합니다. 

      Prompting helps guide language model behavior by adding some input text specific to a task. Prompt tuning is an additive method for only training and updating the newly added prompt tokens to a pretrained model. This way, you can use one pretrained model whose weights are frozen, and train and update a smaller set of prompt parameters for each downstream task instead of fully finetuning a separate model. As models grow larger and larger, prompt tuning can be more efficient, and results are even better as model parameters scale.        
    </p>
    <hr>


    <h3 id="1.2">1.2 &hairsp; Prefix Tuning</h3>
    <div style="text-align: center;">
      <img src="{{ 'assets/img/research_03/prefix_tuning.png' | relative_url }}" style="width:430px; height: 70%; display: block; margin: 0 auto;">
    </div>

    <div class="small-spacer"></div>
    <p>&nbsp; 
      Prefix tuning is an additive method where only a sequence of continuous task-specific vectors is attached to the beginning of the input, or prefix. Only the prefix parameters are optimized and added to the hidden states in every layer of the model. The tokens of the input sequence can still attend to the prefix as virtual tokens. As a result, prefix tuning stores 1000x fewer parameters than a fully finetuned model, which means you can use one large language model for many tasks.
    </p>


      <div class="spacer"></div>
      <h3>P Tuning</h3>
      <div style="text-align: center;">
        <img src="{{ 'assets/img/research_03/p_tuning.png' | relative_url }}" style="width:80%; height:180px; display: block; margin: 0 auto;">
      </div>

      <div class="small-spacer"></div>
      <p>&nbsp; 
        It is challenging to finetune large language models for downstream tasks because they have so many parameters. To work around this, you can use prompts to steer the model toward a particular downstream task without fully finetuning a model. Typically, these prompts are handcrafted, which may be impractical because you need very large validation sets to find the best prompts. P-tuning is a method for automatically searching and optimizing for better prompts in a continuous space.
      </p>
      <hr>

      <div class="spacer"></div>
      <h3>LoRA</h3>
      <div class="small-spacer"></div>
      <p>&nbsp; 
        Low-Rank Adaptation (LoRA) is a reparametrization method that aims to reduce the number of trainable parameters with low-rank representations. The weight matrix is broken down into low-rank matrices that are trained and updated. All the pretrained model parameters remain frozen. After training, the low-rank matrices are added back to the original weights. This makes it more efficient to store and train a LoRA model because there are significantly fewer parameters.
      </p>
      <hr>

      <div class="spacer"></div>
      <h3>IA3</h3>
      <div class="small-spacer"></div>
      <p>prefix tuning
      </p>
      <hr>


    <h2>2. &hairsp; Experimental Setup</h2>

      <div class="small-spacer"></div>
      <h3>Data Setup</h3>
      <div class="small-spacer"></div>
      <p>Sequence Classification을 위한 대표적 데이터 셋 중하나인 AG_News를 사용.
        데이터의 크기는 Train/ Valid/ Test 1000/ 100/ 100으로 설정
        총 네가지 데이터 Label을 모든 데이터에서 고르게 분배해서 한쪽으로 치우치지 않도록 가공
      </p>

      <div class="spacer"></div>
      <h3>Model Setup</h3>
      <div class="small-spacer"></div>
      <p>대표적인 PreTrained Model인 BERT를 사용.
      </p>


      <div class="spacer"></div>
      <h3>Training Setup</h3>
      <div class="small-spacer"></div>

      <div class="code-container">
        <div class="code-snippet">
          <pre><code class="python">
TrainingArguments(
        output_dir= f'ckpt/{strategy}',
        num_train_epochs= 5,
        learning_rate= 1e-5,
        per_device_train_batch_size= 32,
        per_device_eval_batch_size= 32,
        lr_scheduler_type='reduce_lr_on_plateau',
        load_best_model_at_end= True,

        save_strategy= 'epoch',
        logging_strategy= 'epoch',
        evaluation_strategy= 'epoch',

        fp16= True if config.strategy in ['fp16', 'all'] else False,
        fp16_opt_level= '02' if config.strategy in ['fp16', 'all'] else '01',
        gradient_accumulation_steps = True if config.strategy in ['grad_accumulation', 'all'] else 4,
        gradient_checkpointing= True if config.strategy in ['grad_checkpointing', 'all'] else False,
        optim = 'adafactor' if config.strategy in ['optim', 'all'] else 'adamw_torch'
)
          </code></pre>
        </div>
      </div>



    <h2>3. &hairsp; Result</h2>
    <div class="small-spacer"></div>
    <table class="result-table">
      
      <thead>
        <tr>
          <th>PEFT Strategy</th>
          <th>Train Time</th>
          <th>Accuracy</th>
          <th>Trainable Param Ratio</th>
          <th>Avg GPU Memory</th>
          <th>Max GPU Memory</th>
          
        </tr>
      </thead>

      <tbody>
        <tr>
          <td>Vanilla Fine Tuning</td>
          <td>36 sec</td>
          <td>0.81</td>
          <td>100 %</td>
          <td>0.43 GB</td>
          <td>1.18 GB</td>
        </tr>

        <tr>
          <td>Prompt Tuning</td>
          <td>23.7 sec</td>
          <td>0.25</td>
          <td>0.0098 %</td>
          <td>0.43 GB</td>
          <td>0.68 GB</td>
        </tr>

        <tr>
          <td>Prefix Tuning</td>
          <td>70 sec</td>
          <td>0.20</td>
          <td>0.17 %</td>
          <td>0.43 GB</td>
          <td>0.67 GB</td>
        </tr>

        <tr>
          <td>P Tuning</td>
          <td>68.1 sec</td>
          <td>0.25</td>
          <td>0.20 %</td>
          <td>0.43 GB</td>
          <td>0.69 GB</td>
        </tr>

        <tr>
          <td>LoRA</td>
          <td>57.3 sec</td>
          <td>0.24</td>
          <td>0.54 %</td>
          <td>0.43 GB</td>
          <td>0.50 GB</td>
        </tr>

        <tr>
          <td>IA3</td>
          <td>61.2 sec</td>
          <td>0.24</td>
          <td>0.0035 %</td>
          <td>0.42 GB</td>
          <td>0.50 GB</td>
        </tr>

      </tbody>
    </table>

    <div class="small-spacer"></div>
    <table class="result-table">
      
      <thead>
        <tr>
          <th>PEFT Strategy</th>
          <th>Train Time</th>
          <th>Accuracy</th>
          <th>Trainable Param Ratio</th>
          <th>Avg GPU Memory</th>
          <th>Max GPU Memory</th>
          
        </tr>
      </thead>

      <tbody>
        <tr>
          <td>Vanilla Fine Tuning</td>
          <td>36 sec</td>
          <td>0.81</td>
          <td>100 %</td>
          <td>0.43 GB</td>
          <td>1.18 GB</td>
        </tr>

        <tr>
          <td>Prompt Tuning</td>
          <td>23.7 sec</td>
          <td>0.25</td>
          <td>0.0098 %</td>
          <td>0.43 GB</td>
          <td>0.68 GB</td>
        </tr>

        <tr>
          <td>Prefix Tuning</td>
          <td>70 sec</td>
          <td>0.20</td>
          <td>0.17 %</td>
          <td>0.43 GB</td>
          <td>0.67 GB</td>
        </tr>

        <tr>
          <td>P Tuning</td>
          <td>68.1 sec</td>
          <td>0.25</td>
          <td>0.20 %</td>
          <td>0.43 GB</td>
          <td>0.69 GB</td>
        </tr>

        <tr>
          <td>LoRA</td>
          <td>57.3 sec</td>
          <td>0.24</td>
          <td>0.54 %</td>
          <td>0.43 GB</td>
          <td>0.50 GB</td>
        </tr>

        <tr>
          <td>IA3</td>
          <td>61.2 sec</td>
          <td>0.24</td>
          <td>0.0035 %</td>
          <td>0.42 GB</td>
          <td>0.50 GB</td>
        </tr>

      </tbody>
    </table>
    <hr>

    <h2>4. &hairsp; Conclusion</h2>
    <div class="small-spacer"></div>
    <p> &nbsp; 균형이 잘 맞는 경우가 가장 이상적. 불균형은 자연스레 성능 하락을 야기
    </p>  
    <hr>


    <h2 id="5">5. &hairsp; Reference</h2>
    <div class="small-spacer"></div>
    <a class="reference" href="https://huggingface.co/docs/peft/index">&nbsp; Huggingface PEFT</a> <br>
    <hr>


  </div>
</div>




<div class="pagination">
  <a href="{{ '/eff_model' | relative_url }}" class="btn-prev"></a>
  <a href="{{ '/back_translation' | relative_url }}" class="btn-next"></a>
</div>