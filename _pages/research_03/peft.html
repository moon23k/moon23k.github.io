---
layout: default
permalink: /peft
---


<div id="detail">
  <h1 class="title">Parameter Efficient Fine Tuning Analystics</h1>  
  <div class="post">

    <hr>
    <div class="callout">
      <p class="highlighted-text"><span>Project Objective</span></p>
      <div class="small-spacer"></div>
      <p>&nbsp; 이 프로젝트에서는 대규모 모델을 효율적으로 원하는 과제에서 미세조정 하기 위한 방법론인 PEFT(Parameter Efficient Fine Tuning)에 대해 자세히 탐구합니다.
        총 5개의 PEFT 방식(Prompt Tuning, Prefix Tuning, P tuning, LoRA, IA3)을 비교 분석하며, PEFT 방식을 제외한 모든 실험 변수는 동일하게 적용했습니다. 모델은 사전 학습 된 BERT 모델을 사용했으며, 학습 데이터로는 AG News를 선정했습니다. 


      </p>
    </div>

    <div class="tblContents">
      <ul>

        <li>
          <a href="#1">1. &hairsp; Introduction</a>
        </li>

        <li>
          <a href="#2">2. &hairsp; PEFT Strategy</a>
        </li>

        <li>
          <a href="#3">3. &hairsp; Experimental Setup</a>
        </li>

        <li>
          <a href="#4">4. &hairsp; Results</a>
        </li>

        <li>
          <a href="#5">5. &hairsp; Conclusion</a>
        </li>

        <li>
          <a href="#6">6. &hairsp; Reference</a>
        </li>

      </ul>
    </div>
    <hr>    



<!-- 1. Introduction -->    
    <h2 id="1">&hairsp; 1. &hairsp; Introduction</h2>
    <div class="chapter-box">

      <div class="list-container">
        <ul>
          <li>Description</li>
            <p> &nbsp; 대규모 사전학습 언어 모델(Large Language Models, LLM)들이 다양한 분야에서 우수한 성능을 보이고 있습니다. 
            일반적으로 LLM은 사용자가 원하는 Down Stream Task에 맞게 Fine Tuning하는 방식을 채택하지만, 대규모 모델의 파라미터를 전부 업데이트 하는 것은 비용과 시간이 많이 소요되는 일입니다. 
            이러한 도전에 대응하여, 파라미터의 효율적인 업데이트에 중점을 둔 새로운 방법론이 등장했는데, 그것이 바로 PEFT(Parameter Efficient Fine Tuning)입니다. 
            PEFT는 모델을 고도로 튜닝하면서도 파라미터 업데이트를 효율적으로 수행함으로써 리소스를 절약하는 방법을 제공합니다.
        
            이 프로젝트에서는 PEFT를 적용한 총 5가지 방식을 직접 실험해보고, 메모리와 시간 측면에서의 효율성과 더불어 AG News 분류 문제에서의 성능을 비교 분석하려고 합니다. 
            각각의 PEFT 방식을 구체적으로 살펴보면서 어떻게 모델의 성능 향상과 효율적인 파라미터 업데이트가 이루어지는지 확인해 보겠습니다.
            </p>

          <div class="spacer"></div>
          <li>Objective
            <ul>
              <li style="font-weight: normal;">다양한 PEFT 방법론 별 성능 및 효율성 측정</li>
              <li style="font-weight: normal;">효율성 증진과 성능 하락 크로스 오버 정도 확인</li>
            </ul>
          </li>

        </ul>
      </div>

    </div>



    <h2 id="2">&hairsp; 2. &hairsp; PEFT Strategy</h2>
    <div class="chapter-box">
      <div class="list-container">
        <ul>
          <li>Prompt Tuning</li>
          <div style="text-align: center;">
            <img src="{{ 'assets/img/research_03/prompt_tuning.png' | relative_url }}" style="width:530px; display: block; margin: 0 auto;">
          </div>
          <div class="small-spacer"></div>
          <p>&nbsp; 프롬프팅은 작업에 특정한 입력 텍스트를 추가하여 언어 모델의 동작을 안내하는 데 도움이 됩니다. 프롬프트 튜닝은 사전 훈련된 모델에 새롭게 추가된 프롬프트 토큰만을 훈련하고 업데이트하는 추가적인 방법입니다. 이렇게 하면 가중치가 고정된 하나의 사전 훈련된 모델을 사용하고, 각 하위 작업에 대해 완전히 새로운 모델을 미세 조정하는 대신 새롭게 추가된 프롬프트 매개변수의 작은 세트를 훈련하고 업데이트할 수 있습니다. 모델이 점점 커질수록 프롬프트 튜닝은 더 효율적일 수 있으며, 모델 매개변수가 확장됨에 따라 결과도 더욱 향상될 수 있습니다.    
          </p>


          <div class="spacer"></div>
          <li>Prefix Tuning</li>
          <div style="text-align: center;">
            <img src="{{ 'assets/img/research_03/prefix_tuning.png' | relative_url }}" style="width:430px; height: 70%; display: block; margin: 0 auto;">
          </div>
          <p>&nbsp; Prefix Tuning은 입력의 시작 부분 또는 접두사에 연속적인 작업 특정 벡터 시퀀스만 추가되는 추가적인 방법입니다. 오직 접두사 매개변수만 최적화되며 모델의 각 레이어에서 숨겨진 상태에 추가됩니다. 입력 시퀀스의 토큰은 여전히 가상 토큰으로서 접두사에 주의를 기울일 수 있습니다. 결과적으로 프리픽스 튜닝은 완전히 미세 조정된 모델보다 매우 적은 매개변수를 저장하므로 하나의 큰 언어 모델을 여러 작업에 사용할 수 있습니다.
          </p>


          <div class="spacer"></div>
          <li>P Tuning</li>
          <div style="text-align: center;">
            <img src="{{ 'assets/img/research_03/p_tuning.png' | relative_url }}" style="width:80%; height:180px; display: block; margin: 0 auto;">
          </div>

          <p>&nbsp; 앞선 두 방식에서 모델을 완전히 미세 조정하지 않고도 특정 하위 작업으로 모델을 유도하기 위해 프롬프트를 사용했습니다. 일반적으로 이러한 프롬프트는 수동으로 만들어지는데, 이는 가장 좋은 프롬프트를 찾기 위해 매우 큰 검증 세트가 필요하여 비현실적일 수 있습니다. P-튜닝은 연속 공간에서 더 나은 프롬프트를 자동으로 탐색하고 최적화하기 위한 방법입니다.
          </p>

          <div class="spacer"></div>
          <li>LoRA
            <p>&nbsp; LoRA는 미세 조정을 더 효율적으로 만들기 위해 낮은 순위 분해를 통해 두 개의 작은 행렬(업데이트 행렬)로 가중치 업데이트를 표현하는 것입니다. 이러한 새로운 행렬은 전반적인 변경 수를 낮추면서 새로운 데이터에 적응할 수 있도록 훈련될 수 있습니다. 원래의 가중치 행렬은 고정되어 있으며 추가적인 조정을 받지 않습니다. 최종 결과를 생성하기 위해 원래 가중치와 적응된 가중치를 결합합니다. LoRA는 다음과 같은 이점을 지닙니다.
            </p>

            <div class="small-spacer"></div>
            <ul>
              <li>LoRA는 훈련 가능한 매개변수의 수를 급격히 줄여 미세 조정을 더 효율적으로 만듭니다.</li>
              <li>원래의 사전 훈련된 가중치는 고정되어 있어 여러 가벼운 이식 가능한 LoRA 모델을 구축하여 다양한 하위 작업에 사용할 수 있습니다.</li>
              <li>LoRA는 많은 다른 매개변수 효율적인 방법들과 결합되어 활용될 수 있습니다.</li>
              <li>LoRA를 사용하여 미세 조정된 모델의 성능은 완전히 미세 조정된 모델의 성능과 비교할만한 성능을 보입니다.</li>
              <li>LoRA는 어댑터 가중치를 기본 모델과 병합할 수 있으므로 어떠한 추론 지연도 추가하지 않습니다.</li>
            </ul>

          </li>

          <div class="spacer"></div>
          <li>IA3
            <p> &nbsp; 미세 조정을 더 효율적으로 만들기 위해 IA3 (Infused Adapter by Inhibiting and Amplifying Inner Activations)는 학습된 벡터로 내부 활성화를 다시 조절합니다. 이러한 학습된 벡터는 전형적인 트랜스포머 기반 아키텍처의 어텐션 및 피드포워드 모듈에 주입됩니다. 미세 조정 중에는 이러한 학습된 벡터만이 훈련 가능한 매개변수이며, 따라서 원래의 가중치는 고정된 상태를 유지합니다. (LoRA와 같이 가중치 행렬에 대한 저차원 업데이트가 아닌 학습된 벡터를 다루기 때문에) 학습된 벡터를 처리함으로써 훈련 가능한 매개변수의 수를 훨씬 작게 유지할 수 있습니다. LoRA와 유사하게, IA3 역시 많은 이점을 가지고 있습니다.
            </p>

            <div class="small-spacer"></div>
            <ul>
              <li>IA3는 훈련 가능한 매개변수의 수를 급격히 줄여 미세 조정을 더 효율적으로 만듭니다. IA3 모델은 약 0.01%의 훈련 가능한 매개변수만 가지고 있으며, LoRA조차 0.1% 이상임에 비해 매우 적습니다.</li>
              <li>원래 사전 훈련된 가중치는 고정되어 있어 여러 가벼운 휴대용 IA3 모델을 만들어 다양한 하위 작업에 사용할 수 있습니다.</li>
              <li>IA3를 사용하여 미세 조정된 모델의 성능은 완전히 미세 조정된 모델의 성능과 비교 가능한 성능을 보입니다.</li>
              <li>IA3는 어댑터 가중치를 기본 모델과 병합할 수 있으므로 어떠한 추론 지연도 추가하지 않습니다.</li>
            </ul>
          </li>
        
        <ul>
      </div>
    </div>



    <h2 id="3">&hairsp; 3. &hairsp; Experimental Setup</h2>
    <div class="chapter-box">

      <div class="list-container">
        <ul>
          <li>Data Setup
            <div class="dual-container">
              <ul>
                <li><b>Dataset</b>: &nbsp; AG News</li>
                <li><b>Tokenizer</b>: &nbsp; BERT Tokenizer</li>
                <li><b>Vocab Size</b>: &nbsp; 15,000</li>
              </ul>
              <ul>
                <li><b>Train Data Volumn</b>: &nbsp; 1,000</li>
                <li><b>Valid Data Volumn</b>: &nbsp; 100</li>
                <li><b>Test Data Volumn</b>: &nbsp; 100</li>                
              </ul>
            </div>
          </li>

          <div class="spacer"></div>
          <li>Model Setup
            <div class="dual-container">
              <ul>
                <li><b>Input Dim</b>: &nbsp; 15,000</li>
                <li><b>Output Dim</b>: &nbsp; 15,000</li>
                <li><b>Hidden Dim</b>: &nbsp; 256</li>
                <li><b>PFF Dim</b>: &nbsp; 512</li>
              </ul>
              <ul>
                <li><b>N Layers</b>: &nbsp; 3</li>
                <li><b>N Heads</b>: &nbsp; 8</li>
                <li><b>Model Params</b>: &nbsp; 000</li>
                <li><b>Model Size</b>: &nbsp; 000</li>                
              </ul>
            </div>
          </li>
    
          <div class="spacer"></div>
          <li>Training Setup</li>
          <div class="code-container">
            <div class="code-snippet">
              <pre><code class="python">TrainingArguments(
    output_dir= f'ckpt/{strategy}',
    num_train_epochs= 5,
    learning_rate= 1e-5,
    per_device_train_batch_size= 32,
    per_device_eval_batch_size= 32,
    lr_scheduler_type='reduce_lr_on_plateau',
    load_best_model_at_end= True,

    save_strategy= 'epoch',
    logging_strategy= 'epoch',
    evaluation_strategy= 'epoch',

    fp16= True if config.strategy in ['fp16', 'all'] else False,
    fp16_opt_level= '02' if config.strategy in ['fp16', 'all'] else '01',
    gradient_accumulation_steps = True if config.strategy in ['grad_accumulation', 'all'] else 4,
    gradient_checkpointing= True if config.strategy in ['grad_checkpointing', 'all'] else False,
    optim = 'adafactor' if config.strategy in ['optim', 'all'] else 'adamw_torch'
)</code></pre>
            </div>
          </div>


        </ul>
      </div>
    </div>




    <h2 id="4">&hairsp; 4. &hairsp; Result</h2>
    <div class="chapter-box">
      <div class="list-container">
        <ul>
          <li>AG News</li>
          <table class="result-table">
            
            <thead>
              <tr>
                <th>PEFT Strategy</th>
                <th>Epoch Time</th>
                <th>Accuracy</th>
                <th>Param Ratio</th>
                <th>Avg GPU</th>
                <th>Max GPU</th>
                
              </tr>
            </thead>

            <tbody>
              <tr>
                <td>Vanilla Fine Tuning</td>
                <td>0m 36s</td>
                <td>81</td>
                <td>100 %</td>
                <td>0.43 GB</td>
                <td>1.18 GB</td>
              </tr>

              <tr>
                <td>Prompt Tuning</td>
                <td>0m 23s</td>
                <td>25</td>
                <td>0.0098 %</td>
                <td>0.43 GB</td>
                <td>0.68 GB</td>
              </tr>

              <tr>
                <td>Prefix Tuning</td>
                <td>1m 10s</td>
                <td>20</td>
                <td>0.17 %</td>
                <td>0.43 GB</td>
                <td>0.67 GB</td>
              </tr>

              <tr>
                <td>P Tuning</td>
                <td>1m 8s</td>
                <td>25</td>
                <td>0.20 %</td>
                <td>0.43 GB</td>
                <td>0.69 GB</td>
              </tr>

              <tr>
                <td>LoRA</td>
                <td>0m 57s</td>
                <td>24</td>
                <td>0.54 %</td>
                <td>0.43 GB</td>
                <td>0.50 GB</td>
              </tr>

              <tr>
                <td>IA3</td>
                <td>1m 1s</td>
                <td>24</td>
                <td>0.0035 %</td>
                <td>0.42 GB</td>
                <td>0.50 GB</td>
              </tr>

            </tbody>
          </table>


          <div class="spacer"></div>
          <li>Result Analysis</li>
          <p>&nbsp; 
          </p>
        </ul>
      </div>
    </div>
    

    <h2 id="5">&hairsp; 5. &hairsp; Conclusion</h2>
    <div class="chapter-box">

      <div class="list-container">
        <ul>
          <li>직접 코드 구현을 통한 Transformer 모델에 대한 깊은 이해</li>
            <p>모델 구현을 통해 논문에서 글로 서술된 Transformer를 코드적으로 이해하게 되었습니다. 
                특히 모델의 구현을 위한 다양한 코드 구현을 했지만, 간단한 부분에서의 다른 구현이 성능의 하락을 야기하기도 했습니다.
                가장 대표적으로는 코드적으로 깔끔해 보이기는 했지만, 안좋은 성능을 보였던 아래와 같은 경우도 존재했습니다.
                결과적으로 가장 직관적으로 구현한 코드가 좋은 성능을 보였습니다. <br>
                추가적으로 논문에서 소개한 Transformer Base 모델과 동일한 하이퍼 파라미터를 적용하면, 데이터량 대비 과도하게 큰 모델이 되어 학습이 정상적으로 진행되지 않음을 실험적으로 확인.
                학습 요건에 맞는 모델링 구성이 중요함을 알게됨.
            </p>
        

          <div class="spacer"></div>
          <li>Transformer의 자연어 생성 능력 측정</li>
          <p>&nbsp; 5만개라는 소규모 데이터로 학습했음에도 불구하고, Transformer는 기계번역, 대화생성, 문서요약이라는 어려운 과제에서 000이라는 성능을 기록했습니다.
            Transformer 이전 가장 많이 활용되던 RNN Sequence to Sequence with Attention 모델과 비교하면 00%의 성능 개선, 00%의 효율성 개선이라는 뛰어난 기술적 진보를 만들어냈습니다.
          </p>

        </ul>
      </div>
    </div>


    <h2 id="6">&hairsp; 6. &hairsp; Reference</h2>
    <div class="chapter-box">
      <a class="reference" href="https://huggingface.co/docs/peft/index">&nbsp; Huggingface PEFT</a> <br>
    </div>

  </div>
</div>




<div class="pagination">
  <a href="{{ '/eff_model' | relative_url }}" class="btn-prev"></a>
  <a href="{{ '/back_translation' | relative_url }}" class="btn-next"></a>
</div>