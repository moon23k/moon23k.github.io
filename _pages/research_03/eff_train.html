---
layout: default
permalink: /eff_train
---


<div id="detail">
  <h1 class="title">Effective Training Methodology Analytics</h1>  
  <div class="post">
    <h2>1. &hairsp; Project Desc</h2>

      <div class="small-spacer"></div>
      <h3>Deep Learning Model Training</h3>
      <div class="small-spacer"></div>
      <p>
        &nbsp; 현재 다양한 사전학습 딥러닝 모델들이 다양한 분야에서 좋은 성능을 보여주고 있습니다. 
        대부분의 사전학습 모델들은 성능을 극대화 하기 위해, 더 큰 모델 파라미터를 보유하는 경향을 지니고 개발되어 왔습니다.
        모델의 크기가 커지면서 성능이 좋아지는 경향성이 있기는 하지만, 그만큼 
      </p>

      <div class="spacer"></div>
      <h3>Mixed Precision Training</h3>
      <div class="small-spacer"></div>
      <p> &nbsp; 혼합 정밀도 훈련(Mixed Precision Training)은 모델 훈련의 계산 효율성을 최적화하기 위한 기술로, 특정 변수에 대해 낮은 정밀도 숫자 형식을 활용합니다. 
        일반적으로 대부분의 모델은 변수를 나타내고 처리하기 위해 32비트 부동 소수점 정밀도(fp32 또는 float32)를 사용합니다. 
        그러나 모든 변수가 정확한 결과를 얻기 위해 이러한 높은 정밀도 수준을 필요로 하는 것은 아닙니다. 
        특정 변수의 정밀도를 16비트 부동 소수점(fp16 또는 float16)과 같은 낮은 숫자 형식으로 줄임으로써 계산 속도를 높일 수 있습니다. 
        이 방법에서는 일부 계산이 반 정밀도에서 수행되고 일부는 여전히 전체 정밀도에서 수행되기 때문에 이 방법을 혼합 정밀도 훈련이라고 부릅니다.
      </p>  

      <div class="spacer"></div>
      <h3>Gradient Accumulation</h3>
      <div class="small-spacer"></div>
      <p> &nbsp; 기울기 누적 방법은 전체 배치를 한 번에 계산하는 대신 작은 증분으로 기울기를 계산하는 방법을 지향합니다. 
        이 방법은 모델을 통해 순전파와 역전파를 수행하면서 작은 배치에서 반복적으로 기울기를 계산하고 이 과정 동안 기울기를 누적하는 방식으로 이루어집니다. 
        충분한 수의 기울기가 누적되면 모델의 최적화 단계가 실행됩니다. 
        기울기 누적을 사용하면 GPU 메모리 용량의 제한을 초과하여 효과적인 배치 크기를 증가시킬 수 있습니다. 
        그러나 기울기 누적에 의해 도입된 추가적인 순전파와 역전파는 훈련 과정을 느리게 만들 수 있다는 점을 주의해야 합니다.
      </p>  


      <div class="spacer"></div>
      <h3>Gradient Checkpointing</h3>
      <div class="small-spacer"></div>
      <p> &nbsp; 순전파에서 모든 활성화 값을 저장하여 역전파 중에 기울기를 계산하는 것은 상당한 메모리 오버헤드를 초래할 수 있습니다. 
        활성화 값을 버리고 역전파 중에 필요할 때 다시 계산하는 대안적인 방법은 상당한 계산 오버헤드를 도입하고 훈련 과정을 느리게 만들 수 있습니다.
        Gradient Checkpointing은 이 두 가지 접근법 사이의 타협점을 제공하며 계산 그래프 전체에서 전략적으로 선택된 활성화 값을 저장하여 기울기를 다시 계산해야 하는 활성화 값의 일부만 다시 계산하면 됩니다.
      </p>  


      <div class="spacer"></div>
      <h3>Efficient Optimizer</h3>
      <div class="small-spacer"></div>
      <p> &nbsp; 트랜스포머 모델을 훈련하는 데 가장 일반적으로 사용되는 옵티마이저는 Adam 또는 AdamW(가중치 감소가 적용된 Adam)입니다. 
        Adam은 이전 기울기의 이동 평균을 저장하여 좋은 수렴을 달성하지만, 모델 파라미터 수에 비례한 추가적인 메모리 풋프린트를 추가합니다. 
        이를 해결하기 위해 대체 옵티마이저를 사용할 수 있습니다. 

        대표적인 대체 옵티마인저인 Adafactor는 각 가중치 행렬의 각 원소에 대한 롤링 평균을 저장하지 않습니다. 
        대신에 롤링 평균의 행 및 열별로 집계된 정보를 유지하여 풋프린트를 크게 감소시킵니다. 
        그러나 Adam과 비교하여 Adafactor는 특정 경우에는 수렴이 더 느릴 수 있습니다.
      </p>  


    <h2>2. &hairsp; Experimental Setup</h2>

      <div class="small-spacer"></div>
      <h3>Data Setup</h3>
      <div class="small-spacer"></div>
      <p>Sequence Classification을 위한 대표적 데이터 셋 중하나인 AG_News를 사용.
        데이터의 크기는 Train/ Valid/ Test 1000/ 100/ 100으로 설정
        총 네가지 데이터 Label을 모든 데이터에서 고르게 분배해서 한쪽으로 치우치지 않도록 가공
      </p>

      <div class="spacer"></div>
      <h3>Model Setup</h3>
      <div class="small-spacer"></div>
      <p>대표적인 PreTrained Model인 BERT를 사용.
      </p>


      <div class="spacer"></div>
      <h3>Training Setup</h3>
      <div class="small-spacer"></div>

      <div class="code-container">
        <div class="code-snippet">
          <pre><code class="python">
TrainingArguments(
        output_dir= f'ckpt/{strategy}',
        num_train_epochs= 5,
        learning_rate= 1e-5,
        per_device_train_batch_size= 32,
        per_device_eval_batch_size= 32,
        lr_scheduler_type='reduce_lr_on_plateau',
        load_best_model_at_end= True,

        save_strategy= 'epoch',
        logging_strategy= 'epoch',
        evaluation_strategy= 'epoch',

        fp16= True if config.strategy in ['fp16', 'all'] else False,
        fp16_opt_level= '02' if config.strategy in ['fp16', 'all'] else '01',
        gradient_accumulation_steps = True if config.strategy in ['grad_accumulation', 'all'] else 4,
        gradient_checkpointing= True if config.strategy in ['grad_checkpointing', 'all'] else False,
        optim = 'adafactor' if config.strategy in ['optim', 'all'] else 'adamw_torch'
)
          </code></pre>
        </div>
      </div>



    <h2>3. &hairsp; Result</h2>
    <div class="small-spacer"></div>
    <table class="result-table">
      <thead>
        <tr>
          <th>Training Strategy</th>
          <th>Training Time</th>
          <th>Accuracy</th>
          <th>Avg GPU Memory</th>
          <th>Max GPU Memory</th>
          
        </tr>
      </thead>
      <tbody>
        <tr>
          <td>Vanilla Training</td>
          <td>174</td>
          <td>7.00</td>
          <td>79</td>
        </tr>
        <tr>
          <td>Mixed Precision (FP 16)</td>
          <td>69</td>
          <td>5.47</td>
          <td>78</td>
        </tr>
        <tr>
          <td>Gradient Accumulation</td>
          <td>182</td>
          <td>6.29</td>
          <td>83</td>
        </tr>

        <tr>
          <td>Gradient Checkpointing</td>
          <td>239</td>
          <td>3.54</td>
          <td>79</td>
        </tr>

        <tr>
          <td>Adafactor Optimizer</td>
          <td>179</td>
          <td>6.72</td>
          <td>79</td>
        </tr>

        <tr>
          <td>All Strategies Applied</td>
          <td>85</td>
          <td>2.91</td>
          <td>80</td>
        </tr>

      </tbody>
    </table>


    <h2>4. &hairsp; Conclusion</h2>
    <div class="small-spacer"></div>
    <p> &nbsp; 순전파에서 모든 활성화 값을 저장하여 역전파 중에 기울기를 계산하는 것은 상당한 메모리 오버헤드를 초래할 수 있습니다. 
      활성화 값을 버리고 역전파 중에 필요할 때 다시 계산하는 대안적인 방법은 상당한 계산 오버헤드를 도입하고 훈련 과정을 느리게 만들 수 있습니다.
      Gradient Checkpointing은 이 두 가지 접근법 사이의 타협점을 제공하며 계산 그래프 전체에서 전략적으로 선택된 활성화 값을 저장하여 기울기를 다시 계산해야 하는 활성화 값의 일부만 다시 계산하면 됩니다.
    </p>  

    <h2 id="7" class="h3-mt">5. &hairsp; Reference</h2>


  </div>
</div>

<div class="pagination">
  <a href="{{ '/transformer_variants' | relative_url }}" class="btn-prev"></a>
  <a href="{{ '/eff_model' | relative_url }}" class="btn-next"></a>
</div>