---
layout: default
permalink: /eff_train
---

<div id="detail">
  <h1 class="title">Efficient Training Methodology Analytics</h1>  
  <div class="post">

    <hr>
    <div class="callout">
      <p class="highlighted-text"><span>Project Objective</span></p>
      <div class="small-spacer"></div>
      <p>&nbsp; 이 프로젝트에서는 효율적인 사전학습 모델들의 효율성과 성능을 비교합니다. 총 두가지의 모델군으로 나누어 비교를 진행합니다.
        첫번째 모델군에는 파라미터의 수를 줄여 경량화 함으로써 효율성을 제고시킨 모델들을. 두번째 모델군은 어텐션 연산과정에서 효율성을 증대시킨 모델들.
        모든 모델군의 Baseline 모델로는 BERT를 설정
        첫번째 모델군에

      사전학습된 모델이 좋은 성능을 보인다는 것은 다양한 방면에 걸쳐 입증되어 왔습니다. 일반적으로 더 큰 모델일수록 더 좋은 성능을 보이고는 있지만, 그 만큼 많은 컴퓨팅자원을 필요로합니다. 때문에 모델의 성능을 최대한 유지하면서도 모델의 크기를 줄이거나, 연산을 단순화 하는 등의 효율성을 제고시키기 위한 모델들이 제시되었습니다. 하지만 이러한 효율적 모델들을 동일한 조건에서 비교 검증해보는 연구는 부재합니다. 이런 문제를 해결하고, 효율성의 측면에서 다양한 사전학습 모델들을 검증해봅니다.</p>
    </div>

    <div class="tblContents">
      <ul>

        <li>
          <a href="#1">1. &hairsp; Introduction</a>
        </li>

        <li>
          <a href="#2">2. &hairsp; Efficient Trainings</a>
        </li>

        <li>
          <a href="#3">3. &hairsp; Attention-wise Efficient Models</a>
        </li>

        <li>
          <a href="#4">4. &hairsp; Experimental Setup</a>
        </li>

        <li>
          <a href="#5">5. &hairsp; Results</a>
        </li>

        <li>
          <a href="#6">6. &hairsp; Conclusion</a>
        </li>

        <li>
          <a href="#7">7. &hairsp; Reference</a>
        </li>

      </ul>
    </div>
    <hr>


Mixed Precision Training
Gradient Accumulation
Gradient Checkpointing
Efficient Optimizer


<!-- 1. Introduction -->    
    <h2 id="1">&hairsp; 1. &hairsp; Introduction</h2>
    <div class="chapter-box">

      <div class="list-container">
        <ul>
          <li>Description</li>
            <p> &nbsp; 현재 다양한 대규모 사전학습 언어 모델들이 자연어 처리 분야에서 뛰어난 활약을 보이고 있습니다. 하지만 모델이 과도하게 크기 때문에 일반적인 컴퓨팅 자원에서 학습 및 추론이 어렵다는 단점이 존재. 
              비단 커지고, 고성능 만을 쫒는 연구뿐만아니라, 효율적인 모델을 향한 다양한 연구 역시 진행되고 있음. 대표적인 효율적 모델 연구는 두 갈래로 나누어볼 수 있음. 모델의 파라미터 개수를 감소시키는 방향과, 핵심연산인 Attention의 \( O^2 \) 복잡성을 개선시키는 방향의 연구.
              첫번째 연구를 Parameter-wise Efficient Model, 두번째는 Attention-wise Efficient Model이라고 명명하고, 각 연구에 부합하는 모델들을
              <br>
              파라미터 효율적 모델들로는 Distil BERT, AlBERT, Mobile BERT
              어텐션 효율적 모델로는 Longformer, BigBird
              <br>
              두 모델 군의 비교의 근거가 되는 Baseline Model로는 모두 BERT를 사용.
              <br>
              데이터를 굳이 두개로 쪼개서 사용할 이유가 없을듯 ... ㅇㅇㅇ 이거는 코드 구현을 위한 참고!

            </p>

          <div class="spacer"></div>
          <li>Objective
            <ul>
              <li style="font-weight: normal;">효율성 증진과 크로스 오버로 발생가능한 성능 하락 사이 </li>
              <li style="font-weight: normal;">다양한 구현 방식에 따른 Transformer의 성능 차이를 직접 확인</li>
              <li style="font-weight: normal;">Transformer의 자연어 생성 능력 측정을 통해 Transformer BaseLine 확립</li>
              <li style="font-weight: normal;">직접 구현한 Transformer와 Pytorch에서 제공하는 Transformer모델의 성능 비교</li>
            </ul>
          </li>

        </ul>
      </div>

    </div>



<!-- 1. Introduction -->    
    <h2>2. &hairsp; Experimental Setup</h2>

      <div class="small-spacer"></div>
      <h3>Data Setup</h3>
      <div class="small-spacer"></div>
      <p>Sequence Classification을 위한 대표적 데이터 셋 중하나인 AG_News를 사용.
        데이터의 크기는 Train/ Valid/ Test 1000/ 100/ 100으로 설정
        총 네가지 데이터 Label을 모든 데이터에서 고르게 분배해서 한쪽으로 치우치지 않도록 가공
      </p>

      <div class="spacer"></div>
      <h3>Model Setup</h3>
      <div class="small-spacer"></div>
      <p>대표적인 PreTrained Model인 BERT를 사용.
      </p>


      <div class="spacer"></div>
      <h3>Training Setup</h3>
      <div class="small-spacer"></div>

      <div class="code-container">
        <div class="code-snippet">
          <pre><code class="python">
TrainingArguments(
        output_dir= f'ckpt/{strategy}',
        num_train_epochs= 5,
        learning_rate= 1e-5,
        per_device_train_batch_size= 32,
        per_device_eval_batch_size= 32,
        lr_scheduler_type='reduce_lr_on_plateau',
        load_best_model_at_end= True,

        save_strategy= 'epoch',
        logging_strategy= 'epoch',
        evaluation_strategy= 'epoch',

        fp16= True if config.strategy in ['fp16', 'all'] else False,
        fp16_opt_level= '02' if config.strategy in ['fp16', 'all'] else '01',
        gradient_accumulation_steps = True if config.strategy in ['grad_accumulation', 'all'] else 4,
        gradient_checkpointing= True if config.strategy in ['grad_checkpointing', 'all'] else False,
        optim = 'adafactor' if config.strategy in ['optim', 'all'] else 'adamw_torch'
)
          </code></pre>
        </div>
      </div>



    <h2>3. &hairsp; Result</h2>
    <div class="small-spacer"></div>
    <table class="result-table">
      <thead>
        <tr>
          <th>Training Strategy</th>
          <th>Training Time</th>
          <th>Accuracy</th>
          <th>Avg GPU Memory</th>
          <th>Max GPU Memory</th>
          
        </tr>
      </thead>
      <tbody>
        <tr>
          <td>Vanilla Training</td>
          <td>174</td>
          <td>7.00</td>
          <td>79</td>
        </tr>
        <tr>
          <td>Mixed Precision (FP 16)</td>
          <td>69</td>
          <td>5.47</td>
          <td>78</td>
        </tr>
        <tr>
          <td>Gradient Accumulation</td>
          <td>182</td>
          <td>6.29</td>
          <td>83</td>
        </tr>

        <tr>
          <td>Gradient Checkpointing</td>
          <td>239</td>
          <td>3.54</td>
          <td>79</td>
        </tr>

        <tr>
          <td>Adafactor Optimizer</td>
          <td>179</td>
          <td>6.72</td>
          <td>79</td>
        </tr>

        <tr>
          <td>All Strategies Applied</td>
          <td>85</td>
          <td>2.91</td>
          <td>80</td>
        </tr>

      </tbody>
    </table>


    <h2>4. &hairsp; Conclusion</h2>
    <div class="small-spacer"></div>
    <p> &nbsp; 순전파에서 모든 활성화 값을 저장하여 역전파 중에 기울기를 계산하는 것은 상당한 메모리 오버헤드를 초래할 수 있습니다. 
      활성화 값을 버리고 역전파 중에 필요할 때 다시 계산하는 대안적인 방법은 상당한 계산 오버헤드를 도입하고 훈련 과정을 느리게 만들 수 있습니다.
      Gradient Checkpointing은 이 두 가지 접근법 사이의 타협점을 제공하며 계산 그래프 전체에서 전략적으로 선택된 활성화 값을 저장하여 기울기를 다시 계산해야 하는 활성화 값의 일부만 다시 계산하면 됩니다.
    </p>  

    <h2 id="7" class="h3-mt">5. &hairsp; Reference</h2>


  </div>
</div>

<div class="pagination">
  <a href="{{ '/transformer_variants' | relative_url }}" class="btn-prev"></a>
  <a href="{{ '/eff_model' | relative_url }}" class="btn-next"></a>
</div>


<div class="pagination">
  <a href="{{ '/gan_train' | relative_url }}" class="btn-prev"><span>SeqGAN</span></a>
  <a href="{{ '/eff_model' | relative_url }}" class="btn-next"><span>Efficient Models</span></a>
</div>