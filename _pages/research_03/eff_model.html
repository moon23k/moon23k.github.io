---
layout: default
permalink: /eff_model
---


<div id="detail">
  <h1 class="title">Efficient PLM Comparision</h1>  
  <div class="post">


    <div class="callout">
      <p class="highlighted-text"><span>Project Objective</span></p>
      <div class="small-spacer"></div>
      <p>&nbsp; 사전학습된 모델이 좋은 성능을 보인다는 것은 다양한 방면에 걸쳐 입증되어 왔습니다. 일반적으로 더 큰 모델일수록 더 좋은 성능을 보이고는 있지만, 그 만큼 많은 컴퓨팅자원을 필요로합니다. 때문에 모델의 성능을 최대한 유지하면서도 모델의 크기를 줄이거나, 연산을 단순화 하는 등의 효율성을 제고시키기 위한 모델들이 제시되었습니다. 하지만 이러한 효율적 모델들을 동일한 조건에서 비교 검증해보는 연구는 부재합니다. 이런 문제를 해결하고, 효율성의 측면에서 다양한 사전학습 모델들을 검증해봅니다.</p>
    </div>

    <div class="tblContents">
      <ul>

        <li>
          <a href="#1">1. &hairsp; Efficient Pretrained Models</a>
          <ul>
            <li>
              <a href="#1.1">1.1 &hairsp; Parameter wise Efficient Models</a>
            </li>
            <li>
              <a href="#1.2">1.2 &hairsp; Attention wise Efficient Models</a>
            </li>
          </ul>
        </li>

        <li>
          <a href="#2">2. &hairsp; Experimental Setup</a>
        </li>

        <li>
          <a href="#3">3. &hairsp; Results</a>
        </li>

        <li>
          <a href="#4">4. &hairsp; Conclusion</a>
        </li>

        <li>
          <a href="#5">5. &hairsp; Reference</a>
        </li>

      </ul>
    </div>
    <hr>


<!-- Project Desc -->    
    <h2 id="1">1. &hairsp; Efficient Pretrained Models</h2>

      <h3 id="1.1">1.1 &hairsp; Attention wise Efficient Models</h3>

      <div class="list-container">
        <ul>
          <li>AlBERT</li>        
            <p> 기울기 누적 방법은 전체 배치를 한 번에 계산하는 대신 작은 증분으로 기울기를 계산하는 방법을 지향합니다. 
              이 방법은 모델을 통해 순전파와 역전파를 수행하면서 작은 배치에서 반복적으로 기울기를 계산하고 이 과정 동안 기울기를 누적하는 방식으로 이루어집니다. 
              충분한 수의 기울기가 누적되면 모델의 최적화 단계가 실행됩니다. 
              기울기 누적을 사용하면 GPU 메모리 용량의 제한을 초과하여 효과적인 배치 크기를 증가시킬 수 있습니다. 
              그러나 기울기 누적에 의해 도입된 추가적인 순전파와 역전파는 훈련 과정을 느리게 만들 수 있다는 점을 주의해야 합니다.
            </p>  

          <div class="half-spacer"></div>
          <li>Distill BERT</li>        
            <p> &nbsp; Knowledge Distillation을 활용해 BERT와 거의 비슷하면서도, 매우 적은 파라미터를 보유한 모델.
            </p>  

          <div class="half-spacer"></div>
          <li>Mobile BERT</li>        
            <p> &nbsp; 생활 필수품인 모바일 디바이스에서도 사용할 수 있을 정도로 경량화하는 것을 목표로 한 모델.
              경량화를 위해 Mobile BERT는 
            </p>  

        </ul>
      </div>


      <div class="spacer"></div>
      <h3 id="1.2">1.2 &hairsp; Attention wise Efficient Models</h3>

      <div class="list-container">
        <ul>
          <li>Longformer</li>        
            <p> &nbsp; 기울기 누적 방법은 전체 배치를 한 번에 계산하는 대신 작은 증분으로 기울기를 계산하는 방법을 지향합니다. 
              이 방법은 모델을 통해 순전파와 역전파를 수행하면서 작은 배치에서 반복적으로 기울기를 계산하고 이 과정 동안 기울기를 누적하는 방식으로 이루어집니다. 
              충분한 수의 기울기가 누적되면 모델의 최적화 단계가 실행됩니다. 
              기울기 누적을 사용하면 GPU 메모리 용량의 제한을 초과하여 효과적인 배치 크기를 증가시킬 수 있습니다. 
              그러나 기울기 누적에 의해 도입된 추가적인 순전파와 역전파는 훈련 과정을 느리게 만들 수 있다는 점을 주의해야 합니다.
            </p>  

          <div class="half-spacer"></div>
          <li>Bigbird</li>        
            <p> &nbsp; 기울기 누적 방법은 전체 배치를 한 번에 계산하는 대신 작은 증분으로 기울기를 계산하는 방법을 지향합니다. 
              이 방법은 모델을 통해 순전파와 역전파를 수행하면서 작은 배치에서 반복적으로 기울기를 계산하고 이 과정 동안 기울기를 누적하는 방식으로 이루어집니다. 
              충분한 수의 기울기가 누적되면 모델의 최적화 단계가 실행됩니다. 
              기울기 누적을 사용하면 GPU 메모리 용량의 제한을 초과하여 효과적인 배치 크기를 증가시킬 수 있습니다. 
              그러나 기울기 누적에 의해 도입된 추가적인 순전파와 역전파는 훈련 과정을 느리게 만들 수 있다는 점을 주의해야 합니다.
            </p>  

        </ul>
      </div>
      <hr>


<!-- Experimental Setup -->    
    <h2>2. Experimental Setup</h2>

    <h3>Setups for Param-wise Models</h3>
    <table class="result-table">
      <thead>
        <tr>
          <th>Data Setup</th>
          <th>Model Setup</th>
          <th>Training Setup</th>
        </tr>
      </thead>
      <tbody>
        <tr>

          <td>
            <ul>
              <li><b>Dataset</b>: &hairsp; AG News</li>
              <li><b>Train Data Volumn</b>: &hairsp; 50,000</li>
              <li><b>Valid Data Volumn</b>: &hairsp; 5,000</li>
              <li><b>Test Data Volumn</b>: &hairsp; 100</li>
            </ul>
          </td>

          <td>
            <ul>
              <li><b>BERT</b>: &hairsp; bert-base-uncased</li>
              <li><b>Distil BERT</b>: &hairsp; 512</li>
              <li><b>AlBERT</b>: &hairsp; 000</li>
              <li><b>Mobile BERT</b>: &hairsp; 000</li>
            </ul>
          </td>

          <td>
            <ul>
              <li><b>Batch Size</b>: &hairsp; 32</li>
              <li><b>Num Epochs</b>: &hairsp; 5</li>
              <li><b>Apply AMP</b>: &hairsp; True</li>
              <li><b>Learning Rate</b>: &hairsp; 5e-4</li>
              <li><b>Optimizer</b>: &hairsp; Adafactor</li>
              <li><b>Gradient Checkpointing</b>: &hairsp; True</li>
              <li><b>Gradient Accumulation Step</b>: &hairsp; 4</li>
            </ul>
          </td>          

        </tr>

      </tbody>
    </table>

    <div class="spacer"></div>
    <h3>Setups for Attention-wise Models</h3>
    <table class="result-table">
      <thead>
        <tr>
          <th>Data Setup</th>
          <th>Model Setup</th>
          <th>Training Setup</th>
        </tr>
      </thead>
      <tbody>
        <tr>

          <td>
            <ul>
              <li><b>Dataset</b>: &hairsp; IMDB</li>
              <li><b>Train Data Volumn</b>: &hairsp; 1,000</li>
              <li><b>Valid Data Volumn</b>: &hairsp; 100</li>
              <li><b>Test Data Volumn</b>: &hairsp; 100</li>
            </ul>
          </td>

          <td>
            <ul>
              <li><b>BERT</b>: &hairsp; bert-base-uncased</li>
              <li><b>Longbird</b>: &hairsp; 512</li>
              <li><b>Bigbird</b>: &hairsp; 000</li>
            </ul>
          </td>

          <td>
            <ul>
              <li><b>Batch Size</b>: &hairsp; 32</li>
              <li><b>Num Epochs</b>: &hairsp; 5</li>
              <li><b>Apply AMP</b>: &hairsp; True</li>
              <li><b>Learning Rate</b>: &hairsp; 5e-4</li>
              <li><b>Optimizer</b>: &hairsp; Adafactor</li>
              <li><b>Gradient Checkpointing</b>: &hairsp; True</li>
              <li><b>Gradient Accumulation Step</b>: &hairsp; 4</li>
            </ul>
          </td>          

        </tr>

      </tbody>
    </table>
    <div class="half-spacer"></div>
    <hr>



<!-- Result -->    
    <h2>3. Result</h2>

    <h3>Param wise Efficient Models</h3>
    <table class="result-table">
      <thead>
        <tr>
          <th>Model Type</th>
          <th>Accuracy</th>
          <th>Epoch Time</th>
          <th>Avg GPU</th>
          <th>Max GPU</th>
        </tr>
      </thead>
      <tbody>
        <tr>
          <td>BERT (Baseline Model)</td>
          <td>-</td>
          <td>-</td>
          <td>-</td>
          <td>-</td>
        </tr>
        <tr>
          <td>Distil BERT</td>
          <td>-</td>
          <td>-</td>
          <td>-</td>
          <td>-</td>
        </tr>
        <tr>
          <td>AlBERT</td>
          <td>-</td>
          <td>-</td>
          <td>-</td>
          <td>-</td>
        </tr>
        <tr>
          <td>MobileBERT</td>
          <td>-</td>
          <td>-</td>
          <td>-</td>
          <td>-</td>
        </tr>        

      </tbody>
    </table>

    <div class="half-spacer"></div>
    <h3>Attention wise Efficient Models</h3>
    <table class="result-table">
      <thead>
        <tr>
          <th>Model Type</th>
          <th>Accuracy</th>
          <th>Epoch Time</th>
          <th>Avg GPU</th>
          <th>Max GPU</th>
        </tr>
      </thead>
      <tbody>
        <tr>
          <td>BERT (Baseline Model)</td>
          <td>-</td>
          <td>-</td>
          <td>-</td>
          <td>-</td>
        </tr>
        <tr>
          <td>Longformer</td>
          <td>-</td>
          <td>-</td>
          <td>-</td>
          <td>-</td>
        </tr>
        <tr>
          <td>Bigbird</td>
          <td>-</td>
          <td>-</td>
          <td>-</td>
          <td>-</td>
        </tr>

      </tbody>
    </table>
    <hr>


<!-- Conclusion -->    
    <h2>4. Conclusion</h2>



<!-- Reference -->    
    <h2>5. Reference</h2>




  </div>
</div>


<div class="pagination">
  <a href="{{ '/eff_train' | relative_url }}" class="btn-prev"></a>
  <a href="{{ '/peft' | relative_url }}" class="btn-next"></a>
</div>