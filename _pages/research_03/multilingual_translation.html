---
layout: default
permalink: /multilingual_translation
---


<div id="detail">
  <h1 class="title">Multi-Lingual&hairsp; Machine Translation</h1>  
  <div class="post">

    <hr>
    <div class="callout">
      <p class="highlighted-text"><span>Project Objective</span></p>
      <div class="small-spacer"></div>
      <p>&nbsp; 이 프로젝트에서는 Multi Modality 증진을 위해, 효과적인 모델 디자인을 탐구합니다.
        데이터로는 AI HUB의 한국어-영어 번역 데이터셋을 사용했으며, 표준 트랜스포머에서 깊이, 너비, 다양성을 변인으로 하여, 각 모델 디자인 별 Multi Modality에서의 효용성을 탐구합니다. 
      </p>
    </div>

    <div class="tblContents">
      <ul>

        <li>
          <a href="#1">1. &hairsp; Introduction</a>
        </li>

        <li>
          <a href="#2">2. &hairsp; Background</a>
        </li>

        <li>
          <a href="#3">3. &hairsp; Architecture</a>
        </li>

        <li>
          <a href="#4">4. &hairsp; Experimental Setup</a>
        </li>

        <li>
          <a href="#5">5. &hairsp; Results</a>
        </li>

        <li>
          <a href="#6">6. &hairsp; Conclusion</a>
        </li>

        <li>
          <a href="#7">7. &hairsp; Reference</a>
        </li>

      </ul>
    </div>
    <hr>


<!-- 1. Introduction -->    
    <h2 id="1">&hairsp; 1. &hairsp; Introduction</h2>
    <div class="chapter-box">

      <div class="list-container">
        <ul>
          <li>Description</li>
          <p>&nbsp; 기계 번역은 가장 유명한 자연어 생성과제중 하나입니다. 덕분에 단일 언어 번역 과제에서의 성능을 향상시키기 위한 모델 관점에서의, 학습법 관점에서의 다양한 연구들이 있습니다. 하지만, 다언어 번역을 위한 모델링 및 학습 방식에 대한 연구는 상대적으로 부족합니다. 이 프로젝트에서는 다언어 번역 모델에 대한 연구가 부족하다는 문제점을 직접 해결하려 합니다.
          다언어 번역도 크게 세 가지로 나누어볼수 있습니다. 다언어를 이해하고, 하나의 언어로 변환하는 모델. 하나의 언어를 이해하고, 다언어로 표현할 수 있는 모델. 다언어를 이해하고, 다언어로 변환할 수 있는 모델.
          각 모델은 인코더 혹은 디코더의 무게중심이 상이합니다. 첫번째는 인코더의 성능이, 두번째는 디코더의 성능이, 마지막은 인코더와 디코더가 모두 중요하게 작용합니다.

          </p>

          <div class="spacer"></div>
          <li>Objective
            <ul>
              <li style="font-weight: normal;">다언어 번역을 위한 깊은 이해 증진</li>
              <li style="font-weight: normal;">인코더와 디코더의 디자인 밸런스 조정을 통해 목적에 맞는 모델 디자인 적합성 확인</li>
              <li style="font-weight: normal;">다언어 모델링의 가능성을 다양한 방면에서 확인</li>
            </ul>
          </li>

        </ul>
      </div>

    </div>



<!-- 2. Background -->    
    <h2 id="2">&hairsp; 2. &hairsp; Background</h2>
    <div class="chapter-box">

      <div class="list-container">
        <ul>
          <li>Multi Encoding Single Decoding</li>
          <p>&nbsp; 프로젝트의 실험에서는 한국어-영어 번역과제를 선정했습니다. 이를 위한 데이터 셋으로는 Ai Hub의 공개데이터를 사용했습니다. 총 10개의 세부 항목으로 나누어진 데이터 중, 가장 범용적인 언어 표현 도메인으로 이루어진 000 데이터 들을 사용합니다. 총 데이터 개수는 000개이며, 이 중, 학습, 검증, 테스트 데이터 셋으로 각각 000 개씩 사용합니다. 데이터 예시는 아래와 같습니다. 
          </p>

          <div class="spacer"></div>
          <li>Single Encoding Multi Decoding</li>
          <p>&nbsp; PLE Fused Model은 "Incorporating BERT into Neural Machine Translation" 논문에서 제시한 모델 구조를 사용합니다. 사전학습된 인코더 모델을 그대로 인코더로만 사용하는것이 아니라, 인코딩과 디코딩 과정에 추가적인 정보전달을 위한 모델로 활용합니다.
          </p>

          <div class="spacer"></div>
          <li>Multi Encoding Multi Decoding</li>
          <p>&nbsp; PLE Fused Model은 "Incorporating BERT into Neural Machine Translation" 논문에서 제시한 모델 구조를 사용합니다. 사전학습된 인코더 모델을 그대로 인코더로만 사용하는것이 아니라, 인코딩과 디코딩 과정에 추가적인 정보전달을 위한 모델로 활용합니다.
          </p>

        </ul>
      </div>
    </div>



<!-- 3. Architecture -->    
    <h2 id="3">&hairsp; 3.&hairsp; Architecture</h2>
    <div class="chapter-box">      
      <div class="list-container">
        <ul>

          <li>Hierarchical Linear Encoder Model
            <ul>
              <li></li>
              <li></li>
              <li></li>
            </ul>
          </li>   


          <div class="spacer"></div>
          <li>Hierarchical RNN Encoder Model
            <ul>
              <li></li>
              <li></li>
              <li></li>
            </ul>
          </li>           


          <div class="spacer"></div>
          <li>Hierarchical Attention Encoder Model
            <ul>
              <li></li>
              <li></li>
              <li></li>
            </ul>
          </li>           


          <div class="spacer"></div>
          <li>Hierarchical Fusion Encoder Model
            <ul>
              <li></li>
              <li></li>
              <li></li>
            </ul>
          </li>           

        </ul>
      </div>      
    </div>         


<!-- 4. Experimental Setup -->
    <h2 id="4">&hairsp; 4. &hairsp; Experimental Setup</h2>
    <div class="chapter-box">

      <div class="list-container">
        <ul>
          <li>Data Setup
            <div class="dual-container">
              <ul>
                <li>Dataset: &nbsp; Conala</li>
                <li>Dialogue Task: &nbsp; Daily Dialogue</li>
                <li>Summarization Task: &nbsp; CNN Daily</li>
                <li>Tokenizer: &nbsp; AlBERT Tokenizer</li>
              </ul>
              <ul>
                <li>Train Data Volumn: &nbsp; 50,000</li>
                <li>Valid Data Volumn: &nbsp; 5,000</li>
                <li>Test Data Volumn: &nbsp; 100</li>
                <li>Vocab Size: &nbsp; 10,000</li>                
              </ul>
            </div>
          </li>

          <div class="half-spacer"></div>
          <li>Model Setup
            <div class="dual-container">
              <ul>
                <li>PLE Architecture: &nbsp; AlBERT</li>
                <li>PLE Name: &nbsp; albert-v2</li>
                <li>Input Dim: &nbsp; 10,000</li>
                <li>Output Dim: &nbsp; 10,000</li>
              </ul>
              <ul>
                <li>Embedding Dim: &nbsp; 512</li>
                <li>Hidden Dim: &nbsp; 512</li>
                <li>Model Params: &nbsp; 000</li>
                <li>Model Size: &nbsp; 000</li>                
              </ul>
            </div>
          </li>

          <div class="half-spacer"></div>
          <li>Training Setup
            <div class="dual-container">
              <ul>
                <li>Num Epochs: &nbsp; 10</li>
                <li>Batch Size: &nbsp; 32</li>
                <li>Learning Rate: &nbsp; 5e-4</li>
                <li>LR Scheduler: &nbsp; pleature</li>
              </ul>
              <ul>
                <li>Optimizer: &nbsp; AdamW</li>
                <li>Gradient Accumulation Steps: &nbsp; 4
                <li>Teacher Forcing Ratio: &nbsp; 0.5</li>                
              </ul>
            </div>
          </li>
        
        </ul>
      </div>
    </div>


<!-- 5. Result -->
    <h2 id="5">&hairsp; 5. &hairsp; Result</h2>
    <div class="chapter-box">

      <div class="list-container">
        <ul>
          <li>Result Table</li>

            <table class="result-table">
              <thead>
                <tr>
                  <th>Model Type</th>
                  <th>Eval Score</th>
                  <th>Epoch Time</th>
                  <th>Avg GPU</th>
                  <th>Max GPU</th>
                </tr>
              </thead>
              <tbody>
                <tr>
                  <td>RNN Model</td>
                  <td>2.12</td>
                  <td>3m 58s</td>
                  <td>0.22GB</td>
                  <td>0.82GB</td>
                </tr>
                <tr>
                  <td>LSTM Model</td>
                  <td>8.35</td>
                  <td>4m 16s</td>
                  <td>0.27GB</td>
                  <td>1.30GB</td>
                </tr>
                <tr>
                  <td>GRU Model</td>
                  <td>9.75</td>
                  <td>4m 6s</td>
                  <td>0.26GB</td>
                  <td>1.16GB</td>
                </tr>

              </tbody>
            </table>
          

          <div class="spacer"></div>
          <li>Result Analysis</li>
            <p> &nbsp; RNN은 다양한 정보를 함유하기에 지나치게 단순하고, LSTM은 GATE구조가 복잡하기에, 그만큼 영향을 받은 요소가 많아 학습 수렴이 어렵습니다. 때문에 모든 학습 결과에서는 GATE를 사용하면서도 단순화된 연산으로 학습의 이점이 큰 GRU가 가장 뛰어난 성능을 보임을 알 수 있습니다. 
            </p>
        </ul>
      </div>
    </div>


<!-- 6. Conclusion -->
    <h2 id="6">&hairsp; 6. &hairsp; Conclusion</h2>
    <div class="chapter-box">

      <div class="list-container">
        <ul>
          <li>RNN, LSTM, GRU 을 활용한 seq2seq 모델링 방식의 이해</li>
            <p>&nbsp; 토큰의 벡터 변환을 위한 Embedding Layer와 실제 연산을 위한 순환 신경망 레이어만으로 구성된 단순산 Encoder, Decoder 구조로 이루어져 있으며, 
            </p>

          <div class="spacer"></div>
          <li>세 가지 순환 신경망의 성능 비교 검증</li>
            <p>LSTM은 게이트 메커니즘을 통해 장기 의존성 문제를 해결하는 데 강점을 보입니다. 복잡한 문맥을 학습하고 기억하는 데 탁월하며, 긴 시퀀스 데이터에서 뛰어난 성능을 발휘할 수 있습니다. 하지만 LSTM은 더 많은 파라미터를 가지고 있어서 더 많은 데이터와 계산 리소스가 필요할 수 있습니다.
            </p>

        </ul>
      </div>
    </div>


<!-- Reference -->    
    <h2 id="7">7. &hairsp; Reference</h2>
    <div class="chapter-box">
      <a class="reference" href="https://arxiv.org/abs/1706.03762">&nbsp; Attention Is All You Need</a> <br> 
      <a class="reference" href="https://aclanthology.org/D18-1045.pdf">&nbsp; Understanding Back-Translation at Scale</a> 
    </div>

  </div>
</div>


<div class="pagination">
  <a href="{{ '/intelligen' | relative_url }}" class="btn-prev"><span>IntelliGEN</span></a>
  <a href="{{ '/multiturn_dialogue' | relative_url }}" class="btn-next"><span>Multi-Turn Dialogue Generation</span></a>
</div>