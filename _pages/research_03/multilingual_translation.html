---
layout: default
permalink: /multilingual_translation
---


<div id="detail">
  <h1 class="title">Multi-Lingual&hairsp; Translation</h1>  
  <div class="post">

    <hr>
    <div class="callout">
      <p class="highlighted-text"><span>Project Objective</span></p>
      <div class="small-spacer"></div>
      <p>&nbsp; 이 프로젝트에서는 다언어 번역 과제 해결에 효과적인 모델 디자인을 탐구합니다. 모델 디자인의 변인으로는 <b>Unbalanced, Evolved Hybrid, KoBERT Fusion</b> 모델들을 사용해 AI HUB의 한국어 번역 데이터셋에서의 성능을 통해 각 모델 별 효용성을 검증합니다. 
      </p>
    </div>

    <div class="tblContents">
      <ul>

        <li>
          <a href="#1">1. &hairsp; Introduction</a>
        </li>

        <li>
          <a href="#2">2. &hairsp; Background</a>
        </li>

        <li>
          <a href="#3">3. &hairsp; Architecture</a>
        </li>

        <li>
          <a href="#4">4. &hairsp; Experimental Setup</a>
        </li>

        <li>
          <a href="#5">5. &hairsp; Results</a>
        </li>

        <li>
          <a href="#6">6. &hairsp; Conclusion</a>
        </li>

        <li>
          <a href="#7">7. &hairsp; Reference</a>
        </li>

      </ul>
    </div>
    <hr>


<!-- 1. Introduction -->    
    <h2 id="1">&hairsp; 1. &hairsp; Introduction</h2>
    <div class="chapter-box">

      <div class="list-container">
        <ul>
          <li>Description</li>
          <p>&nbsp; 기계 번역은 가장 유명한 자연어 생성과제중 하나입니다. 덕분에 단일 언어 번역 과제에서의 성능을 향상시키기 위한 모델 관점에서의, 학습법 관점에서의 다양한 연구들이 있습니다. 하지만, 다언어 번역을 위한 모델링 및 학습 방식에 대한 연구는 상대적으로 부족합니다. 이 프로젝트에서는 다언어 번역 모델에 대한 연구가 부족하다는 문제점을 직접 해결하려 합니다.
          다언어 번역도 크게 세 가지로 나누어볼수 있습니다. 다언어를 이해하고, 하나의 언어로 변환하는 모델. 하나의 언어를 이해하고, 다언어로 표현할 수 있는 모델. 다언어를 이해하고, 다언어로 변환할 수 있는 모델.
          각 모델은 인코더 혹은 디코더의 무게중심이 상이합니다. 첫번째는 인코더의 성능이, 두번째는 디코더의 성능이, 마지막은 인코더와 디코더가 모두 중요하게 작용합니다.
          </p>

          <div class="spacer"></div>
          <li>Objective
            <ul>
              <li style="font-weight: normal;">다언어 번역을 위한 깊은 이해 증진</li>
              <li style="font-weight: normal;">인코더와 디코더의 디자인 밸런스 조정을 통해 목적에 맞는 모델 디자인 적합성 확인</li>
              <li style="font-weight: normal;">다언어 모델링의 가능성을 다양한 방면에서 확인</li>
            </ul>
          </li>

        </ul>
      </div>

    </div>



<!-- 2. Background -->    
    <h2 id="2">&hairsp; 2. &hairsp; Background</h2>
    <div class="chapter-box">

      <div class="list-container">
        <ul>
          <li>Multi Lingual Machine Translation</li>
          <p>&nbsp; 최근 다양한 LLM에서 다언어 번역 기능을 지원하며, 딥러닝 모델을 통한 다양한 번역 모델링의 성공적 가능성을 보여주고 있습니다. 하지만, Foundation 모델 단에서 다언어 기계 번역을 위한 연구는 상대적으로 부족합니다. 이 프로젝트에서는 ...
          </p>

          <div class="spacer"></div>
          <li>Prompt Training</li>
          <p>&nbsp; 학습 방법론에서는 T5에서의 Prompting 방식을 활용해서, 번역을 원하는 언어쌍에 따라 다른 시그널을 모델이 인식할 수 있도록 합니다.
          </p>

        </ul>
      </div>
    </div>



<!-- 3. Architecture -->    
    <h2 id="3">&hairsp; 3.&hairsp; Architecture</h2>
    <div class="chapter-box">      
      <div class="list-container">
        <ul>

          <li>Unbalanced Transformer
            <ul>
              <li>Encoder-Wide, </li>
              <li></li>
              <li></li>
            </ul>
          </li>           


          <div class="spacer"></div>
          <li>Evolved Hybrid Transformer
            <ul>
              <li>이전 Transformer Variants에서 좋은 성능을 보였던, Evolved Hybrid Transformer 모델 구조</li>
              <li></li>
              <li></li>
            </ul>
          </li>           


          <div class="spacer"></div>
          <li>KoBERT Fusion Transformer
            <ul>
              <li>한국어와 더불어 영어에 대한 사전학습이 되어있는 KoBERT라는 사전학습 인코더를 활용한 모델 구조</li>
              <li></li>
              <li></li>
            </ul>
          </li>           

        </ul>
      </div>      
    </div>         


<!-- 4. Experimental Setup -->
    <h2 id="4">&hairsp; 4. &hairsp; Experimental Setup</h2>
    <div class="chapter-box">

      <div class="list-container">
        <ul>
          <li>Data Setup
            <div class="dual-container">
              <ul>
                <li>Dataset: &nbsp; Conala</li>
                <li>Dialogue Task: &nbsp; Daily Dialogue</li>
                <li>Summarization Task: &nbsp; CNN Daily</li>
                <li>Tokenizer: &nbsp; AlBERT Tokenizer</li>
              </ul>
              <ul>
                <li>Train Data Volumn: &nbsp; 50,000</li>
                <li>Valid Data Volumn: &nbsp; 5,000</li>
                <li>Test Data Volumn: &nbsp; 100</li>
                <li>Vocab Size: &nbsp; 10,000</li>                
              </ul>
            </div>
          </li>

          <div class="half-spacer"></div>
          <li>Model Setup
            <div class="dual-container">
              <ul>
                <li>PLE Architecture: &nbsp; AlBERT</li>
                <li>PLE Name: &nbsp; albert-v2</li>
                <li>Input Dim: &nbsp; 10,000</li>
                <li>Output Dim: &nbsp; 10,000</li>
              </ul>
              <ul>
                <li>Embedding Dim: &nbsp; 512</li>
                <li>Hidden Dim: &nbsp; 512</li>
                <li>Model Params: &nbsp; 000</li>
                <li>Model Size: &nbsp; 000</li>                
              </ul>
            </div>
          </li>

          <div class="half-spacer"></div>
          <li>Training Setup
            <div class="dual-container">
              <ul>
                <li>Num Epochs: &nbsp; 10</li>
                <li>Batch Size: &nbsp; 32</li>
                <li>Learning Rate: &nbsp; 5e-4</li>
                <li>LR Scheduler: &nbsp; pleature</li>
              </ul>
              <ul>
                <li>Optimizer: &nbsp; AdamW</li>
                <li>Gradient Accumulation Steps: &nbsp; 4
                <li>Teacher Forcing Ratio: &nbsp; 0.5</li>                
              </ul>
            </div>
          </li>
        
        </ul>
      </div>
    </div>


<!-- 5. Result -->
    <h2 id="5">&hairsp; 5. &hairsp; Result</h2>
    <div class="chapter-box">

      <div class="list-container">
        <ul>
          <li>En-Ko Translation</li>

            <table class="result-table">
              <thead>
                <tr>
                  <th>Model Type</th>
                  <th>Eval Score</th>
                  <th>Epoch Time</th>
                  <th>Avg GPU</th>
                  <th>Max GPU</th>
                </tr>
              </thead>
              <tbody>
                <tr>
                  <td>Standard Transformer</td>
                  <td>2.86</td>
                  <td>0m 41s</td>
                  <td>0.20GB</td>
                  <td>0.85GB</td>
                </tr>
                <tr>
                  <td>Unbalanced Transformer</td>
                  <td>1.81</td>
                  <td>0m 41s</td>
                  <td>0.18GB</td>
                  <td>0.82GB</td>
                </tr>
                <tr>
                  <td>Evolved Hybrid Transformer</td>
                  <td>0.00</td>
                  <td>0m 40s</td>
                  <td>0.19GB</td>
                  <td>0.78GB</td>
                </tr>
                <tr>
                  <td>KoBERT Fusion Transformer</td>
                  <td>0.00</td>
                  <td>0m 40s</td>
                  <td>0.19GB</td>
                  <td>0.78GB</td>
                </tr>

              </tbody>
            </table>
          
          <div class="spacer"></div>
          <li>Ko-En Translation</li>
            <table class="result-table">
              <thead>
                <tr>
                  <th>Model Type</th>
                  <th>Eval Score</th>
                  <th>Epoch Time</th>
                  <th>Avg GPU</th>
                  <th>Max GPU</th>
                </tr>
              </thead>
              <tbody>
                <tr>
                  <td>Standard Transformer</td>
                  <td>2.86</td>
                  <td>0m 41s</td>
                  <td>0.20GB</td>
                  <td>0.85GB</td>
                </tr>
                <tr>
                  <td>Unbalanced Transformer</td>
                  <td>1.81</td>
                  <td>0m 41s</td>
                  <td>0.18GB</td>
                  <td>0.82GB</td>
                </tr>
                <tr>
                  <td>Evolved Hybrid Transformer</td>
                  <td>0.00</td>
                  <td>0m 40s</td>
                  <td>0.19GB</td>
                  <td>0.78GB</td>
                </tr>
                <tr>
                  <td>KoBERT Fusion Transformer</td>
                  <td>0.00</td>
                  <td>0m 40s</td>
                  <td>0.19GB</td>
                  <td>0.78GB</td>
                </tr>

              </tbody>
            </table>
          

          <div class="spacer"></div>
          <li>Result Analysis</li>
            <p> &nbsp; ...
            </p>
        </ul>
      </div>
    </div>


<!-- 6. Conclusion -->
    <h2 id="6">&hairsp; 6. &hairsp; Conclusion</h2>
    <div class="chapter-box">

      <div class="list-container">
        <ul>
          <li>000 모델 구조의 Foundation Model로써의 효용성 확인</li>
            <p>&nbsp; ...
            </p>

          <div class="spacer"></div>
          <li>...</li>
            <p>&nbsp; ...
            </p>

        </ul>
      </div>
    </div>


<!-- Reference -->    
    <h2 id="7">7. &hairsp; Reference</h2>
    <div class="chapter-box">
      <a class="reference" href="https://arxiv.org/abs/1706.03762">&nbsp; Attention Is All You Need</a> <br> 
      <a class="reference" href="https://arxiv.org/abs/1910.10683">&nbsp; Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer</a>
      <a class="reference" href="">&nbsp; Transformer Balance</a><br>
      <a class="reference" href="">&nbsp; Transformer Variants</a><br>
      <a class="reference" href="">&nbsp; Transformer Fusion</a> 
    </div>

  </div>
</div>


<div class="pagination">
  <a href="{{ '/intelligen' | relative_url }}" class="btn-prev"><span>IntelliGEN</span></a>
  <a href="{{ '/multiturn_dialogue' | relative_url }}" class="btn-next"><span>Multi-Turn Dialogue Generation</span></a>
</div>