---
layout: default
permalink: /efficient_summarization
---


<div id="detail">
  <h1 class="title">Efficient&hairsp; Summarization</h1>  
  <div class="post">

    <hr>
    <div class="callout">
      <p class="highlighted-text"><span>Project Objective</span></p>
      <div class="small-spacer"></div>
      <p>&nbsp; 이 프로젝트에서는 긴 시퀀스를 다룸에 있어 비 효율적인 기존 어텐션 연산을 개선한 두 가지 어텐션 기법을 적용한 모델의 성능과 효율성 간의 크로스오버를 면밀히 검토하며. 각 방법론의 효용성을 다각적으로 검증합니다. 
      </p>
    </div>

    <div class="tblContents">
      <ul>

        <li>
          <a href="#1">1. &hairsp; Introduction</a>
        </li>

        <li>
          <a href="#2">2. &hairsp; Background</a>
        </li>

        <li>
          <a href="#3">3. &hairsp; Attention</a>
        </li>

        <li>
          <a href="#4">4. &hairsp; Experimental Setup</a>
        </li>

        <li>
          <a href="#5">5. &hairsp; Result</a>
        </li>

        <li>
          <a href="#6">6. &hairsp; Conclusion</a>
        </li>

        <li>
          <a href="#7">7. &hairsp; Reference</a>
        </li>

      </ul>
    </div>
    <hr>


<!-- 1. Introduction -->    
    <h2 id="1">&hairsp; 1. &hairsp; Introduction</h2>
    <div class="chapter-box">

      <div class="list-container">
        <ul>
          <li>Description</li>
            <p> &nbsp; 자연어 생성을 위한 Sequence to Sequence 모델 구조에서는 Exposure Bias라는 문제점이 빈번하게 발생합니다. 학습과정과 실제 추론과정 사이 괴리에서 기인하는 이 문제를 해결하기 위한 다양한 방법론이 존재하며, 그 방법론 중 하나가 바로 Scheduled Sampling. 하지만 Scheduled Sampling은 학습과정 안에서 생성을 동반하기 때문에 Transformer에 사용하기 적합하지 않습니다. 하지만 이런 문제를 해결하며 새로운 방법론을 제시한 Scheduled Sampling for Transformers라는 논문이 있었습니다. 이 논문에서 아이디어를 얻어, 이 프로젝트에서는 다양한 Sampling Ratio 별 Scheduled Sampling의 효용성을 세가지 자연어 과제에서 검증해봅니다.
            </p>

          <div class="spacer"></div>
          <li>Objective
            <ul>
              <li style="font-weight: normal;">Scheduled Sampling for Transformers 구현</li>
              <li style="font-weight: normal;">Sampling Ratio 별 자연어 생성 능력의 변화 확인</li>
              <li style="font-weight: normal;">Scheduled Sampling의 Exposure Bias 완화 기능 확인</li>
            </ul>
          </li>

        </ul>
      </div>

    </div>


<!-- 2. Background -->    
    <h2 id="2">&hairsp; 2. &hairsp; Background</h2>
    <div class="chapter-box">

      <div class="list-container">
        <ul>
          <li>Complexity of Scaled-Dot Attention
            <p>&nbsp; 일반적인 트랜스포머에서 사용하는 어텐션은 Scaled Dot Attention입니다. 이는 간단한 연산으로 뛰어난 성능을 보이지만, 그 연산 복잡도에 있어서는 O2 만큼의 복잡도가 발생합니다. 때문에 입력시퀀스가 길어지는 과제를 해결함에 있어서는 매우 비효율적인 모습이 보입니다.
            </p>
          </li>

          <div class="spacer"></div>
          <li>Efficient Attention
            <p>&nbsp; Scheduled sampling은 sequence-to-sequence 모델에서 사용되는 훈련 기술 중 하나입니다. 주로 기계 번역 및 자연어 처리 작업에서 적용됩니다.
            </p>
          </li>

        <ul>          
      </div>
    </div>


<!-- 3. Architecture -->    
    <h2 id="3">&hairsp; 3. &hairsp; Architecture</h2>
    <div class="chapter-box">

      <div class="list-container">
        <ul>
          
          <li>Standard Transformer
            <ul>
              <li style="font-weight: normal;">Attention is all you need에서 소개된, 가장 표준적인 트랜스포머 구조</li>
              <li style="font-weight: normal;">Scaled Dot Product Attention을, 모든 모델 구조에서 사용</li>
              <li style="font-weight: normal;">실험의 BaseLine Model</li>
            </ul>
          </li>


          <div class="spacer"></div>
          <li>Half-Efficient Transformer
            <ul>
              <li style="font-weight: normal;">트랜스포머의 인코더에서만, Efficient Attention을 사용</li>
              <li style="font-weight: normal;">디코더에서는 Scaled Dot Product Attention을 사용</li>
              <li style="font-weight: normal;">인코딩과정에서는 효율성을, 디코딩과정에서는 정확성에 집중하기 위해 효율성과 정확성을 모두 가져가기 위한 모델 구조</li>
            </ul>
          </li>

          <div class="spacer"></div>
          <li>Efficient Transformer
            <ul>
              <li style="font-weight: normal;">트랜스포머의 모든 어텐션 연산에 Efficient Attention을 사용한 모델</li>
              <li style="font-weight: normal;">모델의 모든 어텐션 연산에서 효율성을 제고시키는 것이, 전체 성능에 어떤 변화를 가져올지 확인하기 위한 모델 구조</li>
            </ul>
          </li>


        </ul>          
      </div>
    </div>


<!-- 4. Experimental Setup -->
    <h2 id="4">&hairsp; 4. &hairsp; Experimental Setup</h2>
    <div class="chapter-box">

      <div class="list-container">
        <ul>
          <li>Data Setup
            <div class="dual-container">
              <ul>
                <li>Dataset: &nbsp; CNN Daily Mail</li>
                <li>Tokenizer: &nbsp; BPE Tokenizer</li>
                <li>Vocab Size: &nbsp; 15,000</li>
              </ul>
              <ul>
                <li>Train Data Volumn: &nbsp; 50,000</li>
                <li>Valid Data Volumn: &nbsp; 5,000</li>
                <li>Test Data Volumn: &nbsp; 100</li>
                                
              </ul>
            </div>
          </li>

          <div class="half-spacer"></div>
          <li>Model Setup
            <div class="dual-container">
              <ul>
                <li>Architecture: &nbsp; Transformer</li>
                <li>Input Dim: &nbsp; 15,000</li>
                <li>Output Dim: &nbsp; 15,000</li>
              </ul>
              <ul>
                <li>Embedding Dim: &nbsp; 512</li>
                <li>Hidden Dim: &nbsp; 512</li>
                <li>Model Params: &nbsp; 15,488,664</li>
                <li>Model Size: &nbsp; 60.085 MB</li>                
              </ul>
            </div>
          </li>

          <div class="half-spacer"></div>
          <li>Training Setup
            <div class="dual-container">
              <ul>
                <li>Num Epochs: &nbsp; 10</li>
                <li>Batch Size: &nbsp; 32</li>
                <li>Learning Rate: &nbsp; 5e-4</li>
                <li>LR Scheduler: &nbsp; ReduceLROnPlateau</li>
              </ul>
              <ul>
                <li>Optimizer: &nbsp; AdamW</li>
                <li>Apply Gradient Checkpoint: &nbsp; True</li>
                <li>Gradient Accumulation Steps: &nbsp; 4</li>
                <li>Early Stop Patient: &nbsp; 3</li>
              </ul>
            </div>
          </li>
        </ul>
      </div>
    </div>


<!-- 5. Result -->
    <h2 id="5">&hairsp; 5. &hairsp; Result</h2>
    <div class="chapter-box">

      <div class="list-container">
        <ul>
          <li>Text Summarization</li>

            <table class="result-table">
              <thead>
                <tr>
                  <th>Model</th>
                  <th>Score</th>
                  <th>Epoch Time</th>
                  <th>Avg GPU</th>
                  <th>Max GPU</th>
                </tr>
              </thead>
              <tbody>
                <tr>
                  <td>Standard Transformer</td>
                  <td>12.99</td>
                  <td>0m 44s</td>
                  <td>0.20GB</td>
                  <td>0.95GB</td>
                </tr>

                <tr>
                  <td>Half-Efficient Transformer</td>
                  <td>2.11</td>
                  <td>1m 00s</td>
                  <td>0.20GB</td>
                  <td>0.95GB</td>
                </tr>

                <tr>
                  <td>Efficient Transformer</td>
                  <td>2.11</td>
                  <td>1m 00s</td>
                  <td>0.20GB</td>
                  <td>0.95GB</td>
                </tr>

              </tbody>
            </table>



          <div class="half-spacer"></div>  
          <li>Result Analysis</li>
            <p> &nbsp; ...
            </p>
        </ul>
      </div>
    </div>


<!-- 6. Conclusion -->
    <h2 id="6">&hairsp; 6. &hairsp; Conclusion</h2>
    <div class="chapter-box">

      <p>&nbsp; 이 프로젝트에서는 ...
      </p>

    </div>


<!-- 7. Reference -->
    <h2 id="7">&hairsp; 7. &hairsp; Reference</h2>
    <div class="chapter-box">
      <a class="reference" href="https://arxiv.org/abs/1706.03762">&nbsp; Attention Is All You Need</a> <br> 
      <a class="reference" href="https://arxiv.org/abs/1812.01243">&nbsp; Efficient Attention: Attention with Linear Complexities</a> 

    </div>

  </div>
</div>


<div class="pagination">
  <a href="{{ '/multiturn_dialogue' | relative_url }}" class="btn-prev"><span>Multi-Turn Dialogue Generation</span></a>
  <a href="{{ '/context_framework' | relative_url }}" class="btn-next"><span>Context-Aware Translation Framework</span></a>
</div>