---
layout: default
permalink: /efficient_summarization
---


<div id="detail">
  <h1 class="title">Efficient&hairsp; Summarization</h1>  
  <div class="post">

    <hr>
    <div class="callout">
      <p class="highlighted-text"><span>Project Objective</span></p>
      <div class="small-spacer"></div>
      <p>&nbsp; 이 프로젝트에서는 긴 시퀀스를 다룸에 있어 비 효율적인 기존 어텐션 연산을 개선한 두 가지 어텐션 기법을 적용한 모델의 성능과 효율성 간의 크로스오버를 면밀히 검토하며. 각 방법론의 효용성을 다각적으로 검증합니다. 
      </p>
    </div>

    <div class="tblContents">
      <ul>

        <li>
          <a href="#1">1. &hairsp; Introduction</a>
        </li>

        <li>
          <a href="#2">2. &hairsp; Background</a>
        </li>

        <li>
          <a href="#3">3. &hairsp; Attention</a>
        </li>

        <li>
          <a href="#4">4. &hairsp; Experimental Setup</a>
        </li>

        <li>
          <a href="#5">5. &hairsp; Result</a>
        </li>

        <li>
          <a href="#6">6. &hairsp; Conclusion</a>
        </li>

        <li>
          <a href="#7">7. &hairsp; Reference</a>
        </li>

      </ul>
    </div>
    <hr>


<!-- 1. Introduction -->    
    <h2 id="1">&hairsp; 1. &hairsp; Introduction</h2>
    <div class="chapter-box">

      <div class="list-container">
        <ul>
          <li>Description</li>
            <p> &nbsp; 자연어 생성을 위한 Sequence to Sequence 모델 구조에서는 Exposure Bias라는 문제점이 빈번하게 발생합니다. 학습과정과 실제 추론과정 사이 괴리에서 기인하는 이 문제를 해결하기 위한 다양한 방법론이 존재하며, 그 방법론 중 하나가 바로 Scheduled Sampling. 하지만 Scheduled Sampling은 학습과정 안에서 생성을 동반하기 때문에 Transformer에 사용하기 적합하지 않습니다. 하지만 이런 문제를 해결하며 새로운 방법론을 제시한 Scheduled Sampling for Transformers라는 논문이 있었습니다. 이 논문에서 아이디어를 얻어, 이 프로젝트에서는 다양한 Sampling Ratio 별 Scheduled Sampling의 효용성을 세가지 자연어 과제에서 검증해봅니다.
            </p>

          <div class="spacer"></div>
          <li>Objective
            <ul>
              <li style="font-weight: normal;">Scheduled Sampling for Transformers 구현</li>
              <li style="font-weight: normal;">Sampling Ratio 별 자연어 생성 능력의 변화 확인</li>
              <li style="font-weight: normal;">Scheduled Sampling의 Exposure Bias 완화 기능 확인</li>
            </ul>
          </li>

        </ul>
      </div>

    </div>


<!-- 2. Background -->    
    <h2 id="2">&hairsp; 2. &hairsp; Background</h2>
    <div class="chapter-box">

      <div class="list-container">
        <ul>
          <li>Scaled Dot Attention
            <p>&nbsp; 일반적인 트랜스포머에서 사용하는 어텐션은 Scaled Dot Attention입니다. 이는 간단한 연산으로 뛰어난 성능을 보이지만, 그 연산 복잡도에 있어서는 O2 만큼의 복잡도가 발생합니다. 때문에 입력시퀀스가 길어지는 과제를 해결함에 있어서는 매우 비효율적인 모습이 보입니다.
            </p>
          </li>

          <div class="spacer"></div>
          <li>Efficient Attention
            <p>&nbsp; Scheduled sampling은 sequence-to-sequence 모델에서 사용되는 훈련 기술 중 하나입니다. 주로 기계 번역 및 자연어 처리 작업에서 적용됩니다.
            </p>
          </li>

        <ul>          
      </div>
    </div>


<!-- 3. Attention -->    
    <h2 id="3">&hairsp; 3. &hairsp; Attention</h2>
    <div class="chapter-box">

      <div class="list-container">
        <ul>
          
          <li>Concept
            <img src="{{ 'assets/img/research_02/scheduled_sampling.png' | relative_url }}" style="width: 65%; height: 50%; display: block; margin: 0 auto;">
            <div class="small-spacer"></div>
            <p>&nbsp; Scheduled Sampling을 Transformer에서 구현하기 위해, 위 그림과 같이 디코딩 과정을 두번 진행합니다. 원래의 Scheduling Sampling은 Auto Regressive하게 시퀀스를 생성하고, 매 Time Step마다 Scheduling Sampling을 사용할지 말지 선택합니다. 하지만 Transformer 모델의 특성 상, 생성 과정에 효율성이 매우 하락합니다. 효율성 하락이라는 이슈를 해결하고, 가장 Transformer 적인 연산안에서 Scheduled Sampling을 사용하기 위해, 위와 같은 방식을 사용합니다. <br>

            학습 시 디코딩 과정을 자세히 살펴보자면, 우선 첫번째 디코딩에서는 일반적인 Transformer의 방식과 동일하게 Self Attention과 Encoder-Decoder Attention을 거쳐 Logit 값을 반환합니다. 이때 반환된 Logit값은 Token으로 반환되어 Scheduled Sampling이 진행됩니다. 이후 Sampled Input을 다시금 디코더의 입력으로 주어 두번째 디코딩이 진행됩니다. 
            </p>
          </li>


          <div class="spacer"></div>
          <li>Code Implementation
            <p>&nbsp; ...
            </p>
          </li>

        </ul>          
      </div>
    </div>


<!-- 4. Experimental Setup -->
    <h2 id="4">&hairsp; 4. &hairsp; Experimental Setup</h2>
    <div class="chapter-box">

      <div class="list-container">
        <ul>
          <li>Data Setup
            <div class="dual-container">
              <ul>
                <li>Translation Task: &nbsp; WMT'14 En-De</li>
                <li>Dialogue Task: &nbsp; Daily Dialogue</li>
                <li>Summarization Task: &nbsp; CNN Daily Mail</li>
                <li>Tokenizer: &nbsp; BPE Tokenizer</li>
              </ul>
              <ul>
                <li>Vocab Size: &nbsp; 15,000</li>
                <li>Train Data Volumn: &nbsp; 50,000</li>
                <li>Valid Data Volumn: &nbsp; 5,000</li>
                <li>Test Data Volumn: &nbsp; 100</li>
                                
              </ul>
            </div>
          </li>

          <div class="half-spacer"></div>
          <li>Model Setup
            <div class="dual-container">
              <ul>
                <li>Architecture: &nbsp; Transformer</li>
                <li>Input Dim: &nbsp; 15,000</li>
                <li>Output Dim: &nbsp; 15,000</li>
              </ul>
              <ul>
                <li>Embedding Dim: &nbsp; 512</li>
                <li>Hidden Dim: &nbsp; 512</li>
                <li>Model Params: &nbsp; 15,488,664</li>
                <li>Model Size: &nbsp; 60.085 MB</li>                
              </ul>
            </div>
          </li>

          <div class="half-spacer"></div>
          <li>Training Setup
            <div class="dual-container">
              <ul>
                <li>Num Epochs: &nbsp; 10</li>
                <li>Batch Size: &nbsp; 32</li>
                <li>Learning Rate: &nbsp; 5e-4</li>
                <li>LR Scheduler: &nbsp; ReduceLROnPlateau</li>
              </ul>
              <ul>
                <li>Optimizer: &nbsp; AdamW</li>
                <li>Apply Gradient Checkpoint: &nbsp; True</li>
                <li>Gradient Accumulation Steps: &nbsp; 4</li>
                <li>Early Stop Patient: &nbsp; 3</li>
              </ul>
            </div>
          </li>
        </ul>
      </div>
    </div>


<!-- 5. Result -->
    <h2 id="5">&hairsp; 5. &hairsp; Result</h2>
    <div class="chapter-box">

      <div class="list-container">
        <ul>
          <li>Machine Translation</li>

            <table class="result-table">
              <thead>
                <tr>
                  <th>Sampling Ratio</th>
                  <th>Score</th>
                  <th>Epoch Time</th>
                  <th>Avg GPU</th>
                  <th>Max GPU</th>
                </tr>
              </thead>
              <tbody>
                <tr>
                  <td>0.0</td>
                  <td>12.99</td>
                  <td>0m 44s</td>
                  <td>0.20GB</td>
                  <td>0.95GB</td>
                </tr>
                <tr>
                  <td>0.1</td>
                  <td>2.11</td>
                  <td>1m 00s</td>
                  <td>0.20GB</td>
                  <td>0.95GB</td>
                </tr>
                <tr>
                  <td>0.3</td>
                  <td>7.34</td>
                  <td>1m 00s</td>
                  <td>0.20GB</td>
                  <td>0.95GB</td>
                </tr>
                <tr>
                  <td>0.5</td>
                  <td>6.94</td>
                  <td>1m 00s</td>
                  <td>0.20GB</td>
                  <td>0.95GB</td>
                </tr>

              </tbody>
            </table>
          
          <div class="half-spacer"></div>
          <li>Dialogue Generation</li>
            <table class="result-table">
              <thead>
                <tr>
                  <th>Sampling Ratio</th>
                  <th>Score</th>
                  <th>Epoch Time</th>
                  <th>Avg GPU</th>
                  <th>Max GPU</th>
                </tr>
              </thead>
              <tbody>
                <tr>
                  <td>0.0</td>
                  <td>2.03</td>
                  <td>0m 43s</td>
                  <td>0.20GB</td>
                  <td>0.85GB</td>
                </tr>
                <tr>
                  <td>0.1</td>
                  <td>2.31</td>
                  <td>0m 59s</td>
                  <td>0.20GB</td>
                  <td>0.85GB</td>
                </tr>
                <tr>
                  <td>0.3</td>
                  <td>2.34</td>
                  <td>0m 59s</td>
                  <td>0.20GB</td>
                  <td>0.85GB</td>
                </tr>
                <tr>
                  <td>0.5</td>
                  <td>0.82</td>
                  <td>0m 59s</td>
                  <td>0.20GB</td>
                  <td>0.85GB</td>
                </tr>

              </tbody>
            </table>


          <div class="half-spacer"></div>
          <li>Text Summarization</li>
            <table class="result-table">
              <thead>
                <tr>
                  <th>Sampling Ratio</th>
                  <th>Score</th>
                  <th>Epoch Time</th>
                  <th>Avg GPU</th>
                  <th>Max GPU</th>
                </tr>
              </thead>
              <tbody>
                <tr>
                  <td>0.0</td>
                  <td>7.60</td>
                  <td>2m 44s</td>
                  <td>0.21GB</td>
                  <td>2.78GB</td>
                </tr>
                <tr>
                  <td>0.1</td>
                  <td>2.95</td>
                  <td>3m 8s</td>
                  <td>0.21GB</td>
                  <td>2.78GB</td>
                </tr>
                <tr>
                  <td>0.3</td>
                  <td>4.10</td>
                  <td>3m 8s</td>
                  <td>0.21GB</td>
                  <td>2.78GB</td>
                </tr>
                <tr>
                  <td>0.5</td>
                  <td>5.39</td>
                  <td>3m 8s</td>
                  <td>0.21GB</td>
                  <td>2.78GB</td>
                </tr>

              </tbody>
            </table>


          <div class="half-spacer"></div>  
          <li>Result Analysis</li>
            <p> &nbsp; ...
            </p>
        </ul>
      </div>
    </div>


<!-- 6. Conclusion -->
    <h2 id="6">&hairsp; 6. &hairsp; Conclusion</h2>
    <div class="chapter-box">

      <p>&nbsp; 이 프로젝트에서는 ...
      </p>

    </div>


<!-- 7. Reference -->
    <h2 id="7">&hairsp; 7. &hairsp; Reference</h2>
    <div class="chapter-box">
      <a class="reference" href="https://arxiv.org/abs/1706.03762">&nbsp; Attention Is All You Need</a> <br> 
      <a class="reference" href="https://arxiv.org/abs/1906.07651">&nbsp; Scheduled Sampling for Transformers</a> 

    </div>

  </div>
</div>


<div class="pagination">
  <a href="{{ '/multiturn_dialogue' | relative_url }}" class="btn-prev"><span>Multi-Turn Dialogue Generation</span></a>
  <a href="{{ '/context_framework' | relative_url }}" class="btn-next"><span>Context-Aware Translation Framework</span></a>
</div>