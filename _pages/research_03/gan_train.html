---
layout: default
permalink: /gan_train
---


<div id="detail">
  <h3 class="title">SeqGAN을 통한 자연어 생성능력 향상 여부 검증</h3>

  <div class="tblContents">
    <ul>
      <li>
        <a href="#1">1. Introduction</a>
      </li>

      <li>
        <a href="#2">2. Background</a>
        <ul>
          <li>
            <a href="#2.1">2.1 GAN</a>
          </li>
          <li>
            <a href="#2.2">2.2 Sequence GAN</a>
          </li>
        </ul>
      </li>
      
      <li>
        <a href="#3">3. Training</a>
        <ul>
          <li>
            <a href="#3.1">3.1 Pre Training</a>
          </li>
          <li>
            <a href="#3.2">3.2 SeqGAN Training</a>
          </li>
        </ul>
      </li>

      <li>
        <a href="#4">4. Experimental Setup</a>
        <ul>
          <li>
            <a href="#4.1">4.1 Data Setups</a>
          </li>
          <li>
            <a href="#4.2">4.2 Model Setups</a>
          </li>
          <li>
            <a href="#4.3">4.3 Training Setups</a>
          </li>
          <li>
            <a href="#4.4">4.4 Model desc</a>
          </li>
        </ul>
      </li>

      <li>
        <a href="#5">5. Results</a>
      </li>

      <li>
        <a href="#6">6. Conclusion</a>
      </li>

      <li>
        <a href="#7">7. Reference</a>
      </li>

    </ul>
  </div>


  
  <div class="post">
    <h2 id="1">1. Introduction</h2>
      <p>
        &nbsp; MLE 방식의 학습 방식의 한계점

      </p>


    <h2 id="2">2. Background</h2>

      <h3 id="2.1">2.1 GAN</h3>
        <p>
          &nbsp; Transformer의 도래 이전, 시퀀스 데이터의 처리에는 RNN 기반의 모델들이 지배적 위치를 점하고 있었습니다. RNN은 순서 정보의 의존성을 잘 포착할 수 있다는 장점이 있지만, 순차적인 연산과정에서 발생하는 병렬화의 어려움과 장거리 데이터간의 의존성 처리의 어려움이라는 단점이 존재했습니다. Transformer는 이러한 한계를 극복하기 위해 self-attention 메커니즘을 통해 모든 입력 토큰 간의 관계를 동시에 계산하는 방법을 선택했으며, 덕분에 병렬 처리가 가능해져 RNN보다 훨씬 빠른 학습과 추론 속도를 제공했습다. 뿐만 아니라 Transformer는 self-attention 메커니즘을 통해 임의 위치 간 의존성을 효과적으로 포착하며 장기 의존성을 보다 잘 처리했습니다. 결과적으로 Transformer는 문장 전체의 의미를 파악하거나, 긴 문맥을 이해하는 등 어려운 환경에서도 우수한 성능을 보이며, 성공적으로 RNN 구조를 대체하게 되었습니다.
        </p>

      <h3 id="2.2" class="h3-mt">2.2 SeqGAN</h3>
        <p>
          &nbsp; Inductive Bias는 기계 학습 모델이 주어진 학습 데이터로부터 일반화하는 데에 어떤 가정이나 제한사항을 가지고 있는 것을 말합니다. 이러한 가정이나 제한사항은 모델이 어떤 가설 공간을 탐색하고 어떤 가설을 선호하는지에 영향을 미칩니다.nductive Bias는 모델이 일반화 능력과 학습 데이터에 대한 의존성을 조절하는 역할을 합니다. 적절한 nductive Bias는 모델이 주어진 문제에 적합하고 일반화하기 쉽도록 도와줍니다.
        </p>
        <p>
          하지만 Transformer의 경우, 복잡한 모델 구조 때문에 Inductive Bias가 부족하기 쉽습니다. 때문에 간단한 과제 해결과정에서 RNN보다 못한 성능을 보이는 경우도 종종 발생합니다. Universal Transformer 논문에서는 Transformer 레이어 연결과정에서 RNN의 핵심개념이었던 재귀적인 연결 방식을 차용함으로써 Inductive Bias의 향상을 도모할 수 있다고 주장합니다.
        </p>


    <h2 id="3">3. Training</h2>
      <h3>3.1 Pre Training</h3>
        <div class="img-container">
          <ul>
            <li>Encoder Decoder Architecture
              <ul>
                <li>Encoder</li>
                <li>Decoder</li>
              </ul>
            </li>

            <li>Embeddings
              <ul>
                <li>Token Embedding</li>
                <li>Position Embedding</li>
              </ul>
            </li>

            <li>Multi-Head Attention
              <ul>
                <li>다수의 어텐션 헤드를 통해 입력 시퀀스의 의미를 다각도로 포착</li>
                <li>어텐션은 주어진 쿼리, 키, 밸류 쌍 간의 관련성을 계산해 가중 평균을 산출</li>
              </ul>
            </li>
          
          </ul>
        </div>

      <h3 class="h3-mt">3.2 Standard Transfomrer</h3>
        <div class="img-container">
          <img src="{{ 'assets/img/project_01/standard_transformer.png' | relative_url }}">
          <ul>
            <li>Standard Transformer
              <ul>
                <li>Encoder</li>
                <li>Decoder</li>
              </ul>
            </li>

            <li>Encoder Layer
              <ul>
                <li>Sub-Layer 1</li>
                <li>Sub-Layer 2</li>
              </ul>
            </li>

            <li>Decoder Layer
              <ul>
                <li>Sub-Layer 1</li>
                <li>Sub-Layer 2</li>
                <li>Sub-Layer 3</li>
              </ul>
            </li>

          </ul>
        </div>

      <h3 id="2" class="h3-mt">3.3 Recurrent Transformer</h3>
        <div class="img-container">
          <img src="{{ 'assets/img/project_01/recurrent_transformer.png' | relative_url }}">
          <ul>
            <li>Recurrent Transformer
              <ul>
                <li>Encoder</li>
                <li>Decoder</li>
              </ul>
            </li>

            <li>Encoder Layer
              <ul>
                <li>Sub-Layer 1</li>
                <li>Sub-Layer 2</li>
                <li>Sub-Layer 3</li>
              </ul>
            </li>

            <li>Decoder Layer
              <ul>
                <li>Sub-Layer 1</li>
                <li>Sub-Layer 2</li>
                <li>Sub-Layer 3</li>
                <li>Sub-Layer 4</li>
              </ul>
            </li>

          </ul>
        </div>

      <h3 id="3" class="h3-mt">3.4 Evolved Transformer</h3>
        <div class="img-container">
          <img src="{{ 'assets/img/project_01/evolved_transformer.png' | relative_url }}">
          <ul>
            <li>Evolved Transformer
              <ul>
                <li>Encoder</li>
                <li>Decoder</li>
              </ul>
            </li>

            <li>Cell
              <ul>
                <li>Sub-Layer 1</li>
                <li>Standard Transformer에서 2개 레이어에 걸친 연산 과정을 고도화 시킨 SubLayer를 Evolved Transformer에서는 Cell이라고 표현</li>
              </ul>
            </li>

            <li>Encoder Cell
              <ul>
                <li>Block 1</li>
                <li>Block 2</li>
                <li>Block 3</li>
                <li>Block 4</li>
                <li>Block 5 & 6</li>
              </ul>
            </li>

            <li>Decoder Cell
              <ul>
                <li>Block 1</li>
                <li>Block 2</li>
                <li>Block 3</li>
                <li>Block 4</li>
                <li>Block 5</li>
                <li>Block 6 & 7</li>
              </ul>
            </li>

          </ul>
        </div>

    <h2 id="4" class="h3-mt">4. Experimental Setup</h2>
      <h3 id="4.1">4.1 Data Setups</h3>

      <h3 id="4.2" class="h3-mt">4.2 Model Setups</h3>
      
      <h3 id="4.3" class="h3-mt">4.3 Training Setups</h3>
      
      <h3 id="4.4" class="h3-mt">4.4 Model desc</h3>


    <h2 id="5" class="h3-mt">5. Result</h2>


    <h2 id="6" class="h3-mt">6. Conclusion</h2>


    <h2 id="7" class="h3-mt">7. Reference</h2>


  </div>
</div>

<div class="pagination">
  <a href="{{ '/project_03' | relative_url }}" class="btn-prev"></a>
  <a href="{{ '/project_05' | relative_url }}" class="btn-next"></a>
</div>