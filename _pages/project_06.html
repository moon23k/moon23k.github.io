---
layout: default
permalink: /project_06
---


<div id="detail">
  <h3 class="title">데이터 부족 해결을 위한 Back Translation 조건 별 성능 비교</h3>

  <div class="tblContents">
    <ul>
      <li>
        <a href="#1">1. Introduction</a>
      </li>

      <li>
        <a href="#2">2. Background</a>
        <ul>
          <li>
            <a href="#2.1">2.1 Neural Machine Translation</a>
          </li>
          <li>
            <a href="#2.2">2.2 Back Translation</a>
          </li>
          <li>
            <a href="#2.3">2.3 Generative Strategy</a>
          </li>
          <li>
            <a href="#2.4">2.4 Data Noise</a>
          </li>          
        </ul>
      </li>
      
      <li>
        <a href="#3">3. Strategies</a>
        <ul>
          <li>
            <a href="#3.1">3.1 Data Volumn</a>
          </li>
          <li>
            <a href="#3.2">3.2 Generative Thing</a>
          </li>
          <li>
            <a href="#3.2">3.3 Noise</a>
          </li>
        </ul>
      </li>

      <li>
        <a href="#4">4. Experimental Setup</a>
        <ul>
          <li>
            <a href="#4.1">4.1 Data Setups</a>
          </li>
          <li>
            <a href="#4.2">4.2 Model Setups</a>
          </li>
          <li>
            <a href="#4.3">4.3 Training Setups</a>
          </li>
          <li>
            <a href="#4.4">4.4 Model desc</a>
          </li>
        </ul>
      </li>

      <li>
        <a href="#5">5. Results</a>
      </li>

      <li>
        <a href="#6">6. Conclusion</a>
      </li>

      <li>
        <a href="#7">7. Reference</a>
      </li>

    </ul>
  </div>


  
  <div class="post">
    <h2 id="1">1. Introduction</h2>
      <p>
        &nbsp; 성공적인 딥러닝 모델의 학습을 위해서는 양질의 대량 데이터가 필수적입니다. 하지만 원하는 도메인의 데이터를 필요한 만큼 확보하기란 매우 어렵습니다. 특히 기계 번역과 같이 Source-Target Language Pair 형식의 데이터가 필요한 경우에는 더욱 그러합니다. 쌍으로 된 데이터에 비해 단일 언어로만 구성된 데이터는 상대적으로 구하기 용이합니다. 이런 단일 언어 데이터를 활용해 학습 데이터를 증강시키는 방식이 Back Translation이라고 합니다. Back Translation을 통한 데이터 증강이 모델 학습에서 긍정적인 효과를 줄 수 있음은 다양한 연구를 통해 증명되었습니다. 특히나 ~~라는 논문에서는 디테일한 연구로 Back Translation의 효용가치를 증명했습니다. 하지만, 이러한 연구는 영어라는 사용자가 많은 언어에 기반하고 있으며, 상대적으로 사용자 및 데이터가 적은 한국어에서 Back Translation에 관한 연구는 체계적으로 이루어지지 않았습니다. ~~논문에서 처럼 대규모 데이터에 관한 연구는 안되겠지만, 가능한 선에서 한국어 번역 데이터의 부족을 해결하기 위한 일련의 실험은 진행합니다.   
      </p>
      <p>
        실험에서는 Translation 모델과, Back Translation모델을 별개로 두고, 최종 목적은 Translation 모델의 성공적인 학습으로 설정하고, 이를 돕기 위한 다양한 방법론을 사용해봅니다.
        교착어라는 한국어의 언어적인 특성에 맞추어 일련의 실험 세부설정에 변화를 추가합니다.

        Back Translation의 주요 변인은 다음과 같습니다.

        1. 생성 데이터와 실제 데이터의 비율에서의 비교
        2. 데이터를 생성하는 과정에서의 디테일 비교
        3. 데이터에 의도적인 노이즈를 추가하는 방식에 따른 비교
      </p>

    <h2 id="2">2. Background</h2>

      <h3 id="2.1">2.1 Neural Machine Translation</h3>
        <p>
          &nbsp; Neural Machine Translation, 딥러닝 모델을 통한 기계번역은 자연어 처리의 대표적인 과제 중 하나입니다. 기계번역은 원천언어 시퀀스를 입력값으로 받아, 동일한 의미를 갖는 목표언어 시퀀스로의 변환을 목표로 합니다. 모델의 학습을 위해서는   
        </p>

      <h3 id="2.2" class="h3-mt">2.2 Back Translation</h3>
        <p>
          &nbsp; Inductive Bias는 기계 학습 모델이 주어진 학습 데이터로부터 일반화하는 데에 어떤 가정이나 제한사항을 가지고 있는 것을 말합니다. 이러한 가정이나 제한사항은 모델이 어떤 가설 공간을 탐색하고 어떤 가설을 선호하는지에 영향을 미칩니다.nductive Bias는 모델이 일반화 능력과 학습 데이터에 대한 의존성을 조절하는 역할을 합니다. 적절한 nductive Bias는 모델이 주어진 문제에 적합하고 일반화하기 쉽도록 도와줍니다.
        </p>
        <p>
          하지만 Transformer의 경우, 복잡한 모델 구조 때문에 Inductive Bias가 부족하기 쉽습니다. 때문에 간단한 과제 해결과정에서 RNN보다 못한 성능을 보이는 경우도 종종 발생합니다. Universal Transformer 논문에서는 Transformer 레이어 연결과정에서 RNN의 핵심개념이었던 재귀적인 연결 방식을 차용함으로써 Inductive Bias의 향상을 도모할 수 있다고 주장합니다.
        </p>

      <h3 id="2.3" class="h3-mt">2.3 Generative Strategy</h3>
        <p>
          &nbsp; 시퀀스 생성을 위한 Decoding Strategy가 다양하게 존재합니다. 
          <br>
          <strong>Greedy</strong>
          <br>
          <strong>Beam</strong>
          <br>
          <strong>Sampling</strong>
          <br>
          <strong>Sample P</strong>
          <br>

        </p>

      <h3 id="2.3" class="h3-mt">2.4 Data Noise</h3>
        <p>
          &nbsp; 데이터에 의도적인 노이즈를 추가하기 위한 전략을 서술
          MLM
        </p>

    <h2 id="3">3. Strategy</h2>
      <h3>3.1 Data Volumn Ratio</h3>
        <div class="img-container">
          <ul>
            <li>Standard Transformer
              <ul>
                <li>Encoder</li>
                <li>Decoder</li>
              </ul>
            </li>

            <li>Encoder Layer
              <ul>
                <li>Sub-Layer 1</li>
                <li>Sub-Layer 2</li>
              </ul>
            </li>

            <li>Decoder Layer
              <ul>
                <li>Sub-Layer 1</li>
                <li>Sub-Layer 2</li>
                <li>Sub-Layer 3</li>
              </ul>
            </li>

          </ul>
        </div>

      <h3 class="h3-mt">3.2 Standard Transfomrer</h3>
        <p>
          
        </p>

      <h3 id="3.2" class="h3-mt">3.3 Recurrent Transformer</h3>
        <p>
          
        </p>

      <h3 id="3.3" class="h3-mt">3.4 Evolved Transformer</h3>
        <p>
          
        </p>

    <h2 id="4" class="h3-mt">4. Experimental Setup</h2>
    
      <h3 id="4.1">4.1 Data Setups</h3>

      <h3 id="4.2" class="h3-mt">4.2 Model Setups</h3>
      
      <h3 id="4.3" class="h3-mt">4.3 Training Setups</h3>
      
      <h3 id="4.4" class="h3-mt">4.4 Model desc</h3>


    <h2 id="5" class="h3-mt">5. Result</h2>


    <h2 id="6" class="h3-mt">6. Conclusion</h2>


    <h2 id="7" class="h3-mt">7. Reference</h2>


  </div>
</div>

<div class="pagination">
  <a href="{{ '/project_05' | relative_url }}" class="btn-prev"></a>
  <a href="{{ '/project_01' | relative_url }}" class="btn-next"></a>
</div>