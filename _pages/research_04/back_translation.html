---
layout: default
permalink: /back_translation
---


<div id="detail">
  <h1 class="title">Back Translation Validation Study</h1>  
  <div class="post">

    <hr>
    <div class="callout">
      <p class="highlighted-text"><span>Project Objective</span></p>
      <div class="small-spacer"></div>
      <p>&nbsp; 이 프로젝트에서는 Back Translation의 효용성을 다른 관점에서 검토해봅니다. 일반적으로 Back Translation은 이미 학습 데이터가 충분한 경우에, 추가적인 성능 개선을 위해 사용하는 기법입니다. 하지만, 일반적으로 원하는 데이터를 충분하게 확보하기란 어렵습니다. 때문에 이 프로젝트에서는 데이터가 부족한 상황에서 Back Translation기법이 얼마나 효용성이 있을지 실험을 통해 직접 확인해봅니다. Translation은 Ko-En Task를 선정했고, 데이터는 5만개.
      Back Translation을 위한 사전학습 모델로는 Kobart를 선정했습니다.
      </p>
    </div>

    <div class="tblContents">
      <ul>

        <li>
          <a href="#1">1. &hairsp; Introduction</a>
        </li>

        <li>
          <a href="#2">2. &hairsp; Background</a>
        </li>

        <li>
          <a href="#3">3. &hairsp; Experimental Setup</a>
        </li>

        <li>
          <a href="#4">4. &hairsp; Results</a>
        </li>

        <li>
          <a href="#5">5. &hairsp; Conclusion</a>
        </li>

        <li>
          <a href="#6">6. &hairsp; Reference</a>
        </li>

      </ul>
    </div>
    <hr>


<!-- 1. Introduction -->    
    <h2 id="1">&hairsp; 1. &hairsp; Introduction</h2>
    <div class="chapter-box">

      <div class="list-container">
        <ul>
          <li>Description</li>
            <p> &nbsp; Back Translation   
            </p>

          <div class="half-spacer"></div>
          <li>Objective
            <ul>
              <li style="font-weight: normal;">사전학습 인코더 모델을 생성과제에서 사용하기 위한 지식 습득</li>
              <li style="font-weight: normal;">세 가지 방법론의 성능 비교 검증</li>
              <li style="font-weight: normal;">실험 결과를 통해, 이후 자연어 생성을 위한 모델링 다양성 확보</li>
            </ul>
          </li>

        </ul>
      </div>

    </div>



<!-- 2. Background -->    
    <h2 id="2">&hairsp; 2. &hairsp; Background</h2>
    <div class="chapter-box">

      <div class="list-container">
        <ul>
          <li>Ko En Translation</li>
          <p>&nbsp; 프로젝트의 실험에서는 한국어-영어 번역과제를 선정했습니다. 이를 위한 데이터 셋으로는 Ai Hub의 공개데이터를 사용했습니다. 총 10개의 세부 항목으로 나누어진 데이터 중, 가장 범용적인 언어 표현 도메인으로 이루어진 000 데이터 들을 사용합니다. 총 데이터 개수는 000개이며, 이 중, 학습, 검증, 테스트 데이터 셋으로 각각 000 개씩 사용합니다. 데이터 예시는 아래와 같습니다. 
          </p>

          <div class="spacer"></div>
          <li>Back Translation</li>
          <p>&nbsp; PLE Fused Model은 "Incorporating BERT into Neural Machine Translation" 논문에서 제시한 모델 구조를 사용합니다. 사전학습된 인코더 모델을 그대로 인코더로만 사용하는것이 아니라, 인코딩과 디코딩 과정에 추가적인 정보전달을 위한 모델로 활용합니다.
          </p>
          
        </ul>
      </div>
    </div>



<!-- 3. Experimental Setup -->
    <h2 id="3">&hairsp; 3. &hairsp; Experimental Setup</h2>
    <div class="chapter-box">

      <div class="list-container">
        <ul>
          <li>Data Setup
            <div class="dual-container">
              <ul>
                <li>Dataset: &nbsp; Ai Hub 한국어-영어 번역 말뭉치</li>
                <li>Dialogue Task: &nbsp; Daily Dialogue</li>
                <li>Summarization Task: &nbsp; CNN Daily</li>
                <li>Tokenizer: &nbsp; AlBERT Tokenizer</li>
              </ul>
              <ul>
                <li>Train Data Volumn: &nbsp; 50,000</li>
                <li>Valid Data Volumn: &nbsp; 5,000</li>
                <li>Test Data Volumn: &nbsp; 100</li>
                <li>Vocab Size: &nbsp; 10,000</li>                
              </ul>
            </div>
          </li>

          <div class="half-spacer"></div>
          <li>Model Setup
            <div class="dual-container">
              <ul>
                <li>PLE Architecture: &nbsp; AlBERT</li>
                <li>PLE Name: &nbsp; albert-v2</li>
                <li>Input Dim: &nbsp; 10,000</li>
                <li>Output Dim: &nbsp; 10,000</li>
              </ul>
              <ul>
                <li>Embedding Dim: &nbsp; 512</li>
                <li>Hidden Dim: &nbsp; 512</li>
                <li>Model Params: &nbsp; 000</li>
                <li>Model Size: &nbsp; 000</li>                
              </ul>
            </div>
          </li>

          <div class="half-spacer"></div>
          <li>Training Setup
            <div class="dual-container">
              <ul>
                <li>Num Epochs: &nbsp; 10</li>
                <li>Batch Size: &nbsp; 32</li>
                <li>Learning Rate: &nbsp; 5e-4</li>
                <li>LR Scheduler: &nbsp; pleature</li>
              </ul>
              <ul>
                <li>Optimizer: &nbsp; AdamW</li>
                <li>Gradient Accumulation Steps: &nbsp; 4
                <li>Teacher Forcing Ratio: &nbsp; 0.5</li>                
              </ul>
            </div>
          </li>
        
        </ul>
      </div>
    </div>



<!-- 4. Result -->
    <h2 id="4">&hairsp; 4. &hairsp; Result</h2>
    <div class="chapter-box">

      <div class="list-container">
        <ul>
          <li>Machine Translation</li>

            <table class="result-table">
              <thead>
                <tr>
                  <th>Model Type</th>
                  <th>Eval Score</th>
                  <th>Epoch Time</th>
                  <th>Avg GPU</th>
                  <th>Max GPU</th>
                </tr>
              </thead>
              <tbody>
                <tr>
                  <td>RNN Model</td>
                  <td>2.12</td>
                  <td>3m 58s</td>
                  <td>0.22GB</td>
                  <td>0.82GB</td>
                </tr>
                <tr>
                  <td>LSTM Model</td>
                  <td>8.35</td>
                  <td>4m 16s</td>
                  <td>0.27GB</td>
                  <td>1.30GB</td>
                </tr>
                <tr>
                  <td>GRU Model</td>
                  <td>9.75</td>
                  <td>4m 6s</td>
                  <td>0.26GB</td>
                  <td>1.16GB</td>
                </tr>

              </tbody>
            </table>
          
          <div class="half-spacer"></div>
          <li>Dialogue Generation</li>
            <table class="result-table">
              <thead>
                <tr>
                  <th>Model Type</th>
                  <th>Eval Score</th>
                  <th>Epoch Time</th>
                  <th>Avg GPU</th>
                  <th>Max GPU</th>
                </tr>
              </thead>
              <tbody>
                <tr>
                  <td>RNN Model</td>
                  <td>0.10</td>
                  <td>3m 40s</td>
                  <td>0.22GB</td>
                  <td>0.79GB</td>
                </tr>
                <tr>
                  <td>LSTM Model</td>
                  <td>0.37</td>
                  <td>4m 5s</td>
                  <td>0.27GB</td>
                  <td>1.23GB</td>
                </tr>
                <tr>
                  <td>GRU Model</td>
                  <td>2.16</td>
                  <td>3m 58s</td>
                  <td>0.26GB</td>
                  <td>1.11GB</td>
                </tr>

              </tbody>
            </table>

                
          <div class="half-spacer"></div>
          <li>Text Summarization</li>
            <table class="result-table">
              <thead>
                <tr>
                  <th>Model Type</th>
                  <th>Eval Score</th>
                  <th>Epoch Time</th>
                  <th>Avg GPU</th>
                  <th>Max GPU</th>
                </tr>
              </thead>
              <tbody>
                <tr>
                  <td>RNN Model</td>
                  <td>0.00</td>
                  <td>8m 6s</td>
                  <td>0.23GB</td>
                  <td>1.20GB</td>
                </tr>
                <tr>
                  <td>LSTM Model</td>
                  <td>2.23</td>
                  <td>9m 7s</td>
                  <td>0.29GB</td>
                  <td>2.00GB</td>
                </tr>
                <tr>
                  <td>GRU Model</td>
                  <td>2.19</td>
                  <td>8m 42s</td>
                  <td>0.27GB</td>
                  <td>1.82GB</td>
                </tr>

              </tbody>
            </table>

          <div class="half-spacer"></div>
          <li>Result Analysis</li>
            <p> &nbsp; RNN은 다양한 정보를 함유하기에 지나치게 단순하고, LSTM은 GATE구조가 복잡하기에, 그만큼 영향을 받은 요소가 많아 학습 수렴이 어렵습니다. 때문에 모든 학습 결과에서는 GATE를 사용하면서도 단순화된 연산으로 학습의 이점이 큰 GRU가 가장 뛰어난 성능을 보임을 알 수 있습니다. 
            </p>
        </ul>
      </div>
    </div>


<!-- 5. Conclusion -->
    <h2 id="5">&hairsp; 5. &hairsp; Conclusion</h2>
    <div class="chapter-box">

      <div class="list-container">
        <ul>
          <li>RNN, LSTM, GRU 을 활용한 seq2seq 모델링 방식의 이해</li>
            <p>&nbsp; 토큰의 벡터 변환을 위한 Embedding Layer와 실제 연산을 위한 순환 신경망 레이어만으로 구성된 단순산 Encoder, Decoder 구조로 이루어져 있으며, 
            </p>

          <div class="half-spacer"></div>
          <li>세 가지 순환 신경망의 성능 비교 검증</li>
            <p>LSTM은 게이트 메커니즘을 통해 장기 의존성 문제를 해결하는 데 강점을 보입니다. 복잡한 문맥을 학습하고 기억하는 데 탁월하며, 긴 시퀀스 데이터에서 뛰어난 성능을 발휘할 수 있습니다. 하지만 LSTM은 더 많은 파라미터를 가지고 있어서 더 많은 데이터와 계산 리소스가 필요할 수 있습니다.
            </p>

          <div class="half-spacer"></div>
          <li>가설 검증
            <ul>
              <li>LSTM, GRU, RNN 순으로 높은 성능</li>
                <p> LSTM의 연산과정이 가장 복잡하고, 다양한 GATE를 사용하기 떄문에, 좋은 성능을 낼 것이라고 예측했으나, 되려 복잡한 연산으로 인해, 학습 시 수렴에 어려움이 생겼습니다.
                오히려 GATE를 사용하면서도, 연산의 단순화를 도모한 GRU의 학습 성능이 가장 좋은 것을 확인했습니다
                </p>

              <li>RNN, GRU, LSTM 순으로 빠른 학습</li>
                <p> 학습 속도 및 GPU 사용량으로 확인해본 효율성은 가설과 마찬가지로 RNN, GRU, LSTM 순으로 나타났습니다.
                </p>
              <li></li>
            </ul>
          </li>
            
          <div class="half-spacer"></div>
          <li>최소 기준치 확립</li>
            <p>앞서 언급했듯이 순환 신경망을 사용한 Seq2Seq모델은 앞으로 다루게 될 다양한 Seq2Seq 모델들의 시작점입니다. 고도화 된 모델일지라도 하이퍼 파라미터 설정이나, 훈련 방식 등의 이유에서 잘못된 성능이 나올수도 있습니다. 이때, 최소한의 동작은 하는 구나 하는 판단의 근거로 다음과 같은 스코어를 활용할 수 있습니다. <br> 
              기계번역 BLEU Score: 00.00, 대화 생성 Rouge Score: 00.00, 문서 요약 Rouge Score: 00.00 이하의 성능ㅇ
            </p>

        </ul>
      </div>
    </div>


<!-- Reference -->    
    <h2 id="6">&hairsp; 6. &hairsp; Reference</h2>
    <div class="chapter-box">
      <a class="reference" href="https://arxiv.org/abs/1706.03762">&nbsp; Attention Is All You Need</a> <br> 
      <a class="reference" href="https://aclanthology.org/D18-1045.pdf">&nbsp; Understanding Back-Translation at Scale</a> 
    </div>

  </div>
</div>



<div class="pagination">
  <a href="{{ '/peft' | relative_url }}" class="btn-prev"><span>Parameteter Efficient Fine Tuning</span></a>
  <a href="{{ '/code_translation' | relative_url }}" class="btn-next"><span>Code Translation</span></a>
</div>