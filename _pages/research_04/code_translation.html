---
layout: default
permalink: /code_translation
---


<div id="detail">
  <h1 class="title">Code Translation</h1>  
  <div class="post">

    <hr>
    <div class="callout">
      <p class="highlighted-text"><span>Project Overview</span></p>
      <div class="small-spacer"></div>
      <p>&nbsp; 이 프로젝트에서는 Code Translation을 잘 수행하기 위해 모델 선택, 학습 방식에서 다양한 방법론을 코드로 구현해보고, 그 결과를 분석합니다. 모델로는 기본적인 Transformer부터, Seq2Seq 사전학습 모델, 그리고 GPT 계열의 사전학습 모델을 변인으로 삼고, 학습방식에서는 기본적인 MLE방식의 학습과, 더 나은 생성을 위한 추가 학습까지 진행하며 그 결과를 도출합니다.
      결과적으로 사전학습 모델을 기본적인 학습방식으로 사용했을때 가장 좋은 성능을 보였으며, 사용자가 원하는 코드 번역 과제에서도 실제 적용가능한 가능성을 확인했습니다.
      </p>
    </div>

    <div class="tblContents">
      <ul>

        <li>
          <a href="#1">1. Introduction</a>
          <ul>
            <li>
              <a href="#1.1">1.1 &hairsp; Code Translation</a>
            </li>
            <li>
              <a href="#1.2">1.2 &hairsp; Dataset</a>
            </li>
            <li>
              <a href="#1.3">1.3 &hairsp; Model</a>
            </li>
            <li>
              <a href="#1.4">1.4 &hairsp; Training</a>
            </li>              
          </ul>
        </li>

        <li>
          <a href="#2">2. &hairsp; Code Implementation</a>
          <ul>
            <li>
              <a href="#2.1">2.1 &hairsp; Cache Usable Decoder</a>
            </li>
            <li>
              <a href="#2.2">2.2 &hairsp; Two-way Forward Model</a>
            </li>                      
          </ul>
        </li>
        

        <li>
          <a href="#3">3. Experimental Setup</a>
        </li>

        <li>
          <a href="#4">4. Results</a>
        </li>

        <li>
          <a href="#5">5. Conclusion</a>
        </li>

        <li>
          <a href="#6">6. Reference</a>
        </li>

      </ul>
    </div>
    <hr>    


<!-- Training -->    
    <h2 id="1">1. &hairsp; Introduction</h2>

    <h3 id="1.1">1.1 &hairsp; Code Translation</h3>
    <p>
      &nbsp; Code Generation이라고 불리기도 하는 Code Translation은 자연어 시퀀스 입력값을 받아, 입력 시퀀스가 의미하는 바를 제대로 구현하는 프로그래밍 언어 시퀀스를 반환하는 과제입니다.
      최근 ChatGPT등 딥러닝 모델을 통한 Code Translation은 


      Code Translation은 사람의 자연어를 컴퓨터의 프로그래밍 언어로 변환하는 작업입니다. 프로그래밍 언어의 진입장벽을 낮추기도 하고, 개발자들 역시, 간편한 작업을 위해 최근 각광받는 Task입니다.

      자연어-자연어 번역보다는 생소하기 때문에, Code Translation이라는 Task 자체에 집중해서, 
      기본 Transformer model에서, 그리고 사전학습 모델에서, 그리고 특정 학습기법을 사용하는 방식에 따라서 어떻게 달라지는지 다양한 면모를 직접 확인해보고 인사이트를 공유하려고 합니다.
    </p>
    <hr>

    <h3 id="1.2">1.2 &hairsp; Conala Dataset</h3>
    <p>
      &nbsp; Conala는 Code Translation을 위한 학습 데이터셋.
      데이터의 구성은 다음과 같음.
    </p>
    <hr>

    <h3 id="1.3">1.3 &hairsp; Model</h3>
    <div class="list-container">
      <ul>
        <li>Transformer</li>   
          <p>일반적인 standard Trianin방식</p>   

        <div class="small-spacer"></div>
        <li>Code BERT Fusion</li>   
          <p>일반적인 standard Trianin방식</p>   

        <div class="small-spacer"></div>
        <li>Code LLaMA</li>
          <p>매번 모델의 순전파에서 Generation 방식을 사용한 방식.
             주어진 모든 학습 데이터에 대해 생성적 학습 만을 적용
          </p>

        <div class="small-spacer"></div>
        <li>Code T5</li>
          <p>Teacher Forcing 이후, Generative를 추가적인 학습으로 이어진행하는 학습방식.
          </p>             
          
      </ul>
    </div>

    <table class="result-table">
      
      <thead>
        <tr>
          <th>Model</th>
          <th>Params & size</th>
          <th>note</th>
        </tr>
      </thead>

      <tbody>
        <tr>
          <td>Vanilla Transformer</td>
          <td>-</td>
          <td>-</td>
        </tr>
      
        <tr>
          <td>Simple Model</td>
          <td>결과2-1</td>
          <td>결과2-2</td>
        </tr>
      
        <tr>
          <td>Fused Model</td>
          <td>결과2-1</td>
          <td>결과2-2</td>
        </tr>

        <tr>
          <td>Fused Model</td>
          <td>결과2-1</td>
          <td>결과2-2</td>
        </tr>        

      </tbody>
    </table>

    <hr>      


<!-- Experimental Setup -->    
    <h2>2. Experimental Setup</h2>

    <h3>Training Strategies</h3>
    <div class="list-container">
      <ul>
        <li>Teacher Forcing Training</li>   
          <p>일반적인 standard Trianin방식</p>   

        <div class="small-spacer"></div>
        <li>Generative Training</li>
          <p>매번 모델의 순전파에서 Generation 방식을 사용한 방식.
             주어진 모든 학습 데이터에 대해 생성적 학습 만을 적용
          </p>

        <div class="small-spacer"></div>
        <li>Consecutive Training</li>
          <p>Teacher Forcing 이후, Generative를 추가적인 학습으로 이어진행하는 학습방식.
          </p>             
          
      </ul>
    </div>
    <hr>

    <div class="half-spacer"></div>
    <h3>Setup Arguments</h3>
    <table class="result-table">
      <thead>
        <tr>
          <th>Data Setup</th>
          <th>Training Setup</th>
        </tr>
      </thead>
      <tbody>
        <tr>

          <td>
            <ul>
              <li><b>Dataset</b>: &nbsp; Conala</li>
              <li><b>Train Data Volumn</b>: &nbsp; 50,000</li>
              <li><b>Valid Data Volumn</b>: &nbsp; 5,000</li>
              <li><b>Test Data Volumn</b>: &nbsp; 100</li>
            </ul>
          </td>

          <td>
            <ul>
              <li><b>Num Epochs</b>: &nbsp; 10</li>
              <li><b>Batch Size</b>: &nbsp; 32</li>
              <li><b>Learning Rate</b>: &nbsp; 5e-4</li>
              <li><b>LR Scheduler</b>: &nbsp; pleature</li>
              <li><b>Optimizer</b>: &nbsp; AdamW</li>
              <li><b>Gradient Accumulation Steps</b>: &nbsp; 4</li>
            </ul>
          </td>          
        </tr>

      </tbody>
    </table>
    <div class="half-spacer"></div>
    <hr>


<!-- Result -->    
    <h2>3. Result</h2>
    <div class="small-spacer"></div>
    <table class="result-table">
      
      <thead>
        <tr>
          <th>Attention Strategy</th>
          <th>Machine Translation</th>
          <th>Dialogue Generation</th>
          <th>Text Summarization</th>
        </tr>
      </thead>

      <tbody>
        <tr>
          <td>Vanilla Transformer</td>
          <td>-</td>
          <td>-</td>
          <td>-</td>
        </tr>
      
        <tr>
          <td>Simple Model</td>
          <td>결과2-1</td>
          <td>결과2-2</td>
          <td>결과2-3</td>
        </tr>
      
        <tr>
          <td>Fused Model</td>
          <td>결과2-1</td>
          <td>결과2-2</td>
          <td>결과2-3</td>
        </tr>

      </tbody>
    </table>
    <hr>


<!-- Conclusion -->    
    <h2>4. Conclusion</h2>
    <hr>


<!-- Reference -->    
    <h2>5. Reference</h2>
    <a class="reference" href="https://arxiv.org/abs/1706.03762">&nbsp; Attention Is All You Need</a> <br>
    <hr>


  </div>
</div>


<div class="pagination">
  <a href="{{ '/gen_train' | relative_url }}" class="btn-prev"><span>Generative Training</span></a>
  <a href="{{ '/custom_pretrain' | relative_url }}" class="btn-next"><span>Customized Pretraining</span></a>
</div>