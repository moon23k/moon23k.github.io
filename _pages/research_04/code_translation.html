---
layout: default
permalink: /code_translation
---


<div id="detail">
  <h1 class="title">Code Translation</h1>  
  <div class="post">


    <h2>1. &hairsp; Project Desc</h2>

      <div class="half-spacer"></div>
      <h3>Code Translation</h3>
      <div class="small-spacer"></div>
      <p>
        &nbsp; Code Translation은 사람이 사용하는 자연어를 코드로 변환해주는 과제입니다.
        이 프로젝트에서는 코드 생성을 위해, 사전학습된 코드 생성 모델을 GAN의 테크닉을 활용해, 원하는 프로그래밍 언어 및 라이브러리에 특화되도록
        학습시키는 것을 목표로 합니다.


        코드 번역 작업은 개발을 필요로 하는 사람들로 하여금, 새로운 언에 익숙해지는데 필요한 노력을 최소화 하며, 프로그래밍의 진입장벽을 낮추는 데 유용한 작업입니다.
      </p>

      <div class="spacer"></div>
      <h3>Code T5</h3>
      <div class="small-spacer"></div>
      <p>
        &nbsp; Code T5는 통합된 사전 훈련 인코더-디코더 트랜스포머 모델입니다.

        기존의 Translation이 from NL to NL이었던것에 반해, Code Translation은 from NL to PL(Programming Language)입니다.

        일반적으로 Code Generation에서 간과되었던, Programming Language를 다루는 방식에 차이를 준 모델.
      </p>

      <div class="spacer"></div>
      <h3>Conala Dataset</h3>
      <div class="small-spacer"></div>
      <p>
        &nbsp; prompt tuning
      </p>

      <div class="spacer"></div>
      <h3>SeqGAN Training</h3>
      <div class="small-spacer"></div>
      <p>
        &nbsp; prompt tuning
      </p>


    <h2>2. &hairsp; Experimental Setup</h2>

      <div class="small-spacer"></div>
      <h3>Data Setup</h3>
      <div class="small-spacer"></div>
      <p>Sequence Classification을 위한 대표적 데이터 셋 중하나인 AG_News를 사용.
        데이터의 크기는 Train/ Valid/ Test 1000/ 100/ 100으로 설정
        총 네가지 데이터 Label을 모든 데이터에서 고르게 분배해서 한쪽으로 치우치지 않도록 가공
      </p>

      <div class="spacer"></div>
      <h3>Model Setup</h3>
      <div class="small-spacer"></div>
      <p>대표적인 PreTrained Model인 BERT를 사용.
      </p>


      <div class="spacer"></div>
      <h3>Training Setup</h3>
      <div class="small-spacer"></div>

      <div class="code-container">
        <div class="code-snippet">
          <pre><code class="python">
TrainingArguments(
        output_dir= f'ckpt/{strategy}',
        num_train_epochs= 5,
        learning_rate= 1e-5,
        per_device_train_batch_size= 32,
        per_device_eval_batch_size= 32,
        lr_scheduler_type='reduce_lr_on_plateau',
        load_best_model_at_end= True,

        save_strategy= 'epoch',
        logging_strategy= 'epoch',
        evaluation_strategy= 'epoch',

        fp16= True if config.strategy in ['fp16', 'all'] else False,
        fp16_opt_level= '02' if config.strategy in ['fp16', 'all'] else '01',
        gradient_accumulation_steps = True if config.strategy in ['grad_accumulation', 'all'] else 4,
        gradient_checkpointing= True if config.strategy in ['grad_checkpointing', 'all'] else False,
        optim = 'adafactor' if config.strategy in ['optim', 'all'] else 'adamw_torch'
)
          </code></pre>
        </div>
      </div>



    <h2>3. &hairsp; Result</h2>
    <div class="small-spacer"></div>
    <table class="result-table">
      
      <thead>
        <tr>
          <th>PEFT Strategy</th>
          <th>Train Time</th>
          <th>GPU Occupation</th>
          <th>Accuracy</th>
        </tr>
      </thead>

      <tbody>
        <tr>
          <td>Baseline</td>
          <td>174</td>
          <td>7.00</td>
          <td>79</td>
        </tr>

        <tr>
          <td>Prompt Tuning</td>
          <td>69</td>
          <td>5.47</td>
          <td>78</td>
        </tr>
        <tr>
          <td>Prefix Tuning</td>
          <td>182</td>
          <td>6.29</td>
          <td>83</td>
        </tr>

        <tr>
          <td>P Tuning</td>
          <td>239</td>
          <td>3.54</td>
          <td>79</td>
        </tr>

        <tr>
          <td>IA3</td>
          <td>179</td>
          <td>6.72</td>
          <td>79</td>
        </tr>

      </tbody>
    </table>


    <h2>4. &hairsp; Conclusion</h2>
    <div class="small-spacer"></div>
    <p> &nbsp; 균형이 잘 맞는 경우가 가장 이상적. 불균형은 자연스레 성능 하락을 야기
    </p>  


    <h2>5. Reference</h2>
    <a class="reference" href="https://arxiv.org/abs/2109.00859">&nbsp; CodeT5: Identifier-aware Unified Pre-trained Encoder-Decoder Models for Code Understanding and Generation</a> <br> 


  </div>
</div>



<div class="pagination">
  <a href="{{ '/eff_model' | relative_url }}" class="btn-prev"></a>
  <a href="{{ '/aux_train' | relative_url }}" class="btn-next"></a>
</div>