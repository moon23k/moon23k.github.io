---
layout: default
permalink: /multilingual
---


<div id="detail">
  <h3 class="title">다언어 번역 모델의 성능 비교</h3>

  <div class="tblContents">
    <ul>
      <li>
        <a href="#1">1. Introduction</a>
      </li>

      <li>
        <a href="#2">2. Background</a>
        <ul>
          <li>
            <a href="#2.1">2.1 BERT</a>
          </li>
          <li>
            <a href="#2.2">2.2 BERT SUM</a>
          </li>
          <li>
            <a href="#2.3">2.3 Fused BERT on Encoder-Decoder Architecture</a>
          </li>
          <li>
            <a href="#2.4">2.4 Leveraging Pre-Trained Checkpoints</a>
          </li>          
        </ul>
      </li>
      
      <li>
        <a href="#3">3. Architecure</a>
        <ul>
          <li>
            <a href="#3.1">3.1 Common Encoder</a>
          </li>
          <li>
            <a href="#3.2">3.2 Simple Model</a>
          </li>
          <li>
            <a href="#3.2">3.3 Fused Model</a>
          </li>
          <li>
            <a href="#3.3">3.4 Lever Model</a>
          </li>
        </ul>
      </li>

      <li>
        <a href="#4">4. Experimental Setup</a>
        <ul>
          <li>
            <a href="#4.1">4.1 Data Setups</a>
          </li>
          <li>
            <a href="#4.2">4.2 Model Setups</a>
          </li>
          <li>
            <a href="#4.3">4.3 Training Setups</a>
          </li>
          <li>
            <a href="#4.4">4.4 Model desc</a>
          </li>
        </ul>
      </li>

      <li>
        <a href="#5">5. Results</a>
        <ul>
          <li>
            <a href="#5.1">5.1 Training Convergence</a>
          </li>
          <li>
            <a href="#5.2">5.2 Test Result</a>
          </li>          
        </ul>
      </li>

      <li>
        <a href="#6">6. Conclusion</a>
      </li>

      <li>
        <a href="#7">7. Reference</a>
      </li>

    </ul>
  </div>


  
  <div class="post">
    <h2 id="1">1. Introduction</h2>
      <p>
        &nbsp; 기계번역 과제는 Source Language의 시퀀스를 동일한 의미의 Target Language의 시퀀스로 변환하는 과제입니다. 일반적으로는 단일 언어 쌍 바탕의 기계번역을 과제로 삼고 학습 및 서비스 됩니다. 하지만, 동일한 입력 언어를 통해 다양한 언어로의 변환이 필요한 경우가 발생할 수 있습니다. 이러한 경우, 필요한 언어 쌍만큼 모델을 학습시키는 것보다, 통합적으로 변환이 가능한 
      </p>


    <h2 id="2">2. Background</h2>

      <h3 id="2.1" class="h3-mt">2.1 BERT</h3>
        <p>
          &nbsp; BERT(Bidirectional Encoder Representations from Transformers)는 Transformer Encoder 기반의 자연어 처리를 위해 사전 훈련된 언어 모델입니다. BERT는 사전학습과정에서 Masked Language Modeling을 통해, Masked Token을 예측하는 과정에서 모든 문맥정보를 사용하는 훈련을 하게 됩니다. 덕분에 BERT는 양방향으로 문맥을 고려하며 보다 깊은 이해를 할 수 있는 능력을 갖습니다. BERT는 다양한 자연어 처리 과제에서 좋은 성능을 기록했으며, 사용 방법 또한 간단해 많은 인기를 얻은 모델입니다.
        </p>

      <h3 id="2.2" class="h3-mt">2.2 BERT SUM</h3>
        <p>
          &nbsp; 사전학습된 BERT 모델을 문서 요약에서 활용하는데에는 몇가지 제약사항이 존재합니다. 긴 입력 시퀀스의 길이와 여러 문장으로 구성된 시퀀스의 구조, BERT가 수용가능한 최대 시퀀스 길이의 한계 등이 그런 제약에 해당합니다. “Fine-tune BERT for Extractive Summarization” 논문에서는 앞서 언급한 제약을 뛰어넘어 BERT라는 효과적인 사전학습 모델을 텍스트 요약 과제에서 효율적으로 사용할 수 있는 방법론을 제시합니다. 
        </p>

      <h3 id="2.3" class="h3-mt">2.3 Fused BERT on Encoder-Decoder Architecture</h3>
        <p>
          &nbsp; Fine Tuning을 통한 일반적인 BERT의 사용 방식이 아닌, BERT를 추가적인 문맥 임베딩으로 사용하는 방식 역시 좋은 성능 향상을 이끌어 낼 수 있습니다. “INCORPORATING BERT INTO NEURAL MACHINE TRANSLATION” 논문에서는 앞서 언급한 접근방식을 보다 고도화 시킨 BERT-fused 모델을 제시합니다. BERT-fused 모델은 먼저 BERT를 사용해 입력 시퀀스의 표현을 추출하고, 해당 표현을 전체 모델의 인코더와 디코더에서 융합적으로 사용합니다. 이를 통해 다양한 자연어 생성 과제에서 Sota성능을 기록했다고 합니다.
        </p>

      <h3 id="2.4" class="h3-mt">2.4 Leveraging Pre-trained Checkpoints</h3>
        <p>
          &nbsp; 대규모 사전학습 모델의 체크포인트를 활용해, 필요 과제에 맞게 미세조정하는 방식은 다양한 자연어 처리 과제에서 혁신적인 발전을 견인했습니다. 그런데 대부분 사전학습 모델들은 Natural Language Understanding 해결에 용이한 Encoder 구조로 구성되어 있기에 자연어 생성 과제을 위한 선택지가 제한적입니다. 물론 BART, T5 등 Encoder-Decoder 모델이 존재하긴 하지만, Encoder로써 활용가능한 사전학습 모델의 수에 비한다면, 그 다양성이 상대적으로 부족한 실정입니다.
        </p>
        <p>
          그래서 "Leveraging Pre-trained Checkpoints for Sequence Generation Tasks" 논문에서는 BERT, GPT-2, 그리고 RoBERTa와 호환되는 Transformer 기반 시퀀스-투-시퀀스 모델을 개발하였으며, 이러한 체크포인트를 활용하여 모델의 인코더와 디코더를 초기화하는 유틸리티에 대해 광범위한 실험을 진행하였습니다. 이 모델은 기계 번역, 텍스트 요약, 문장 분리 등의 과제에서 Sota를 기록했습니다.
        </p>


    <h2 id="3">3. Architecture</h2>
      <h3>3.1 Common Encoder</h3>
        <div class="img-container">
          <img src="{{ 'assets/img/project_02/bert_sum.png' | relative_url }}" height="250px">
          <ul>
            <li>BERT SUM
              <ul>
                <li>Simple, Fused, Lever Model 모든 실험 모델의 Encoder에 BERTSUM 방법론을 차용</li>
                <li>여러 문장간 의미를 파악할 수 있도록 Token Type Id에 변형을 가함</li>
                <li>최대 수용가능한 문장길이 512 → 2048로 확장</li>
              </ul>
            </li>
          </ul>
        </div>

      <h3 id="3.2" class="h3-mt">3.2 Simple Model</h3>
        <div class="img-container">
          <img src="{{ 'assets/img/project_02/standard_transformer.png' | relative_url }}">
          <ul>
            <li>Standard Transformer
              <ul>
                <li>Encoder</li>
                <li>Decoder</li>
              </ul>
            </li>

            <li>Encoder-Decoder Connection
              <ul>
                <li>Transformer의 방법론과 동일</li>
                <li>Pre Trained Encoder와 Randomly Initialized Decoder간의 학습 간극을 좁히기 위해 Encoder와 Decoder에 서로 상이한 Optimization 수행</li>
              </ul>
            </li>

          </ul>
        </div>

      <h3 id="3.3" class="h3-mt">3.3 Fused Model</h3>
        <div class="img-container">
          <img src="{{ 'assets/img/project_02/fused.png' | relative_url }}" height="280px">
          <ul>
            <li>Recurrent Transformer
              <ul>
                <li>Encoder</li>
                <li>Decoder</li>
              </ul>
            </li>

            <li>Encoder Layer
              <ul>
                <li>Sub-Layer 1</li>
                <li>Sub-Layer 2</li>
                <li>Sub-Layer 3</li>
              </ul>
            </li>

            <li>Decoder Layer
              <ul>
                <li>Sub-Layer 1</li>
                <li>Sub-Layer 2</li>
                <li>Sub-Layer 3</li>
                <li>Sub-Layer 4</li>
              </ul>
            </li>

          </ul>
        </div>

      <h3 id="3.4" class="h3-mt">3.4 Evolved Transformer</h3>
        <div class="img-container">
          <ul>
            <li>Evolved Transformer
              <ul>
                <li>Encoder</li>
                <li>Decoder</li>
              </ul>
            </li>

            <li>Cell
              <ul>
                <li>Sub-Layer 1</li>
                <li>Standard Transformer에서 2개 레이어에 걸친 연산 과정을 고도화 시킨 SubLayer를 Evolved Transformer에서는 Cell이라고 표현</li>
              </ul>
            </li>

            <li>Encoder Cell
              <ul>
                <li>Block 1</li>
                <li>Block 2</li>
                <li>Block 3</li>
                <li>Block 4</li>
                <li>Block 5 & 6</li>
              </ul>
            </li>

            <li>Decoder Cell
              <ul>
                <li>Block 1</li>
                <li>Block 2</li>
                <li>Block 3</li>
                <li>Block 4</li>
                <li>Block 5</li>
                <li>Block 6 & 7</li>
              </ul>
            </li>

          </ul>
        </div>

    <h2 id="4" class="h3-mt">4. Experimental Setup</h2>
      <h3 id="4.1">4.1 Data Setups</h3>

      <h3 id="4.2" class="h3-mt">4.2 Model Setups</h3>
      
      <h3 id="4.3" class="h3-mt">4.3 Training Setups</h3>
      
      <h3 id="4.4" class="h3-mt">4.4 Model desc</h3>


    <h2 id="5" class="h3-mt">5. Result</h2>


    <h2 id="6" class="h3-mt">6. Conclusion</h2>


    <h2 id="7" class="h3-mt">7. Reference</h2>


  </div>
</div>

<div class="pagination">
  <a href="{{ '/project_01' | relative_url }}" class="btn-prev"></a>
  <a href="{{ '/project_03' | relative_url }}" class="btn-next"></a>
</div>