---
layout: default
permalink: /gan_train
---


<div id="detail">
  <h1 class="title">SeqGAN Training for Transformer</h1>
  <div class="post">

    <div class="callout">
      <p class="highlighted-text"><span>Project Objective</span></p>
      <div class="small-spacer"></div>
      <p>&nbsp; 딥러닝 모델의 학습은 Training Objective를 바탕으로 모델의 파라미터를 최적화 시키는 방향으로 진행됩니다. 일반적으로 NLG Task는 MLE를 Training Objective로 해서 학습됩니다. 좋은 성능을 보이고는 있지만, 그래도 결과값을 생성해보면 그래도 인간이 생성하는 결과값과는 미묘한 차이가 있습니다. 이런 미묘한 차이를 없애고, 더욱 자연스러운 생성을 위해서 SeqGAN을 활용할 수 있습니다. SeqGAN은 GAN의 개념을 시퀀셜 데이터에 적용시키기 위한 기법입니다. 다만 이 기법은 Generative Training을 기반으로 하기에 Transformer에 적용하기는 힘듭니다. 이 프로젝트에서는 Transformer에 적용할수 있도록 SeqGAN의 개념을 차용한 방법론을 제시하고, 실제 실험 후 두 가지 NLG Task의 더욱 자연스럽고 좋은 결과 반환을 위한 추가학습 방법으로써 SeqGAN을 사용해보고, 그 실효성을 검증해봅니다.</p>
    </div>

    <div class="tblContents">
      <ul>

        <li>
          <a href="#1">1. Training Strategy</a>
          <ul>
            <li>
              <a href="#1.1">1.1 &hairsp; GAN</a>
            </li>
            <li>
              <a href="#1.2">1.2 &hairsp; Sequence GAN</a>
            </li>
            <li>
              <a href="#1.3">1.3 &hairsp; SeqGAN for Transformer</a>
            </li>                      
          </ul>
        </li>
        
        <li>
          <a href="#2">2. Model</a>
          <ul>
            <li>
              <a href="#2.1">2.1 &hairsp; Generator</a>
            </li>
            <li>
              <a href="#2.2">2.2 &hairsp; Discriminator</a>
            </li>          
          </ul>
        </li>

        <li>
          <a href="#3">3. Experimental Setup</a>
        </li>

        <li>
          <a href="#4">4. Results</a>
        </li>

        <li>
          <a href="#5">5. Conclusion</a>
        </li>

        <li>
          <a href="#6">6. Reference</a>
        </li>

      </ul>
    </div>
    <hr>


<!-- Training Strategy -->    
    <h2 id="1">1. &hairsp; Training Strategy</h2>

      <h3 id="1.1">1.1 &hairsp; GAN</h3>

      <p>&nbsp; GAN은 Generator와 Discriminator가 적대적으로 학습
        Generator: 실제와 유사한 결과값을 창출하려고, Discriminatofrom r를 속일정도록
        Discriminator: Generator의 생성 결과와 실제 정답 레이블 간의 차이를 잘 찾아내도록 학습

        결과적으로 제너레이터는 더욱 정교한 결과물을 만들어 내도록 학습되고,
        제너레이터가 정교해질 수록, 그 미묘한 차이 마저도 포착할 수 있도록 Dicriminator역시 덩달아 정교해집니다.

        덕분에 단순한 Loss Function만으로는 다듬기 힘든 부분까지 모델이 학습할 수 있게 유도합니다.

        여러 방면에서 학습이 매우 효과적임을 입증함.
      </p><hr>

      <div class="half-spacer"></div>
      <h3 id="1.2">1.2 &hairsp; Sequence GAN</h3>

      <p>&nbsp; GAN은 매우 효과적인 학습 방법론입니다.
        하지만 Sequential 한 데이터에는 사용하기 어렵습니다.
        병렬적으로 

        Generative Training 속에 Discriminator를 활용하는 방법도 있기는 하지만, Transformer를 통한 Generative Training은 효율성도 그렇고, 실효성도 그닥이라서.

        차라리 Transformer의 병렬 학습을 최대한 활용하면서, 그 안에서 GAN의 개념을 삽입시키는 방향으로 학습법을 수정.
      </p><hr>

      <div class="half-spacer"></div>
      <h3 id="1.3">1.3 &hairsp; SeqGAN for Transformer</h3>

      <p>&nbsp; SeqGAN을 Transformer에 적용하기 위한 다음과 같은 방법론을 제시합니다.

        1. 
      </p><hr>

    <h2 id="2">2. &hairsp; Model</h2>

    <h3 id="2.1">2.1 &hairsp; Generator</h3>
    <p>처음부터 SeqGAN을 사용하는 것이 아니라, 처음에는 Generator와 Discriminator 각각이 어느 정도의 생성 및 판별 능력을 배양할 수 있도록 사전학습을 시킵니다.

    우선은 Generator를 일반 Transformer의 그것과 동일한 방식으로 학습시킵니다.
    그리고 사전학습된 Generator가 입력값에 반응해서 도출한 결과값과 실제 레이블 시퀀스를 입력값으로 받아, 참거짓을 분류하는 이진 분류 모델로써 Discriminator를 학습시킵니다.

    </p>
    <div class="code-container">
      <div class="code-snippet">
        <pre><code class="python">class Generator(nn.Module):
    def __init__(self, config):
        super().__init__()

        return x</code></pre>
      </div>
    </div>
    <hr>


    <div class="spacer"></div>
    <h3 id="2.2">2.2 &hairsp; Discriminator</h3>
    <p>대표적인 PreTrained Model인 BERT를 사용.
    </p>
    <div class="code-container">
      <div class="code-snippet">
        <pre><code class="python">class Discriminator(nn.Module):
    def __init__(self, config):
        super().__init__()

        return x</code></pre>
      </div>
    </div>
    <hr>    


<!-- Experimental Setup -->    
    <h2 id="3">3. Experimental Setup</h2>



<!-- Result -->    
    <h2>3. Result</h2>
    <div class="small-spacer"></div>
    <table class="result-table">
      
      <thead>
        <tr>
          <th>Attention Strategy</th>
          <th>Machine Translation</th>
          <th>Dialogue Generation</th>
          <th>Text Summarization</th>
        </tr>
      </thead>

      <tbody>
        <tr>
          <td>Vanilla Transformer</td>
          <td>-</td>
          <td>-</td>
          <td>-</td>
        </tr>
      
        <tr>
          <td>Simple Model</td>
          <td>결과2-1</td>
          <td>결과2-2</td>
          <td>결과2-3</td>
        </tr>
      
        <tr>
          <td>Fused Model</td>
          <td>결과2-1</td>
          <td>결과2-2</td>
          <td>결과2-3</td>
        </tr>

      </tbody>
    </table>


<!-- Conclusion -->    
    <h2>4. Conclusion</h2>



<!-- Reference -->    
    <h2>5. Reference</h2>
    <a class="reference" href="https://arxiv.org/abs/1406.2661">&nbsp; Generative Adversarial Networks</a> <br> 
    <a class="reference" href="https://arxiv.org/abs/1609.05473">&nbsp; SeqGAN: Sequence Generative Adversarial Nets with Policy Gradient</a> <br>

  </div>
</div>


<div class="pagination">
  <a href="{{ '/rnn_seq2seq_attn' | relative_url }}" class="btn-prev"><span>RNN Seq2sSeq with Attention</span></a>
  <a href="{{ '/transformer_variants' | relative_url }}" class="btn-next"><span>Transformer Variants</span></a>
</div>