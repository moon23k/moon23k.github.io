---
layout: default
permalink: /gan_train
---


<div id="detail">
  <h1 class="title">SeqGAN Training for Transformer</h1>
  <div class="post">

    <hr>
    <div class="callout">
      <p class="highlighted-text"><span>Project Objective</span></p>
      <div class="small-spacer"></div>
      <p>&nbsp; 이 프로젝트에서는 생성 능력을 키우기 위한 학습 방식인 GAN을 시퀀셜 데이터에서 사용하기 위한 방법론인 SeqGAN을 통해 성능이 얼마나 개선될 수 있을지 직접 확인해봅니다. 실험을 위한 아키텍처로는 Transformer를 사용하며, 검증을 위한 과제로는 기계번역과 대화생성을 사용합니다.</p>
    </div>

    <div class="tblContents">
      <ul>

        <li>
          <a href="#1">1. &hairsp; Introduction</a>
        </li>

        <li>
          <a href="#2">2. &hairsp; Background</a>
        </li>

        <li>
          <a href="#3">3. &hairsp; Training</a>
        </li>

        <li>
          <a href="#4">4. &hairsp; Experimental Setup</a>
        </li>

        <li>
          <a href="#5">5. &hairsp; Result</a>
        </li>

        <li>
          <a href="#6">6. &hairsp; Conclusion</a>
        </li>

        <li>
          <a href="#7">7. &hairsp; Reference</a>
        </li>

      </ul>
    </div>
    <hr>


<!-- 1. Introduction -->    
    <h2 id="1">1. &hairsp; Introduction</h2>
    <div class="chapter-box">

      <div class="list-container">
        <ul>
          <li>Description</li>
            <p> &nbsp; 딥러닝 모델의 학습은 Training Objective를 바탕으로 모델의 파라미터를 최적화 시키는 방향으로 진행됩니다. 일반적으로 NLG Task는 MLE를 Training Objective로 해서 학습됩니다. 좋은 성능을 보이고는 있지만, 그래도 결과값을 생성해보면 그래도 인간이 생성하는 결과값과는 미묘한 차이가 있습니다. 이런 미묘한 차이를 없애고, 더욱 자연스러운 생성을 위해서 SeqGAN을 활용할 수 있습니다. SeqGAN은 GAN의 개념을 시퀀셜 데이터에 적용시키기 위한 기법입니다. 다만 이 기법은 Generative Training을 기반으로 하기에 Transformer에 적용하기는 힘듭니다. 이 프로젝트에서는 Transformer에 적용할수 있도록 SeqGAN의 개념을 차용한 방법론을 제시하고, 실제 실험 후 두 가지 NLG Task의 더욱 자연스럽고 좋은 결과 반환을 위한 추가학습 방법으로써 SeqGAN을 사용해보고, 그 실효성을 검증해봅니다.
            </p>

          <div class="half-spacer"></div>
          <li>Objective
            <ul>
              <li style="font-weight: normal;">Transformer의 SeqGAN을 위한 학습 파이프라인 설계 및 구현</li>
              <li style="font-weight: normal;">SeqGAN의 효용성을 두 가지 자연어 생성 과제에서 검증</li>
            </ul>
          </li>

        </ul>
      </div>

    </div>



<!-- 2. Background -->    
    <h2 id="2">2. &hairsp; Background</h2>
    <div class="chapter-box">

      <div class="list-container">
        <ul>
          <li>GAN
            <p>&nbsp; 생성적 적대 신경망(GAN)은 2014년에 Ian Goodfellow에 의해 소개된 혁신적인 딥러닝 아키텍처입니다. GAN은 가짜 데이터를 생성하고 진짜 데이터와 거의 구분할 수 없는 정도로 현실적인 데이터를 생성할 수 있는 능력으로 주목받고 있습니다. 이 모델은 Generator와 Discriminator라는 두 가지 주요 구성 요소로 구성되어 있습니다. <br>


            <b>Generator:</b> &nbsp; Generator는 무작위 잡음이나 난수를 입력으로 받아 실제 데이터와 유사한 가짜 데이터를 생성하는 역할을 합니다. Generator의 목표는 판별자를 속이는 것으로, 생성된 가짜 데이터가 실제 데이터와 구별하기 어려울 정도로 현실적이어야 합니다. <br>

            <b>Discriminator:</b> &nbsp; Discriminator는 입력으로 실제 데이터나 생성자가 생성한 가짜 데이터를 받아 그것이 실제 데이터인지, 아니면 생성자에 의해 만들어진 것인지를 판별합니다. Discriminator의 목표는 가짜 데이터와 실제 데이터를 정확히 구분하는 것입니다. <br>

            GAN의 핵심 아이디어는 Generator와 Discriminator가 서로 경쟁하면서 성장하는 것입니다. Generator는 점차적으로 실제 데이터와 더 비슷한 가짜 데이터를 생성하려고 노력하고, Discriminator는 점차적으로 더 정확하게 가짜와 진짜를 구별하도록 학습합니다. 이 경쟁적 학습 과정은 두 네트워크를 모두 개선시키고 결국에는 Generator가 실제와 거의 구분할 수 없는 현실적인 데이터를 생성하게 됩니다.

            GAN은 이미지 생성, 스타일 전이, 이미지 변환, 음악 생성 등 다양한 응용 분야에서 사용되며, 그 창의적이고 혁신적인 성능으로 딥러닝 연구 및 응용 분야에서 활발하게 활용되고 있습니다.
            </p>
          </li>

          <div class="spacer"></div>
          <li>Sequence GAN
            <div class="dual-container">
                <img src="{{ 'assets/img/research_02/seqgan.png' | relative_url }}" style="width: 350px; height: 150px;">
                <p>
                &nbsp; SeqGAN은 GAN을 연속적인 값이 아닌, 이산적인 시퀀셜 데이터에서 사용하기 위한 기법입니다. 시퀀셜 데이터의 경우, 연속데이터와 다르게 이전 시점의 정보가 현시점의 결과를 만들어 내는데 매우 중요하기에 매 생성 시점마다 이전 생성값을 재참조해야 합니다. 이런 비효율성과 직관적이지 않은 생성과정은 생성을 바탕으로 하는 GAN을 시퀀셜 데이터에 적용하기 어렵게 만드는 요소로 작용합니다. 
                </p>
            </div>
            <p>SeqGAN에서는 시퀀셜 데이터에 GAN을 적용하는 과정을 강화학습의 개념을 차용해 설명합니다. Agent는 Generator가 되며, Generator의 생성과정이 Action에 해당하며, 보상을 결정짓는 Reward function에서는 생성한 시퀀스가 생성한 것인지 실제 레이블인지를 판별한 결과를 반영합니다. 그리고 Agent는 이 보상을 최대화 할 수 있도록 Policy를 업데이트 하는 방식으로 학습하게 됩니다.  
            </p>
          </li>
        <ul>          
      </div>
    </div>




<!-- 2. Training -->    
    <h2 id="3">3. &hairsp; Training</h2>
    <div class="chapter-box">

      <div class="list-container">
        <ul>
          <li>Pretrain
            <p>&nbsp; SeqGAN을 하기 이전, 사전학습이 필요합니다. Generator와 Discriminator 모두 사전학습을 진행하며, 우선 Generator를 일반적인 Transformer의 병렬처리 학습 방식으로 Generation Model을 사전 학습하고, 사전 학습된 Generator를 바탕으로 이후 Discriminator 학습에 활용합니다. 사전학습을 위한 코드 구현은 아래와 같습니다. 
            </p>

            <div class="code-container">
              <div class="code-snippet">  
                <pre><code class="python">class PreTrainer(TrainerBase):
    def __init__(self, config, generator, discriminator, train_dataloader, valid_dataloader):
        super(PreTrainer, self).__init__(config, generator, discriminator, train_dataloader, valid_dataloader)

        self.early_stop = config.early_stop
        self.patience = config.patience
        
        self.device_type = config.device_type
        self.scaler = torch.cuda.amp.GradScaler()
        self.iters_to_accumulate = config.iters_to_accumulate
        
        self.set_training_attrs(generator, 'pre_gen')
        self.set_training_attrs(discriminator, 'pre_dis')


    def load_pt_generator(self):
        state = torch.load(
            f'ckpt/{self.task}/pre_gen_model.pt',
            map_location=self.device
        )['model_state_dict']

        self.generator.load_state_dict(state)
        self.generator.eval()
        print('Pretrained Generator has loaded to pretrain discriminator\n')
        

    def print_epoch(self, epoch_report, model_type):
        train_loss = f"{epoch_report['train_loss']:.3f}"
        valid_loss = f"{epoch_report['valid_loss']:.3f}"
        gpu_memory = epoch_report['gpu_memory']
        max_memory = epoch_report['gpu_max_memory']

        max_len = max([len(f"{x:.3f}") if isinstance(x, (float)) else len(str(x)) for x in epoch_report.values()])
        
        elapsed_time = epoch_report['epoch_time']
        elapsed_min = int(elapsed_time / 60)
        elapsed_sec = int(elapsed_time - (elapsed_min * 60))

        txt = f"""Epoch {epoch_report['epoch']}/{self.n_epochs} | Time: {elapsed_min}m {elapsed_sec}s
            >> Train Loss: {train_loss:>{max_len}}  |  Valid Loss: {valid_loss:>{max_len}}
            >> GPU Memory: {gpu_memory:>{max_len}}  |  Max Memory: {max_memory:>{max_len}}\n"""
        print(f"[ {'Generator' if 'gen' in model_type else 'Discriminator'} ]")
        print(txt.replace(' '* 11, ''))

    
    def train(self):
        self.train_model(self.generator, 'pre_gen')
        self.load_pt_generator()
        self.train_model(self.discriminator, 'pre_dis')


    def train_model(self, model, model_type):
        records = []
        patience = self.patience
        prev_loss, best_loss = float('inf'), float('inf')

        optimizer = getattr(self, f'{model_type}_optimizer')
        lr_scheduler = getattr(self, f'{model_type}_lr_scheduler')

        torch.cuda.empty_cache()

        for epoch in range(1, self.n_epochs + 1):
            start_time, end_time = self.set_times()
            start_time.record()

            train_epoch_loss = self.train_epoch(model, optimizer, model_type)
            valid_epoch_loss = self.valid_epoch(model, model_type)

            epoch_gpu_memory, epoch_gpu_max_memory = self.get_gpu_info()

            end_time.record()
            torch.cuda.synchronize()
            elapsed_time = start_time.elapsed_time(end_time) // 1000

            epoch_record = {
                'epoch': epoch,
                'epoch_time': elapsed_time,
                'train_loss': train_epoch_loss,
                'valid_loss': valid_epoch_loss,
                'learning_rate': optimizer.param_groups[0]['lr'],
                'gpu_memory': epoch_gpu_memory,
                'gpu_max_memory': epoch_gpu_max_memory
            }

            records.append(epoch_record)
            self.print_epoch(epoch_record, model_type)
            lr_scheduler.step(valid_epoch_loss)

            if best_loss > valid_epoch_loss:
                best_loss = valid_epoch_loss
                torch.save({'epoch': epoch,
                            'model_state_dict': model.state_dict(),
                            'optimizer_state_dict': optimizer.state_dict()},
                            getattr(self, f'{model_type}_ckpt'))

            if self.early_stop:
                if prev_loss > valid_epoch_loss:
                    patience = self.patience
                else:
                    patience -= 1
                    if not patience:
                        print('--- Training Early Stopped ---\n')
                        break

                prev_loss = valid_epoch_loss

        with open(getattr(self, f'{model_type}_record_path'), 'w') as fp:
            json.dump(records, fp)


    def train_epoch(self, model, optimizer, model_type):
        model.train()
        tot_len = len(self.train_dataloader)
        epoch_loss = 0

        for idx, batch in enumerate(self.train_dataloader):
            idx += 1
            inputs = self.batch2inputs(batch, model_type)

            with torch.autocast(device_type=self.device_type, dtype=torch.float16):
                loss = model(**inputs).loss
                loss = loss / self.iters_to_accumulate

            #Backward Loss
            self.scaler.scale(loss).backward()

            if (idx % self.iters_to_accumulate == 0) or (idx == tot_len):
                #Gradient Clipping
                self.scaler.unscale_(optimizer)
                nn.utils.clip_grad_norm_(model.parameters(), max_norm=self.clip)

                #Gradient Update & Scaler Update
                self.scaler.step(optimizer)
                self.scaler.update()
                optimizer.zero_grad()

            epoch_loss += loss.item()

        return round(epoch_loss / tot_len, 3)



    def valid_epoch(self, model, model_type):
        model.eval()
        epoch_loss = 0

        with torch.no_grad():
            for batch in self.valid_dataloader:
                inputs = self.batch2inputs(batch, model_type)

                with torch.autocast(device_type=self.device_type, dtype=torch.float16):
                    loss = model(**inputs).loss
                    epoch_loss += loss.item()

        return round(epoch_loss / len(self.valid_dataloader), 3)</code></pre>
              </div>
            </div>            
          </li>


          <div class="spacer"></div>
          <li>SeqGAN Train
            <p>&nbsp; 일반적인 SeqGAN에서는 생성과정을 병행하는 학습을 진행하지만, Transformer의 경우 생성을 병행하게 되면서 병렬처리라는 큰 이점을 잃게 됩니다. 이를 보완하기 위해 일반적인 병렬학습과정과 동일하게 Logit을 생성하고 이 Logit에 대한 GAN을 진행합니다. 자세한 코드 구현은 아래와 같습니다.
            </p>
            <div class="code-container">
              <div class="code-snippet">  
                <pre><code class="python">class Trainer(TrainerBase):
    def __init__(self, config, generator, discriminator, train_dataloader, valid_dataloader):
        super(Trainer, self).__init__(config, generator, discriminator, train_dataloader, valid_dataloader)

        self.sigmoid = nn.Sigmoid()
        self.set_training_attrs(generator, 'gen')
        self.set_training_attrs(discriminator, 'dis')
        self.record_path = f"ckpt/{self.task}/gan_repord.json"


    def print_epoch(self, epoch_report):
        gen_train_loss = f"{epoch_report['gen_train_loss']:.3f}"
        gen_valid_loss = f"{epoch_report['gen_valid_loss']:.3f}"

        dis_train_loss = f"{epoch_report['dis_train_loss']:.3f}"
        dis_valid_loss = f"{epoch_report['dis_valid_loss']:.3f}"

        gpu_memory = epoch_report['gpu_memory']
        max_memory = epoch_report['gpu_max_memory']

        max_len = max([len(f"{x:.3f}") if isinstance(x, (float)) else len(str(x)) for x in epoch_report.values()])

        elapsed_time = epoch_report['epoch_time']
        elapsed_min = int(elapsed_time / 60)
        elapsed_sec = int(elapsed_time - (elapsed_min * 60))

        txt = f"""Epoch {epoch_report['epoch']}/{self.n_epochs} | Time: {elapsed_min}m {elapsed_sec}s
            >> Gen Train Loss: {gen_train_loss:>{max_len}}  |  Gen  Valid Loss: {gen_valid_loss:>{max_len}}
            >> Dis Train Loss: {dis_train_loss:>{max_len}}  |  Dis  Valid Loss: {dis_valid_loss:>{max_len}}            
            >> GPU Memory:     {gpu_memory:>{max_len}}  |  Max Memory:      {max_memory:>{max_len}}\n"""
        print(txt.replace(' '* 11, ''))


    def get_losses(self, batch):
        gen_inputs = self.batch2inputs(batch, 'gen')
        dis_inputs = self.batch2inputs(batch, 'dis')


        dis_loss = self.discriminator(**dis_inputs).loss
        samples = self.generator(**gen_inputs).logit.argmax(-1)
    
        logit = self.sigmoid(self.discriminator(samples))
        prob = (logit > 0.5).sum() / logit.size(0)
        prob = torch.clamp(prob, 1e-4, 0.999)
        gen_loss = -torch.log(prob)
        gen_loss.requires_grad_(True)

        return gen_loss, dis_loss


    def train(self):
        records = []

        torch.cuda.empty_cache()
        for epoch in range(1, self.n_epochs + 1):
            start_time, end_time = self.set_times()
            start_time.record()

            gen_train_epoch_loss, dis_train_epoch_loss = self.train_epoch()
            gen_valid_epoch_loss, dis_valid_epoch_loss = self.valid_epoch() 

            epoch_gpu_memory, epoch_gpu_max_memory = self.get_gpu_info()

            end_time.record()
            torch.cuda.synchronize()
            elapsed_time = start_time.elapsed_time(end_time) // 1000

            epoch_record = {
                #common
                'epoch': epoch,
                'epoch_time': elapsed_time,
                'gpu_memory': epoch_gpu_memory,
                'gpu_max_memory': epoch_gpu_max_memory,

                #generator                
                'gen_train_loss': gen_train_epoch_loss,
                'gen_valid_loss': gen_valid_epoch_loss,
                'gen_lr': self.gen_optimizer.param_groups[0]['lr'],

                #discriminator
                'dis_train_loss': dis_train_epoch_loss,
                'dis_valid_loss': dis_valid_epoch_loss,                
                'dis_lr': self.dis_optimizer.param_groups[0]['lr'],
            }

            records.append(epoch_record)
            self.print_epoch(epoch_record)
            
            #lr scheduler update
            self.gen_lr_scheduler.step(gen_valid_epoch_loss)
            self.dis_lr_scheduler.step(dis_valid_epoch_loss)


        torch.save({'model_state_dict': self.generator.state_dict(),
                    'optimizer_state_dict': self.gen_optimizer.state_dict()},
                    self.gen_ckpt)
        torch.save({'model_state_dict': self.discriminator.state_dict(),
                    'optimizer_state_dict': self.dis_optimizer.state_dict()},
                    self.dis_ckpt)

        with open(self.record_path, 'w') as fp:
            json.dump(records, fp)



    def set_model_mode(self, mode):
        if mode == 'train':
            self.generator.train()
            self.discriminator.train()
        else:
            self.generator.eval()
            self.discriminator.eval()


    def train_epoch(self):
        self.set_model_mode('train')
        tot_len = len(self.train_dataloader)
        gen_epoch_loss, dis_epoch_loss = 0, 0

        for batch in self.train_dataloader:
            
            gen_loss, dis_loss = self.get_losses(batch)

            gen_loss.backward()
            dis_loss.backward()

            #Gradient Clipping
            nn.utils.clip_grad_norm_(self.generator.parameters(), max_norm=self.clip)
            nn.utils.clip_grad_norm_(self.discriminator.parameters(), max_norm=self.clip)

            #Gradient Update & Scaler Update
            self.gen_optimizer.step()
            self.dis_optimizer.step()

            self.gen_optimizer.zero_grad()
            self.dis_optimizer.zero_grad()

            gen_epoch_loss += gen_loss.item()
            dis_epoch_loss += dis_loss.item()

        return round(gen_epoch_loss / tot_len, 3), round(dis_epoch_loss / tot_len, 3)


    def valid_epoch(self):
        self.set_model_mode('eval')
        tot_len = len(self.valid_dataloader)
        gen_epoch_loss, dis_epoch_loss = 0, 0

        with torch.no_grad():
            for batch in self.valid_dataloader:
                gen_loss, dis_loss = self.get_losses(batch)

                gen_epoch_loss += gen_loss.item()
                dis_epoch_loss += dis_loss.item()

        return round(gen_epoch_loss / tot_len, 3), round(dis_epoch_loss / tot_len, 3)</code></pre>
              </div>
            </div>            
          </li>

        </ul>          
      </div>
    </div>


<!-- 3. Experimental Setup -->
    <h2 id="3">&hairsp; 3. &hairsp; Experimental Setup</h2>
    <div class="chapter-box">

      <div class="list-container">
        <ul>
          <li>Data Setup
            <div class="dual-container">
              <ul>
                <li>Translation Task: &nbsp; WMT'14 En-De</li>
                <li>Dialogue Task: &nbsp; Daily Dialogue</li>
                <li>Tokenizer: &nbsp; BPE Tokenizer</li>
                <li>Vocab Size: &nbsp; 15,000</li>
              </ul>
              <ul>
                <li>Train Data Volumn: &nbsp; 50,000</li>
                <li>Train Data Volumn for Augment Training: &nbsp; 50,000</li>
                <li>Valid Data Volumn: &nbsp; 5,000</li>
                <li>Test Data Volumn: &nbsp; 100</li>
                                
              </ul>
            </div>
          </li>

          <div class="half-spacer"></div>
          <li>Generator Setup
            <div class="dual-container">
              <ul>
                <li>Architecture: &nbsp; Transformer Seq2Seq</li>
                <li>Input Dim: &nbsp; 15,000</li>
                <li>Output Dim: &nbsp; 15,000</li>
              </ul>
              <ul>
                <li>Hidden Dim: &nbsp; 512</li>
                <li>Pff Dim: &nbsp; 512</li>
              </ul>
            </div>
          </li>

          <div class="half-spacer"></div>
          <li>Discriminator Setup
            <div class="dual-container">
              <ul>
                <li>Architecture: &nbsp; Transformer Binary Classification</li>
                <li>Input Dim: &nbsp; 15,000</li>
                <li>Output Dim: &nbsp; 1</li>
              </ul>
              <ul>
                <li>Hidden Dim: &nbsp; 512</li>
                <li>PFf Dim: &nbsp; 512</li>
              </ul>
            </div>
          </li>


          <div class="half-spacer"></div>
          <li>Training Setup
            <div class="dual-container">
              <ul>
                <li>Num Epochs: &nbsp; 10</li>
                <li>Batch Size: &nbsp; 32</li>
                <li>Learning Rate: &nbsp; 5e-4</li>
                <li>LR Scheduler: &nbsp; pleature</li>
              </ul>
              <ul>
                <li>Optimizer: &nbsp; AdamW</li>
                <li>Gradient Accumulation Steps: &nbsp; 4
                <li>Teacher Forcing Ratio: &nbsp; 0.5</li>                
              </ul>
            </div>
          </li>
        </ul>
      </div>
    </div>


<!-- 4. Result -->
    <h2 id="4">4. &hairsp; Result</h2>
    <div class="chapter-box">

      <div class="list-container">
        <ul>
          <li>Machine Translation</li>

            <table class="result-table">
              <thead>
                <tr>
                  <th>Training Type</th>
                  <th>Score</th>
                  <th>Epoch Time</th>
                  <th>Avg GPU</th>
                  <th>Max GPU</th>
                </tr>
              </thead>
              <tbody>
                <tr>
                  <td>Baseline</td>
                  <td>3.36</td>
                  <td>0m 11s</td>
                  <td>0.20GB</td>
                  <td>0.85GB</td>
                </tr>
                <tr>
                  <td>SeqGAN</td>
                  <td>0.00</td>
                  <td>4m 40s</td>
                  <td>1.92GB</td>
                  <td>3.86GB</td>
                </tr>

              </tbody>
            </table>
          
          <div class="half-spacer"></div>
          <li>Dialogue Generation</li>
            <table class="result-table">
              <thead>
                <tr>
                  <th>Training Type</th>
                  <th>Score</th>
                  <th>Epoch Time</th>
                  <th>Avg GPU</th>
                  <th>Max GPU</th>
                </tr>
              </thead>
              <tbody>
                <tr>
                  <td>Baseline</td>
                  <td>0.41</td>
                  <td>0m 10s</td>
                  <td>0.20GB</td>
                  <td>0.83GB</td>
                </tr>
                <tr>
                  <td>SeqGAN</td>
                  <td>0.00</td>
                  <td>4m 35s</td>
                  <td>1.88GB</td>
                  <td>3.85GB</td>
                </tr>

              </tbody>
            </table>


          <div class="half-spacer"></div>  
          <li>Result Analysis</li>
            <p> &nbsp; 두 가지 과제에서 모두 SeqGAN 방식이 오히려 전체 학습에 악영향을 미쳤음을 확인할 수 있습니다. 다양한 원인이 있겠지만, 가장 추론 가능한 원인 중하나는 SeqGAN의 Loss함수가 지나치게 Robust해서 모델 파라미터 갱신에 따른 변화를 제대로 반영하지 못한 것이라고 판단됩니다. 
            </p>
        </ul>
      </div>
    </div>



<!-- 5. Conclusion -->
    <h2 id="5">5. &hairsp; Conclusion</h2>
    <div class="chapter-box">

      <p>이 프로젝트에서는 SeqGAN의 구현과, 그 결과까지 살펴봤습니다. 실패에 가까운 실험 결과를 통해, GAN이라는 매력적인 학습법을 시퀀셜 데이터에서 성공적으로 사용하기 위해서는 더 많은 연구가 필요하다는 것을 다시 한번 느낄 수 있었습니다. 
      </p>

    </div>


<!-- Reference -->    
    <h2 id="6">&hairsp; 6. &hairsp; Reference</h2>
    <div class="chapter-box">
      <a class="reference" href="https://arxiv.org/abs/1706.03762">&nbsp; Attention Is All You Need</a> <br>
      <a class="reference" href="https://arxiv.org/abs/1406.2661">&nbsp; Generative Adversarial Networks</a> <br> 
      <a class="reference" href="https://arxiv.org/abs/1609.05473">&nbsp; SeqGAN: Sequence Generative Adversarial Nets with Policy Gradient</a> <br>
    </div>

  </div>
</div>


<div class="pagination">
  <a href="{{ '/gen_train' | relative_url }}" class="btn-prev"><span>Generative Training</span></a>
  <a href="{{ '/cpt_train' | relative_url }}" class="btn-next"><span>Customized Pretraining</span></a>
</div>