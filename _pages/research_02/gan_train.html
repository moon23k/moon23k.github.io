---
layout: default
permalink: /gan_train
---


<div id="detail">
  <h1 class="title">SeqGAN Training for Transformer</h1>
  <div class="post">

    <hr>
    <div class="callout">
      <p class="highlighted-text"><span>Project Objective</span></p>
      <div class="small-spacer"></div>
      <p>&nbsp; 이 프로젝트에서는 생성 능력을 키우기 위한 좋은 학습 방식인 GAN을 시퀀셜 데이터에서 사용하기 위한 방법론인 SeqGAN을 통해 성능이 얼마나 개선될 수 있을지 직접 확인해봅니다. 아키텍처로는 Transformer를 </p>
    </div>

    <div class="tblContents">
      <ul>

        <li>
          <a href="#1">1. &hairsp; Introduction</a>
        </li>

        <li>
          <a href="#2">2. &hairsp; Training Methodology</a>
        </li>

        <li>
          <a href="#3">3. &hairsp; Experimental Setup</a>
        </li>

        <li>
          <a href="#4">4. &hairsp; Result</a>
        </li>

        <li>
          <a href="#5">5. &hairsp; Conclusion</a>
        </li>

        <li>
          <a href="#6">6. &hairsp; Reference</a>
        </li>

      </ul>
    </div>
    <hr>


<!-- 1. Introduction -->    
    <h2 id="1">1. &hairsp; Introduction</h2>
    <div class="chapter-box">

      <div class="list-container">
        <ul>
          <li>Description</li>
            <p> &nbsp; 딥러닝 모델의 학습은 Training Objective를 바탕으로 모델의 파라미터를 최적화 시키는 방향으로 진행됩니다. 일반적으로 NLG Task는 MLE를 Training Objective로 해서 학습됩니다. 좋은 성능을 보이고는 있지만, 그래도 결과값을 생성해보면 그래도 인간이 생성하는 결과값과는 미묘한 차이가 있습니다. 이런 미묘한 차이를 없애고, 더욱 자연스러운 생성을 위해서 SeqGAN을 활용할 수 있습니다. SeqGAN은 GAN의 개념을 시퀀셜 데이터에 적용시키기 위한 기법입니다. 다만 이 기법은 Generative Training을 기반으로 하기에 Transformer에 적용하기는 힘듭니다. 이 프로젝트에서는 Transformer에 적용할수 있도록 SeqGAN의 개념을 차용한 방법론을 제시하고, 실제 실험 후 두 가지 NLG Task의 더욱 자연스럽고 좋은 결과 반환을 위한 추가학습 방법으로써 SeqGAN을 사용해보고, 그 실효성을 검증해봅니다.
            </p>

          <div class="half-spacer"></div>
          <li>Objective
            <ul>
              <li style="font-weight: normal;">RNN, LSTM, GRU 을 활용한 seq2seq 모델링 방식의 이해</li>
              <li style="font-weight: normal;">세 가지 순환 신경망의 성능 비교 검증</li>
              <li style="font-weight: normal;">가설에 대한 검증</li>
              <li style="font-weight: normal;">실험 결과를 통해, 이후 자연어 생성을 위한 모델링 성능의 기준점을 확립</li>
            </ul>
          </li>

        </ul>
      </div>

    </div>



<!-- Training -->    
    <h2 id="1">1. &hairsp; Training Strategy</h2>

      <h3 id="1.1">1.1 &hairsp; GAN</h3>

      <p>&nbsp; GAN은 Generator와 Discriminator가 적대적으로 학습
        Generator: 실제와 유사한 결과값을 창출하려고, Discriminatofrom r를 속일정도록
        Discriminator: Generator의 생성 결과와 실제 정답 레이블 간의 차이를 잘 찾아내도록 학습

        결과적으로 제너레이터는 더욱 정교한 결과물을 만들어 내도록 학습되고,
        제너레이터가 정교해질 수록, 그 미묘한 차이 마저도 포착할 수 있도록 Dicriminator역시 덩달아 정교해집니다.

        덕분에 단순한 Loss Function만으로는 다듬기 힘든 부분까지 모델이 학습할 수 있게 유도합니다.

        여러 방면에서 학습이 매우 효과적임을 입증함.
      </p><hr>

      <div class="half-spacer"></div>
      <h3 id="1.2">1.2 &hairsp; Sequence GAN</h3>

      <div class="dual-container">
          <img src="{{ 'assets/img/research_02/seqgan.png' | relative_url }}" style="width: 350px; height: 150px;">
          <p>
          &nbsp; 앞선 프로젝트에서는 RNN을 기반으로한 Sequence to Sequence 모델 구조의 효용성에 대해 면밀히 살펴봤습니다. 
          하지만 RNN 네트워크는 장기기억력이라는 관점에서 고질적인 문제점을 태생적으로 지니고 있으며, 이를 해결하기 위한 Attention 메커니즘의 활용 역시 기대만큼 좋은 성능을 보이지는 못했습니다.
          </p>
      </div>
      <p>
      &nbsp; 앞선 프로젝트에서는 RNN을 기반으로한 Sequence to Sequence 모델 구조의 효용성에 대해 면밀히 살펴봤습니다. 
      하지만 RNN 네트워크는 장기기억력이라는 관점에서 고질적인 문제점을 태생적으로 지니고 있으며, 이를 해결하기 위한 Attention 메커니즘의 활용 역시 기대만큼 좋은 성능을 보이지는 못했습니다.
      </p>      
      <hr>

      <div class="half-spacer"></div>
      <h3 id="1.3">1.3 &hairsp; SeqGAN for Transformer</h3>

      <p>&nbsp; SeqGAN을 Transformer에 적용하기 위한 다음과 같은 방법론을 제시합니다.

      <hr>

<!-- Code Implementation -->    
    <h2 id="2">2. &hairsp; Code Implementation</h2>

    <h3 id="2.1">2.1 &hairsp; Model</h3>

    <div class="list-container">
      <ul>
        <li>Encoder</li>        
            <div class="code-container">
              <div class="code-snippet">
                <pre><code class="python">class StandardEncoder(nn.Module):
    def __init__(self, config):
        super(StandardEncoder, self).__init__()
        self.embeddings = Embeddings(config)
        layer = nn.TransformerEncoderLayer(
            d_model=config.hidden_dim,
            nhead=config.n_heads,
            dim_feedforward=config.pff_dim,
            dropout=config.dropout_ratio,
            activation='gelu',
            batch_first=True
        )
        self.layers = clones(layer, config.n_layers)

    def forward(self, x, e_mask):
        x = self.embeddings(x)
        for layer in self.layers:
            x = layer(x, src_key_padding_mask=e_mask)
        return x</code></pre>
              </div>
            </div>

      <div class="half-spacer"></div>
        <li>Decoder</li>        
            <div class="code-container">
              <div class="code-snippet">
                <pre><code class="python">class StandardDecoder(nn.Module):
    def __init__(self, config):
        super(StandardDecoder, self).__init__()
        self.embeddings = Embeddings(config)
        layer = nn.TransformerDecoderLayer(
            d_model=config.hidden_dim,
            nhead=config.n_heads,
            dim_feedforward=config.pff_dim,
            dropout=config.dropout_ratio,
            activation='gelu',
            batch_first=True
        )
        self.layers = clones(layer, config.n_layers)

    def forward(self, x, memory, e_mask, d_mask):
        x = self.embeddings(x)
        for layer in self.layers:
            x = layer(x, memory, memory_key_padding_mask=e_mask, tgt_mask=d_mask)
        return x</code></pre>
              </div>
            </div>

      </ul>
    </div>
    <hr>


    <div class="spacer"></div>
    <h3 id="2.2">2.2 &hairsp; Trainer</h3>
    <p>SeqGAN을 구현하기 위해, 우선 일반적인 Train방식으로 선행학습을 하고, 그 다음 GAN Trainer를 통해 SeqGAN 학습을 진행합니다.
    </p>
    <div class="list-container">
      <ul>
        <li>Encoder</li>        
            <div class="code-container">
              <div class="code-snippet">
                <pre><code class="python">class StandardEncoder(nn.Module):
    def __init__(self, config):
        super(StandardEncoder, self).__init__()
        self.embeddings = Embeddings(config)
        layer = nn.TransformerEncoderLayer(
            d_model=config.hidden_dim,
            nhead=config.n_heads,
            dim_feedforward=config.pff_dim,
            dropout=config.dropout_ratio,
            activation='gelu',
            batch_first=True
        )
        self.layers = clones(layer, config.n_layers)

    def forward(self, x, e_mask):
        x = self.embeddings(x)
        for layer in self.layers:
            x = layer(x, src_key_padding_mask=e_mask)
        return x</code></pre>
              </div>
            </div>

      <div class="half-spacer"></div>
        <li>Decoder</li>        
            <div class="code-container">
              <div class="code-snippet">
                <pre><code class="python">class StandardDecoder(nn.Module):
    def __init__(self, config):
        super(StandardDecoder, self).__init__()
        self.embeddings = Embeddings(config)
        layer = nn.TransformerDecoderLayer(
            d_model=config.hidden_dim,
            nhead=config.n_heads,
            dim_feedforward=config.pff_dim,
            dropout=config.dropout_ratio,
            activation='gelu',
            batch_first=True
        )
        self.layers = clones(layer, config.n_layers)

    def forward(self, x, memory, e_mask, d_mask):
        x = self.embeddings(x)
        for layer in self.layers:
            x = layer(x, memory, memory_key_padding_mask=e_mask, tgt_mask=d_mask)
        return x</code></pre>
              </div>
            </div>

      </ul>
    </div>
    <hr>


<!-- Experimental Setup -->    
    <h2 id="3">3. Experimental Setup</h2>
    <div class="small-spacer"></div>
    <table class="result-table">
      <thead>
        <tr>
          <th>Data Setup</th>
          <th>Model Setup</th>
          <th>Training Setup</th>
        </tr>
      </thead>
      <tbody>
        <tr>

          <td>
            <ul>
              <li><b>Translation</b>: &nbsp; WMT'14 En-De</li>
              <li><b>Dialogue</b>: &nbsp; Daily Dialogue</li>
              <li><b>Summarization</b>: &nbsp; CNN Daily</li>
              <li><b>Train Data Volumn</b>: &nbsp; 50,000</li>
              <li><b>Valid Data Volumn</b>: &nbsp; 5,000</li>
              <li><b>Test Data Volumn</b>: &nbsp; 100</li>
              <li><b>Tokenizer</b>: &nbsp; BPE Tokenizer</li>
              <li><b>Vocab Size</b>: &nbsp; 15,000</li>
            </ul>
          </td>

          <td>
            <ul>
              <li><b>Input Dim</b>: &nbsp; 15,000</li>
              <li><b>Output Dim</b>: &nbsp; 15,000</li>
              <li><b>Embedding Dim</b>: &nbsp; 512</li>
              <li><b>Hidden Dim</b>: &nbsp; 512</li>
              <li><b>Num Layers</b>: &nbsp; 6</li>
            </ul>
          </td>

          <td>
            <ul>
              <li><b>Num Epochs</b>: &nbsp; 10</li>
              <li><b>Batch Size</b>: &nbsp; 32</li>
              <li><b>Learning Rate</b>: &nbsp; 5e-4</li>
              <li><b>LR Scheduler</b>: &nbsp; ReduceLROnPlateau</li>
              <li><b>Optimizer</b>: &nbsp; AdamW</li>
              <li><b>Gradient Accumulation Steps</b>: &nbsp; 4</li>
            </ul>
          </td>          
        </tr>

      </tbody>
    </table>
    <div class="half-spacer"></div>
    <hr>


<!-- Result -->    
    <h2>3. Result</h2>
    <div class="small-spacer"></div>
    <table class="result-table">
      
      <thead>
        <tr>
          <th>Attention Strategy</th>
          <th>Machine Translation</th>
          <th>Dialogue Generation</th>
          <th>Text Summarization</th>
        </tr>
      </thead>

      <tbody>
        <tr>
          <td>Vanilla Transformer</td>
          <td>-</td>
          <td>-</td>
          <td>-</td>
        </tr>
      
        <tr>
          <td>Simple Model</td>
          <td>결과2-1</td>
          <td>결과2-2</td>
          <td>결과2-3</td>
        </tr>
      
        <tr>
          <td>Fused Model</td>
          <td>결과2-1</td>
          <td>결과2-2</td>
          <td>결과2-3</td>
        </tr>

      </tbody>
    </table>


<!-- Conclusion -->    
    <h2>4. Conclusion</h2>
    <p>
    </p>
    <hr>


<!-- Reference -->    
    <h2>5. Reference</h2>
    <a class="reference" href="https://arxiv.org/abs/1706.03762">&nbsp; Attention Is All You Need</a> <br>
    <a class="reference" href="https://arxiv.org/abs/1406.2661">&nbsp; Generative Adversarial Networks</a> <br> 
    <a class="reference" href="https://arxiv.org/abs/1609.05473">&nbsp; SeqGAN: Sequence Generative Adversarial Nets with Policy Gradient</a> <br>

  </div>
</div>


<div class="pagination">
  <a href="{{ '/gen_train' | relative_url }}" class="btn-prev"><span>Generative Training</span></a>
  <a href="{{ '/custom_pretrain' | relative_url }}" class="btn-next"><span>Customized Pretraining</span></a>
</div>