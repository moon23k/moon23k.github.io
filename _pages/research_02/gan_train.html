---
layout: default
permalink: /gan_train
---


<div id="detail">
  <h1 class="title">SeqGAN Training for Transformer</h1>
  <div class="post">

    <div class="callout">
      <p class="highlighted-text"><span>Project Objective</span></p>
      <div class="small-spacer"></div>
      <p>&nbsp; RNN은 이전 스텝의 결과값을 다음 스텝의 처리과정에 재귀적으로 반영시키며, 시퀀셜 데이터를 처리하기 용이한 네트워크 구조입니다. 대표적인 RNN Cell 구조로는 RNN, LSTM, GRU
      이 프로젝트에서는 세가지 셀 구조별 Seq2Seq 모델 구조에서 성능이 얼마나 나오는지를 비교 분석해봅니다.</p>
    </div>

    <div class="tblContents">
      <ul>

        <li>
          <a href="#1">1. RNN Cells</a>
          <ul>
            <li>
              <a href="#1.1">1.1 RNN</a>
            </li>
            <li>
              <a href="#1.2">1.2 LSTM</a>
            </li>
            <li>
              <a href="#1.3">1.3 GRU</a>
            </li>          
          </ul>
        </li>
        
        <li>
          <a href="#2">2. Architecure</a>
        </li>

        <li>
          <a href="#3">3. Experimental Setup</a>
        </li>

        <li>
          <a href="#4">4. Results</a>
        </li>

        <li>
          <a href="#5">5. Conclusion</a>
        </li>

        <li>
          <a href="#6">6. Reference</a>
        </li>

      </ul>
    </div>
    <hr>


<!-- Project Desc -->    
    <h2>1. &hairsp; Project Desc</h2>

      <p>
        &nbsp; 현재 다양한 딥러닝 모델들이 다양한 분야에서 좋은 성능을 보여주고 있습니다. 
        성능이 뛰어난, 모델을 사용하기 위해서는 사용자의 의도에 맞는 Down Stream Task Dataset에 맞춰 파인튜닝이 이뤄져야 합니다.
        대규모 모델의 경우, 파인튜닝을 위한 자원도 많이 필요합니다.
        모델의 성능을 포기하는 방법대신, 효율적인 학습방식으로 이 한계를 극복할수 있습니다.
        이 프로젝트에서는 네가지 대표적인 효율적 모델 학습 방법론을 살펴보고, 실제 Sequence Classification Task에서의 효용성을 통해
        각 방법론 별 효용성을 비교 분석해봅니다.
      </p>

      <div class="spacer"></div>
      <h3>Generative Adversarial Network</h3>
      <div class="small-spacer"></div>

      <p>&nbsp; GAN은 Generator와 Discriminator가 적대적으로 학습
        Generator: 실제와 유사한 결과값을 창출하려고, Discriminatofrom r를 속일정도록
        Discriminator: Generator의 생성 결과와 실제 정답 레이블 간의 차이를 잘 찾아내도록 학습

        결과적으로 제너레이터는 더욱 정교한 결과물을 만들어 내도록 학습되고,
        제너레이터가 정교해질 수록, 그 미묘한 차이 마저도 포착할 수 있도록 Dicriminator역시 덩달아 정교해집니다.

        덕분에 단순한 Loss Function만으로는 다듬기 힘든 부분까지 모델이 학습할 수 있게 유도합니다.

        여러 방면에서 학습이 매우 효과적임을 입증함.
      </p>

      <div class="spacer"></div>
      <h3>SeqGAN</h3>
      <div class="small-spacer"></div>

      <p>&nbsp; GAN은 매우 효과적인 학습 방법론입니다.
        하지만 Sequential 한 데이터에는 사용하기 어렵습니다.
        병렬적으로 

        Generative Training 속에 Discriminator를 활용하는 방법도 있기는 하지만, Transformer를 통한 Generative Training은 효율성도 그렇고, 실효성도 그닥이라서.

        차라리 Transformer의 병렬 학습을 최대한 활용하면서, 그 안에서 GAN의 개념을 삽입시키는 방향으로 학습법을 수정.


      </p>

      <div class="spacer"></div>
      <h3>SeqGAN Penalty Training</h3>
      <div class="small-spacer"></div>

      <p>&nbsp; GAN은 매우 효과적인 학습 방법론입니다.
        하지만 Sequential 한 데이터에는 사용하기 어렵습니다.
        병렬적으로 

        Generative Training 속에 Discriminator를 활용하는 방법도 있기는 하지만, Transformer를 통한 Generative Training은 효율성도 그렇고, 실효성도 그닥이라서.

        차라리 Transformer의 병렬 학습을 최대한 활용하면서, 그 안에서 GAN의 개념을 삽입시키는 방향으로 학습법을 수정.


      </p>      

    <h2>2. Training Details</h2>

    <div class="small-spacer"></div>
    <h3>Pre Training</h3>
    <p>처음부터 SeqGAN을 사용하는 것이 아니라, 처음에는 Generator와 Discriminator 각각이 어느 정도의 생성 및 판별 능력을 배양할 수 있도록 사전학습을 시킵니다.

    우선은 Generator를 일반 Transformer의 그것과 동일한 방식으로 학습시킵니다.
    그리고 사전학습된 Generator가 입력값에 반응해서 도출한 결과값과 실제 레이블 시퀀스를 입력값으로 받아, 참거짓을 분류하는 이진 분류 모델로써 Discriminator를 학습시킵니다.

    </p>

    <div class="spacer"></div>
    <h3>Model Setup</h3>
    <p>대표적인 PreTrained Model인 BERT를 사용.
    </p>


<!-- Experimental Setup -->    
    <h2>3. Experimental Setup</h2>

      <div class="small-spacer"></div>
      <h3>Data Setup</h3>
      <p>Sequence Classification을 위한 대표적 데이터 셋 중하나인 AG_News를 사용.
        데이터의 크기는 Train/ Valid/ Test 1000/ 100/ 100으로 설정
        총 네가지 데이터 Label을 모든 데이터에서 고르게 분배해서 한쪽으로 치우치지 않도록 가공
      </p>

      <div class="spacer"></div>
      <h3>Model Setup</h3>
      <p>대표적인 PreTrained Model인 BERT를 사용.
      </p>


      <div class="spacer"></div>
      <h3>Training Setup</h3>
      <div class="small-spacer"></div>
      <p>대표적인 PreTrained Model인 BERT를 사용.
      </p>


<!-- Result -->    
    <h2>3. Result</h2>
    <div class="small-spacer"></div>
    <table class="result-table">
      
      <thead>
        <tr>
          <th>Attention Strategy</th>
          <th>Machine Translation</th>
          <th>Dialogue Generation</th>
          <th>Text Summarization</th>
        </tr>
      </thead>

      <tbody>
        <tr>
          <td>Vanilla Transformer</td>
          <td>-</td>
          <td>-</td>
          <td>-</td>
        </tr>
      
        <tr>
          <td>Simple Model</td>
          <td>결과2-1</td>
          <td>결과2-2</td>
          <td>결과2-3</td>
        </tr>
      
        <tr>
          <td>Fused Model</td>
          <td>결과2-1</td>
          <td>결과2-2</td>
          <td>결과2-3</td>
        </tr>

      </tbody>
    </table>


<!-- Conclusion -->    
    <h2>4. Conclusion</h2>



<!-- Reference -->    
    <h2>5. Reference</h2>




  </div>
</div>


<div class="pagination">
  <a href="{{ '/transformer_variants' | relative_url }}" class="btn-prev"></a>
  <a href="{{ '/eff_train' | relative_url }}" class="btn-next"></a>
</div>