---
layout: default
permalink: /peft
---


<div id="detail">
  <h1 class="title">Parameter Efficient Fine Tuning Analystics</h1>  
  <div class="post">


    <h2>1. &hairsp; Project Desc</h2>

      <div class="half-spacer"></div>
      <h3>Parameter Efficient Fine Tuning</h3>
      <div class="small-spacer"></div>
      <p>
        &nbsp; 대규모 사전학습 언어 모델(Large Language Models, LLM)들이 다양한 분야에서 우수한 성능을 보이고 있습니다. 
        일반적으로 LLM은 사용자가 원하는 Down Stream Task에 맞게 Fine Tuning하는 방식을 채택하지만, 대규모 모델의 파라미터를 전부 업데이트 하는 것은 비용과 시간이 많이 소요되는 일입니다. 
        이러한 도전에 대응하여, 파라미터의 효율적인 업데이트에 중점을 둔 새로운 방법론이 등장했는데, 그것이 바로 PEFT(Parameter Efficient Fine Tuning)입니다. PEFT는 모델을 고도로 튜닝하면서도 파라미터 업데이트를 효율적으로 수행함으로써 리소스를 절약하는 방법을 제공합니다.
        이 프로젝트에서는 PEFT를 적용한 총 5가지 방식을 직접 실험해보고, 메모리와 시간 측면에서의 효율성과 더불어 실제 NMT(Neural Machine Translation) 작업에서의 성능을 비교 분석하려고 합니다. 각각의 PEFT 방식을 구체적으로 살펴보면서 어떻게 모델의 성능 향상과 효율적인 파라미터 업데이트가 이루어지는지 확인해 보겠습니다.
      </p>

      <div class="spacer"></div>
      <h3>Prompt Tuning</h3>
      <div class="small-spacer"></div>
      <p>
        &nbsp; prompt tuning
      </p>


      <div class="spacer"></div>
      <h3>Prefix Tuning</h3>
      <div class="small-spacer"></div>
      <p>prefix tuning
      </p>


      <div class="spacer"></div>
      <h3>P Tuning</h3>
      <div class="small-spacer"></div>
      <p>prefix tuning
      </p>


      <div class="spacer"></div>
      <h3>LoRA</h3>
      <div class="small-spacer"></div>
      <p>prefix tuning
      </p>


      <div class="spacer"></div>
      <h3>IA3</h3>
      <div class="small-spacer"></div>
      <p>prefix tuning
      </p>


    <h2>2. &hairsp; Experimental Setup</h2>

      <div class="small-spacer"></div>
      <h3>Data Setup</h3>
      <div class="small-spacer"></div>
      <p>Sequence Classification을 위한 대표적 데이터 셋 중하나인 AG_News를 사용.
        데이터의 크기는 Train/ Valid/ Test 1000/ 100/ 100으로 설정
        총 네가지 데이터 Label을 모든 데이터에서 고르게 분배해서 한쪽으로 치우치지 않도록 가공
      </p>

      <div class="spacer"></div>
      <h3>Model Setup</h3>
      <div class="small-spacer"></div>
      <p>대표적인 PreTrained Model인 BERT를 사용.
      </p>


      <div class="spacer"></div>
      <h3>Training Setup</h3>
      <div class="small-spacer"></div>

      <div class="code-container">
        <div class="code-snippet">
          <pre><code class="python">
TrainingArguments(
        output_dir= f'ckpt/{strategy}',
        num_train_epochs= 5,
        learning_rate= 1e-5,
        per_device_train_batch_size= 32,
        per_device_eval_batch_size= 32,
        lr_scheduler_type='reduce_lr_on_plateau',
        load_best_model_at_end= True,

        save_strategy= 'epoch',
        logging_strategy= 'epoch',
        evaluation_strategy= 'epoch',

        fp16= True if config.strategy in ['fp16', 'all'] else False,
        fp16_opt_level= '02' if config.strategy in ['fp16', 'all'] else '01',
        gradient_accumulation_steps = True if config.strategy in ['grad_accumulation', 'all'] else 4,
        gradient_checkpointing= True if config.strategy in ['grad_checkpointing', 'all'] else False,
        optim = 'adafactor' if config.strategy in ['optim', 'all'] else 'adamw_torch'
)
          </code></pre>
        </div>
      </div>



    <h2>3. &hairsp; Result</h2>
    <div class="small-spacer"></div>
    <table class="result-table">
      
      <thead>
        <tr>
          <th>PEFT Strategy</th>
          <th>Train Time</th>
          <th>GPU Occupation</th>
          <th>Accuracy</th>
        </tr>
      </thead>

      <tbody>
        <tr>
          <td>Baseline</td>
          <td>174</td>
          <td>7.00</td>
          <td>79</td>
        </tr>

        <tr>
          <td>Prompt Tuning</td>
          <td>69</td>
          <td>5.47</td>
          <td>78</td>
        </tr>
        <tr>
          <td>Prefix Tuning</td>
          <td>182</td>
          <td>6.29</td>
          <td>83</td>
        </tr>

        <tr>
          <td>P Tuning</td>
          <td>239</td>
          <td>3.54</td>
          <td>79</td>
        </tr>

        <tr>
          <td>IA3</td>
          <td>179</td>
          <td>6.72</td>
          <td>79</td>
        </tr>

      </tbody>
    </table>


    <h2>4. &hairsp; Conclusion</h2>
    <div class="small-spacer"></div>
    <p> &nbsp; 균형이 잘 맞는 경우가 가장 이상적. 불균형은 자연스레 성능 하락을 야기
    </p>  


  </div>
</div>

<div class="pagination">
  <a href="{{ '/eff_model' | relative_url }}" class="btn-prev"></a>
  <a href="{{ '/aux_train' | relative_url }}" class="btn-next"></a>
</div>