---
layout: default
permalink: /scheduled_sampling
---


<div id="detail">
  <h1 class="title">Scheduled Sampling for Transformer</h1>  
  <div class="post">


    <div class="callout">
      <p class="highlighted-text"><span>Project Objective</span></p>
      <div class="small-spacer"></div>
      <p>&nbsp; RNN은 이전 스텝의 결과값을 다음 스텝의 처리과정에 재귀적으로 반영시키며, 시퀀셜 데이터를 처리하기 용이한 네트워크 구조입니다. 대표적인 RNN Cell 구조로는 RNN, LSTM, GRU
      이 프로젝트에서는 세가지 셀 구조별 Seq2Seq 모델 구조에서 성능이 얼마나 나오는지를 비교 분석해봅니다.</p>
    </div>

    <div class="tblContents">
      <ul>

        <li>
          <a href="#1">1. RNN Cells</a>
          <ul>
            <li>
              <a href="#1.1">1.1 RNN</a>
            </li>
            <li>
              <a href="#1.2">1.2 LSTM</a>
            </li>
            <li>
              <a href="#1.3">1.3 GRU</a>
            </li>          
          </ul>
        </li>
        
        <li>
          <a href="#2">2. Architecure</a>
        </li>

        <li>
          <a href="#3">3. Experimental Setup</a>
        </li>

        <li>
          <a href="#4">4. Results</a>
        </li>

        <li>
          <a href="#5">5. Conclusion</a>
        </li>

        <li>
          <a href="#6">6. Reference</a>
        </li>

      </ul>
    </div>
    <hr>


<!-- Project Desc -->    
    <h2>1. &hairsp; Project Desc</h2>

      <div class="half-spacer"></div>
      <h3>Exposure Bias</h3>
      <div class="small-spacer"></div>

      <p>
        &nbsp; RNN에서는 Exposure Bias를 완화하기 위한 방법으로 Scheduled Sampling이라는 기법을 사용합니다.
        이는 Teacher Forcing에 지나치게 의존하면서, Exposure Bias 문제가 커지는 것을 방지하기 위한 하나의 방식입니다.

        RNN에서 Scheduled Sampling의 효과는 입증되었습니다. RNN에서는 순차적으로 데이터를 처리하기에, Scheduled Sampling을 적용하는 것이 직관적인 반면.
        하지만 Transformer에서는 병렬적으로 데이터를 처리하기 때문에 Scheduled Sampling의 적용이 직관적이지 않습니다.

        이번 프로젝트에서는 "Scheduled Sampling for Transformers" 논문에서 소개된 방식을 적용해보며, Scheduled Sampling 방식의 효용성에 대해 검증해봅니다.
      </p>

      <div class="spacer"></div>
      <h3>Teacher Forcing Training</h3>
      <div class="small-spacer"></div>

      <p>
        &nbsp; 현재 다양한 딥러닝 모델들이 다양한 분야에서 좋은 성능을 보여주고 있습니다. 
        성능이 뛰어난, 모델을 사용하기 위해서는 사용자의 의도에 맞는 Down Stream Task Dataset에 맞춰 파인튜닝이 이뤄져야 합니다.
        대규모 모델의 경우, 파인튜닝을 위한 자원도 많이 필요합니다.
        모델의 성능을 포기하는 방법대신, 효율적인 학습방식으로 이 한계를 극복할수 있습니다.
        이 프로젝트에서는 네가지 대표적인 효율적 모델 학습 방법론을 살펴보고, 실제 Sequence Classification Task에서의 효용성을 통해
        각 방법론 별 효용성을 비교 분석해봅니다.
      </p>

      <div class="spacer"></div>
      <h3>Scheduled Sampling for Transformers</h3>
      <div class="small-spacer"></div>

      <div class="img-container">
        <img src="{{ 'assets/img/research_02/scheduled_sampling.png' | relative_url }}" style="width: 350px; height: 320px; display: block; margin: 0 1em 0 0;">
        <p>&nbsp; RNN에서 활용하던 Scheduled Sampling방식과 달리, Transformer를 위한 Scheduled Sampling에서는 모델의 병렬처리 된 logit값을 토큰으로 변환해, 사용자가 설정한 sampling ratio만큼 모델의 출력값과 golden label을 섞어 새로운 디코더의 입력값을 세팅합니다. 이후 
        </p>        
      </div>


<!-- Experimental Setup -->    
    <h2>2. Experimental Setup</h2>

      <div class="small-spacer"></div>
      <h3>Data Setup</h3>
      <p>Sequence Classification을 위한 대표적 데이터 셋 중하나인 AG_News를 사용.
        데이터의 크기는 Train/ Valid/ Test 1000/ 100/ 100으로 설정
        총 네가지 데이터 Label을 모든 데이터에서 고르게 분배해서 한쪽으로 치우치지 않도록 가공
      </p>

      <div class="spacer"></div>
      <h3>Model Setup</h3>
      <p>대표적인 PreTrained Model인 BERT를 사용.
      </p>


      <div class="spacer"></div>
      <h3>Training Setup</h3>
      <div class="small-spacer"></div>
      <p>&nbsp; 
      </p>



<!-- Result -->    
    <h2>3. Result</h2>
    <div class="small-spacer"></div>
    <table class="result-table">
      
      <thead>
        <tr>
          <th>Sampling Ratio</th>
          <th>Machine Translation</th>
          <th>Dialogue Generation</th>
          <th>Text Summarization</th>
        </tr>
      </thead>

      <tbody>
        <tr>
          <td>0.0</td>
          <td>-</td>
          <td>-</td>
          <td>-</td>
        </tr>
      
        <tr>
          <td>0.1</td>
          <td>결과2-1</td>
          <td>결과2-2</td>
          <td>결과2-3</td>
        </tr>
      
        <tr>
          <td>0.3</td>
          <td>결과2-1</td>
          <td>결과2-2</td>
          <td>결과2-3</td>
        </tr>

        <tr>
          <td>0.5</td>
          <td>결과2-1</td>
          <td>결과2-2</td>
          <td>결과2-3</td>
        </tr>

        <tr>
          <td>Crescendo</td>
          <td>결과2-1</td>
          <td>결과2-2</td>
          <td>결과2-3</td>
        </tr>

      </tbody>
    </table>


<!-- Conclusion -->    
    <h2>4. Conclusion</h2>



<!-- Reference -->    
    <h2>5. Reference</h2>




  </div>
</div>


<div class="pagination">
  <a href="{{ '/transformer_variants' | relative_url }}" class="btn-prev"></a>
  <a href="{{ '/eff_train' | relative_url }}" class="btn-next"></a>
</div>