---
layout: default
permalink: /eff_train
---

<div id="detail">
  <h1 class="title">Efficient&hairsp; Training&hairsp; Methodology&hairsp; Analytics</h1>  
  <div class="post">

    <hr>
    <div class="callout">
      <p class="highlighted-text"><span>Project Objective</span></p>
      <div class="small-spacer"></div>
      <p>&nbsp; 이 프로젝트에서는 효율적인 사전학습 모델들의 효율성과 성능을 비교합니다. 총 두가지의 모델군으로 나누어 비교를 진행합니다.
        첫번째 모델군에는 파라미터의 수를 줄여 경량화 함으로써 효율성을 제고시킨 모델들을. 두번째 모델군은 어텐션 연산과정에서 효율성을 증대시킨 모델들.
        모든 모델군의 Baseline 모델로는 BERT를 설정
        첫번째 모델군에

      사전학습된 모델이 좋은 성능을 보인다는 것은 다양한 방면에 걸쳐 입증되어 왔습니다. 일반적으로 더 큰 모델일수록 더 좋은 성능을 보이고는 있지만, 그 만큼 많은 컴퓨팅자원을 필요로합니다. 때문에 모델의 성능을 최대한 유지하면서도 모델의 크기를 줄이거나, 연산을 단순화 하는 등의 효율성을 제고시키기 위한 모델들이 제시되었습니다. 하지만 이러한 효율적 모델들을 동일한 조건에서 비교 검증해보는 연구는 부재합니다. 이런 문제를 해결하고, 효율성의 측면에서 다양한 사전학습 모델들을 검증해봅니다.</p>
    </div>

    <div class="tblContents">
      <ul>

        <li>
          <a href="#1">&hairsp; 1. &hairsp; Introduction</a>
        </li>

        <li>
          <a href="#2">&hairsp; 2. &hairsp; Background</a>
        </li>

        <li>
          <a href="#3">&hairsp; 3. &hairsp; Training Strategy</a>
        </li>

        <li>
          <a href="#4">&hairsp; 4. &hairsp; Experimental Setup</a>
        </li>

        <li>
          <a href="#5">&hairsp; 5. &hairsp; Results</a>
        </li>

        <li>
          <a href="#6">&hairsp; 6. &hairsp; Conclusion</a>
        </li>

        <li>
          <a href="#7">&hairsp; 7. &hairsp; Reference</a>
        </li>

      </ul>
    </div>
    <hr>




<!-- 1. Introduction -->    
    <h2 id="1">&hairsp; 1. &hairsp; Introduction</h2>
    <div class="chapter-box">

      <div class="list-container">
        <ul>
          <li>Description</li>
            <p> &nbsp; 현재 다양한 대규모 사전학습 언어 모델들이 자연어 처리 분야에서 뛰어난 활약을 보이고 있습니다. 하지만 모델이 과도하게 크기 때문에 일반적인 컴퓨팅 자원에서 학습 및 추론이 어렵다는 단점이 존재. 
              비단 커지고, 고성능 만을 쫒는 연구뿐만아니라, 효율적인 모델을 향한 다양한 연구 역시 진행되고 있음. 대표적인 효율적 모델 연구는 두 갈래로 나누어볼 수 있음. 모델의 파라미터 개수를 감소시키는 방향과, 핵심연산인 Attention의 \( O^2 \) 복잡성을 개선시키는 방향의 연구.
              첫번째 연구를 Parameter-wise Efficient Model, 두번째는 Attention-wise Efficient Model이라고 명명하고, 각 연구에 부합하는 모델들을
              <br>
              파라미터 효율적 모델들로는 Distil BERT, AlBERT, Mobile BERT
              어텐션 효율적 모델로는 Longformer, BigBird
              <br>
              두 모델 군의 비교의 근거가 되는 Baseline Model로는 모두 BERT를 사용.
              <br>
              데이터를 굳이 두개로 쪼개서 사용할 이유가 없을듯 ... ㅇㅇㅇ 이거는 코드 구현을 위한 참고!

            </p>

          <div class="spacer"></div>
          <li>Objective
            <ul>
              <li style="font-weight: normal;">효율성 증진과 크로스 오버로 발생가능한 성능 하락 사이 </li>
              <li style="font-weight: normal;">다양한 구현 방식에 따른 Transformer의 성능 차이를 직접 확인</li>
              <li style="font-weight: normal;">Transformer의 자연어 생성 능력 측정을 통해 Transformer BaseLine 확립</li>
              <li style="font-weight: normal;">직접 구현한 Transformer와 Pytorch에서 제공하는 Transformer모델의 성능 비교</li>
            </ul>
          </li>

        </ul>
      </div>

    </div>



<!-- 2. Background -->    
    <h2 id="2">&hairsp; 2. &hairsp; Background</h2>
    <div class="chapter-box">
      <div class="list-container">
        <ul>

          <li>Pretrained Language Model
            <ul>
              <li>대규모 사전학습 언어 모델은 언어 이해의 혁신을 이끌고 있습니다. 이 모델들은 방대한 양의 텍스트 데이터를 학습하여 언어의 다양한 측면을 이해하고, 문맥을 파악하는 데에 뛰어난 성과를 보입니다. 이로써 자연어 처리, 기계 번역, 질문 응답 시스템 등 다양한 언어 관련 작업에서 높은 성능을 보이고 있습니다.</li>
              <li>대규모 사전학습 언어 모델은 다양한 도메인에서 적용 가능성이 높습니다. 의학, 법률, 공학, 예술 등 다양한 분야에서 특정 전문 용어나 문맥을 이해하고 처리할 수 있어, 효율적인 작업 수행이 가능합니다. 이는 기존의 작은 규모 모델로는 어려웠던 다양한 분야에서의 자연어 이해 능력을 혁신적으로 개선하고 있습니다.</li>
              <li>대규모 사전학습 언어 모델은 창의적인 활용과 새로운 애플리케이션 개발을 촉진하고 있습니다. 예를 들어, AI 기반의 문학 창작, 음악 작곡, 예술 창작 등에서도 높은 수준의 창의성을 발휘하며 새로운 가능성을 열어가고 있습니다.</li>
            </ul>
          </li>
            
          <div class="spacer"></div>
          <li>Limitation
            <ul>
              <li>대규모 사전학습 언어 모델은 엄청난 양의 매개변수를 가지고 있어 연산 비용이 상당히 높습니다. 이로 인해 모델을 학습하고 사용하는 데에 많은 컴퓨팅 자원이 필요하며, 이는 비용 측면에서 부담이 될 수 있습니다.</li>
              <li>대규모 모델의 사용은 주로 대규모 기업이나 연구기관에 제한되어 있는 경우가 많습니다. 작은 기업이나 연구팀은 이러한 모델을 사용하기 어렵거나 비용 문제로 제약을 받을 수 있어, 기술의 불균형적인 접근이 우려되고 있습니다.</li>
              <li>모델의 경량화, 연산 효율성 증대와 같은 모델 측면의 효율성 제고를 위한 연구뿐만 아니라, 학습과정에서의 효율성 증대를 위한 연구도 필수적</li>
            </ul>
          </li>

        </ul>
      </div>
    </div>



<!-- 2. Background -->    
    <h2 id="3">&hairsp; 3. &hairsp; Training Strategy</h2>
    <div class="chapter-box">
      <div class="list-container">
        <ul>

          <li>Mixed Precision Training
            <ul>
              <li>Mixed Precision Training은 모델 훈련의 계산 효율성을 최적화하기 위해 특정 변수에 대해 낮은 정밀도 숫자 형식을 활용하는 기술입니다.</li>
              <li>일반적으로 대부분의 모델은 변수를 표현하고 처리하기 위해 32비트 부동 소수점 정밀도(float32)를 사용합니다. 그러나 모든 변수가 정확한 결과를 얻기 위해 이 높은 정밀도 수준이 필요한 것은 아닙니다.</li>
              <li>특정 변수의 정밀도를 16비트 부동 소수점(float16)과 같은 낮은 숫자 형식으로 감소시킴으로써 계산 속도를 높일 수 있습니다. 이 방식에서는 일부 계산이 반 정밀도에서 수행되고 일부는 여전히 전 정밀도에서 수행합니다.</li>
            </ul>
          </li>
            
          <div class="spacer"></div>
          <li>Gradient Accumulation
            <ul>
              <li>Gradient Accumulation은 전체 배치를 한 번에 계산하는 대신 작은 증분으로 기울기를 계산하려는 방법입니다.</li>
              <li>이 방식은 모델을 전방향 및 역방향 패스를 수행하여 작은 배치에서 반복적으로 기울기를 계산하고 이를 누적시키는 것을 포함합니다.</li>
              <li>충분한 기울기가 누적되면 모델의 최적화 단계가 실행됩니다.</li>
              <li>경사 누적을 사용하면 GPU 메모리 용량의 제한을 초과하여 효과적인 배치 크기를 증가시킬 수 있습니다.</li>
            </ul>
          </li>

          <div class="spacer"></div>
          <li>Gradient Checkpointing
            <ul>
              <li>일부 대형 모델은 배치 크기를 1로 설정하고 경사 누적을 사용해도 메모리 문제에 직면할 수 있습니다. 이는 메모리 저장 공간이 필요한 다른 구성 요소들도 있기 때문입니다.</li>
              <li>전방향 패스에서 모든 활성화를 저장하여 역방향 패스에서 그래디언트를 계산하는 것은 상당한 메모리 오버헤드를 초래할 수 있습니다. 활성화를 버리고 역방향 패스에서 필요할 때 다시 계산하는 대안적인 접근은 상당한 계산 오버헤드를 도입하고 훈련 프로세스를 늦출 수 있습니다.</li>
              <li>경사 체크포인팅은 이 두 접근 사이의 타협점을 제공하며 계산 그래프 전체에서 전략적으로 선택된 활성화만 저장하여 그래디언트를 계산할 때 일부 활성화만 다시 계산하면 됩니다.</li>
            </ul>
          </li>

          <div class="spacer"></div>
          <li>Efficient Optimizer
            <ul>
              <li>트랜스포머 모델을 훈련하는 데 가장 일반적으로 사용되는 Adam 옵티마이저는 이전 기울기의 이동 평균을 저장하여 좋은 수렴을 달성합니다. 그러나 이로 인해 모델 매개변수의 수만큼의 추가 메모리 공간이 필요합니다. </li>
              <li>이를 해결하기 위해 대안적인 옵티마이저를 사용할 수 있으며, 대표적인 대안책이 Adafactor 옵티마이저입니다</li>
              <li>Adafactor는 가중치 행렬의 각 요소에 대해 이동 평균을 저장하지 않습니다. 대신, 집약된 정보를 유지함으로써 연산 효율성을 개선합니다. 그러나 Adam과 비교할 때 Adafactor는 특정 경우에 느린 수렴 속도를 가질 수 있습니다.</li>
            </ul>
          </li>

        </ul>
      </div>
    </div>



<!-- 1. Introduction -->    
    <h2>&hairsp; 4. &hairsp; Experimental Setup</h2>
    <div class="chapter-box">

      <div class="list-container">
        <ul>
          <li>Data Setup
            <div class="dual-container">
              <ul>
                <li><b>Translation Task</b>: &nbsp; WMT'14 En-De</li>
                <li><b>Dialogue Task</b>: &nbsp; Daily Dialogue</li>
                <li><b>Summarization Task</b>: &nbsp; CNN Daily</li>
                <li><b>Tokenizer</b>: &nbsp; Word-Level Tokenizer</li>
              </ul>
              <ul>
                <li><b>Train Data Volumn</b>: &nbsp; 50,000</li>
                <li><b>Valid Data Volumn</b>: &nbsp; 5,000</li>
                <li><b>Test Data Volumn</b>: &nbsp; 100</li>
                <li><b>Vocab Size</b>: &nbsp; 15,000</li>                
              </ul>
            </div>
          </li>

          <div class="spacer"></div>
          <li>Model Setup
            <div class="dual-container">
              <ul>
                <li><b>Input Dim</b>: &nbsp; 15,000</li>
                <li><b>Output Dim</b>: &nbsp; 15,000</li>
                <li><b>Hidden Dim</b>: &nbsp; 256</li>
                <li><b>PFF Dim</b>: &nbsp; 512</li>
              </ul>
              <ul>
                <li><b>N Layers</b>: &nbsp; 3</li>
                <li><b>N Heads</b>: &nbsp; 8</li>
                <li><b>Model Params</b>: &nbsp; 000</li>
                <li><b>Model Size</b>: &nbsp; 000</li>                
              </ul>
            </div>
          </li>

          <div class="spacer"></div>
          <li>Training Setup</li>

          <div class="code-container">
            <div class="code-snippet">
              <pre><code class="python">TrainingArguments(
    output_dir= f'ckpt/{strategy}',
    num_train_epochs= 5,
    learning_rate= 1e-5,
    per_device_train_batch_size= 32,
    per_device_eval_batch_size= 32,
    lr_scheduler_type='reduce_lr_on_plateau',
    load_best_model_at_end= True,

    save_strategy= 'epoch',
    logging_strategy= 'epoch',
    evaluation_strategy= 'epoch',

    fp16= True if config.strategy in ['fp16', 'all'] else False,
    fp16_opt_level= '02' if config.strategy in ['fp16', 'all'] else '01',
    gradient_accumulation_steps = True if config.strategy in ['grad_accumulation', 'all'] else 4,
    gradient_checkpointing= True if config.strategy in ['grad_checkpointing', 'all'] else False,
    optim = 'adafactor' if config.strategy in ['optim', 'all'] else 'adamw_torch'
)</code></pre>
            </div>
          </div>

        </ul>
      </div>
    </div>



    <h2 id="5">&hairsp; 5. &hairsp; Result</h2>
    <div class="chapter-box">

      <div class="list-container">
        <ul>
          <li>Machine Translation</li>

          <table class="result-table">
            <thead>
              <tr>
                <th>Training Strategy</th>
                <th>Training Time</th>
                <th>Accuracy</th>
                <th>Avg GPU Memory</th>
              </tr>
            </thead>
            <tbody>
              <tr>
                <td>Vanilla Training</td>
                <td>174</td>
                <td>7.00</td>
                <td>0.79GB</td>
              </tr>
              <tr>
                <td>Mixed Precision (FP 16)</td>
                <td>69</td>
                <td>5.47</td>
                <td>0.78GB</td>
              </tr>
              <tr>
                <td>Gradient Accumulation</td>
                <td>182</td>
                <td>6.29</td>
                <td>0.83GB</td>
              </tr>

              <tr>
                <td>Gradient Checkpointing</td>
                <td>239</td>
                <td>3.54</td>
                <td>0.79GB</td>
              </tr>

              <tr>
                <td>Adafactor Optimizer</td>
                <td>179</td>
                <td>6.72</td>
                <td>0.79GB</td>
              </tr>

              <tr>
                <td>All Strategies Applied</td>
                <td>85</td>
                <td>2.91</td>
                <td>0.80GB</td>
              </tr>

            </tbody>
          </table>

        <div class="spacer"></div>  
        <li>Result Analysis</li>
          <p> &nbsp; 위 결과표를 살펴보면, 연산 비용은 모두 비슷하게 나오지만, 속도 개선에 가장 큰 효과가 있었던것은 Mixed Precision이었으며, Gradient Accumulation과 Gradient Checkpointing, Adafactor Optimizer는 성능 하락과 더불어 오히려 학습 시간을 증가시키는 부정적인 효과를 보였습니다.
          추가적으로 모든 전략을 한번에 사용하는 것은 여러가지 기법들이 뒤엉켜 성능적으로 가장 많은 하락을 야기했습니다.   
          </p>
      </ul>
    </div>
    </div>



    <h2 id="6">&hairsp; 6. &hairsp; Conclusion</h2>
    <div class="chapter-box">
      <p>&nbsp; 이 프로젝트에서는 효율적인 학습을 위한 네 가지 방안을 자연어 분류 과제에서 구현하고 그 결과를 살펴봤습니다. 여러 방안들 중에 Mixed Precision이 가장 큰 효율성 제고를 보여줬고, 특정 방법들은 성능 하락과 동시에 효율성 마저 떨어지는 경우도 있었습니다. <br>
      효율성 제고를 위해 다양한 방안에서의 실험을 우선적으로 실시할 필요성이 있음을 확인했습니다. 
      </p>
    </div>



    <h2 id="7">&hairsp; 7. &hairsp; Reference</h2>
    <div class="chapter-box">
      <a class="reference" href="https://huggingface.co/docs/transformers/perf_train_gpu_one">&nbsp; Huggingface Efficient Training Post</a>

    </div>



  </div>
</div>



<div class="pagination">
  <a href="{{ '/gan_train' | relative_url }}" class="btn-prev"><span>SeqGAN</span></a>
  <a href="{{ '/eff_model' | relative_url }}" class="btn-next"><span>Efficient Models</span></a>
</div>