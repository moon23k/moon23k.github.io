---
layout: default
permalink: /intelligen
---


<div id="detail">
  <h1 class="title">IntelliGEN</h1>  
  <div class="post">

    <hr>
    <div class="callout">
      <p class="highlighted-text"><span>Project Objective</span></p>
      <div class="small-spacer"></div>
      <p>&nbsp; 이 프로젝트에서 소규모 데이터 셋에서 Transformer의 Generative Training 방식의 효용성을 검증합니다. <br>
      일반적으로 Transformer는 Attention을 활용한 병렬 학습의 효율성을 바탕으로, 대규모 데이터셋에서 학습을 진행합니다. 하지만 가용한 데이터가 매우 제한적인 경우, 오히려 생성적 학습이 효과적일 수 있다는 가설을 세우고, 이를 검증하는 일련의 실험을 진행합니다. 생성 과정에서 효율성을 증진시키기 위해 Cache를 활용하는 Transformer를 직접 구현해 사용합니다.
      </p>
    </div>

    <div class="tblContents">
      <ul>

        <li>
          <a href="#1">1.&hairsp; Introduction</a>
        </li>

        <li>
          <a href="#2">2.&hairsp; Background</a>
        </li>

        <li>
          <a href="#3">3.&hairsp; Training Strategy</a>
        </li>

        <li>
          <a href="#4">4.&hairsp; Experimental Setup</a>
        </li>

        <li>
          <a href="#5">5.&hairsp; Result</a>
        </li>

        <li>
          <a href="#6">6.&hairsp; Conclusion</a>
        </li>

        <li>
          <a href="#7">7.&hairsp; Reference</a>
        </li>

      </ul>
    </div>
    <hr>


<!-- 1. Introduction -->    
    <h2 id="1">&hairsp; 1.&hairsp; Introduction</h2>
    <div class="chapter-box">

      <div class="list-container">
        <ul>
          <li>Description</li>
            <p> &nbsp; 일반적으로 Transformer는 Attention 기반의 연산으로 병렬처리를 통해 대규모 데이터를 바탕으로 학습을 진행합니다. 이런 학습 방식을 통해 뛰어난 성능을 자랑하지만, 가용한 데이터가 제한적인 경우 일반적인 학습 방식만으로는 원하는 성능을 기록하기 어렵습니다. 데이터를 더욱 확보하여 해결하는 것이 가장 이상적인 방법이지만, 일반적인 사용자가 충분한 양질의 데이터를 확보하는 것은 현실적으로 매우 어려운 일입니다.<br>
            때문에 이 프로젝트에서는 데이터가 제한적인 경우, 추론과정과 동일한 Generative Training 방식으로 Transformer의 성능을 향상시킬수 있을것이라는 가설을 세우고, 직접 가설을 검증해봅니다.
            학습 과정에서 효율적인 생성을 병행하기 위해 디코딩 과정에서 Cache를 사용할 수 있는 Transformer 모델을 구현해 사용합니다. 성능 평가는 기계번역과 대화생성 과제에서 진행하며, 비교군으로는 일반적인 학습방식, 데이터를 증강시킨 경우에서의 학습, 생성적 학습 그리고 일반적인 학습 이후 이어지는 생성적 학습으로 총 네가지 경우의 수를 비교해봅니다.  
            </p>

          <div class="spacer"></div>
          <li>Objective
            <ul>
              <li style="font-weight: normal;">소규모 데이터셋에서 Generative Training의 효용성 검증</li>
              <li style="font-weight: normal;">Cache를 활용해 효울적인 생성이 가능한 Transformer 구현</li>
              <li style="font-weight: normal;">네 가지 학습 경우의 수를 변인으로 두어 다각적인 비교 검증</li>
            </ul>
          </li>

        </ul>
      </div>

    </div>


<!-- 2. Background -->    
    <h2 id="2">&hairsp; 2. &hairsp; Background</h2>
    <div class="chapter-box">

      <div class="list-container">
        <ul>
          <li>Transformer
            <p>&nbsp; Transformer는 자연어 처리 및 기타 시퀀스 기반 작업에 매우 효과적인 신경망 아키텍처입니다. 이 아키텍처의 중요한 특징 중 하나는 어텐션 메커니즘을 통한 병렬 학습에 대한 뛰어난 지원입니다. 기존의 순환 신경망(RNN)과 비교하여 트랜스포머는 많은 입력 토큰을 동시에 처리할 수 있어 훈련 속도를 크게 향상시킵니다. 
            </p>
          </li>


          <div class="spacer"></div>
          <li>Cache Usable Transformer
            <div class="code-container">
              <div class="code-snippet">
                <pre><code class="python">class DecoderLayer(nn.TransformerDecoderLayer):
    def forward(
        self,
        x,
        memory=None,
        e_mask=None,
        d_mask=None,
        use_cache=False
    ):

        if not use_cache:
            return super().forward(
                x,
                memory,
                memory_key_padding_mask=e_mask,
                tgt_mask=d_mask
            )


        last_token = x[:, -1:, :]

        # self attention part
        _x = self.self_attn(last_token, x, x)[0]

        last_token = last_token + self.dropout1(_x)
        last_token = self.norm1(last_token)


        # encoder-decoder attention
        _x = self.multihead_attn(
            last_token, memory, memory,
            key_padding_mask=e_mask,
        )[0]

        last_token = last_token + self.dropout2(_x)
        last_token = self.norm2(last_token)

        # final feed-forward network
        _x = self.activation(self.linear1(last_token))
        _x = self.linear2(self.dropout(_x))
        last_token = last_token + self.dropout3(_x)
        last_token = self.norm3(last_token)
        
        return last_token



class Decoder(nn.TransformerDecoder):

    def forward(
        self,
        x,
        memory=None,
        cache=None,
        e_mask=None,
        d_mask=None,
        use_cache=True
    ):

        output = x

        #In case of not using Cache
        if not use_cache:
            for layer in self.layers:
                output = layer(output, memory, e_mask, d_mask, False)
            return output, None

        #In case of using Cache
        new_token_cache = []
        for idx, layer in enumerate(self.layers):
            output = layer(output, memory, use_cache=True)
            new_token_cache.append(output)
            
            if cache is not None:  
                output = torch.cat([cache[idx], output], dim=1)

        new_cache = torch.stack(new_token_cache, dim=0)

        if cache is not None:
            new_cache = torch.cat([cache, new_cache], dim=2)

        return output, new_cache




class Transformer(nn.Module):
    def __init__(self, config):
        super(Transformer, self).__init__()

        self.bos_id = config.bos_id
        self.eos_id = config.eos_id
        self.pad_id = config.pad_id

        self.device = config.device
        self.max_len = config.max_len
        self.model_type = config.model_type
        self.vocab_size = config.vocab_size

        self.enc_emb = Embeddings(config)
        self.encoder = Encoder(config)

        self.dec_emb = Embeddings(config)
        self.decoder = Decoder(
            DecoderLayer(
                d_model=config.hidden_dim, 
                nhead=config.n_heads, 
                dim_feedforward=config.pff_dim,
                batch_first=True
            ),
            num_layers=config.n_layers,
        )

        self.generator = nn.Linear(config.hidden_dim, self.vocab_size)

        self.out = namedtuple('Out', 'logit loss')
        self.criterion = nn.CrossEntropyLoss()


    @staticmethod
    def shift_y(y):
        return y[:, :-1], y[:, 1:]


    def pad_mask(self, x):
        return x == self.pad_id


    def dec_mask(self, x):
        sz = x.size(1)
        return torch.triu(torch.full((sz, sz), float('-inf')), diagonal=1).to(self.device)


    def encode(self, x, x_mask):
        x = self.enc_emb(x)
        x = self.encoder(x, x_mask)
        return x


    def decode(
        self, x, memory, cache=None, 
        e_mask=None, d_mask=None, use_cache=False
        ):
        
        x = self.dec_emb(x)
        x, cache = self.decoder(x, memory, cache, e_mask, d_mask, use_cache)
        return x, cache        
        

    def teacher_forcing_forward(self, x, y):
        y, label = self.shift_y(y)
        
        e_mask = self.pad_mask(x)
        d_mask = self.dec_mask(y)

        memory = self.encode(x, e_mask)

        dec_out, _ = self.decode(y, memory, None, e_mask, d_mask, use_cache=False)
        logit = self.generator(dec_out)

        self.out.logit = logit
        self.out.loss = self.criterion(
            logit.contiguous().view(-1, self.vocab_size), 
            label.contiguous().view(-1)
        )

        return self.out
    

    def generative_forward(self, x, y):

        _, label = self.shift_y(y)
        batch_size, output_len = label.shape
        logit = torch.empty(batch_size, output_len, self.vocab_size).to(self.device)
        

        pred = torch.zeros((batch_size, 1), dtype=torch.long)
        pred = pred.fill_(self.bos_id).to(self.device)

        cache=None
        e_mask = self.pad_mask(x)
        memory = self.encode(x, e_mask)

        for idx in range(1, output_len+1):
            y = pred[:, :idx]
            d_out, cache = self.decode(y, memory, cache, e_mask, use_cache=True)

            curr_logit = self.generator(d_out[:, -1:, :])
            curr_pred = curr_logit.argmax(dim=-1)

            logit[:, idx-1:idx, :] = curr_logit
            pred = torch.cat([pred, curr_pred], dim=1)
        
        self.out.logit = logit
        self.out.loss = self.criterion(
            logit.contiguous().view(-1, self.vocab_size), 
            label.contiguous().view(-1)
        )        

        return self.out
        


    def forward(self, x, y):
        if self.model_type == 'generative':
            return self.generative_forward(x, y)
        return self.teacher_forcing_forward(x, y)</code></pre>
              </div>
            </div>
          </li>


        <ul>          
      </div>
    </div>


<!-- 3. Training Strategy -->    
    <h2 id="3">3.&hairsp; Training Strategy</h2>
    <div class="chapter-box">

      <div class="list-container">
        <ul>
          <li>Standard&hairsp; Training
            <ul>
              <li></li>
              <li></li>
              <li></li>
            </ul>
          </li>

          <div class="spacer"></div>
          <li>Generative&hairsp; Fine-Tuning
            <ul>
              <li></li>
              <li></li>
              <li></li>
            </ul>
          </li>

          <div class="spacer"></div>
          <li>SlowGAN&hairsp; Fine-Tuning
            <ul>
              <li></li>
              <li></li>
              <li></li>
            </ul>
          </li>

          <div class="spacer"></div>
          <li>Overall&hairsp; Process
            <ul>
              <li>1. Standard Training으로 일반화 능력 배양</li>
              <li>2. Generative Fine-Tuning으로 생성 능력 향상</li>
              <li>3. SlowGAN Fine-Tuning으로 보다 자연스러운 생성 능력 향상</li>
            </ul>
          </li>

        </ul>          
      </div>
    </div>


<!-- 3. Experimental Setup -->
    <h2 id="3">&hairsp; 3. &hairsp; Experimental Setup</h2>
    <div class="chapter-box">

      <div class="list-container">
        <ul>
          <li>Data Setup
            <div class="dual-container">
              <ul>
                <li>Translation Task: &nbsp; WMT'14 En-De</li>
                <li>Dialogue Task: &nbsp; Daily Dialogue</li>
                <li>Tokenizer: &nbsp; BPE Tokenizer</li>
                <li>Vocab Size: &nbsp; 10,000</li>
              </ul>
              <ul>
                <li>Train Data Volumn: &nbsp; 50,000</li>
                <li>Train Data Volumn for Augment Training: &nbsp; 50,000</li>
                <li>Valid Data Volumn: &nbsp; 5,000</li>
                <li>Test Data Volumn: &nbsp; 100</li>
                                
              </ul>
            </div>
          </li>

          <div class="half-spacer"></div>
          <li>Model Setup
            <div class="dual-container">
              <ul>
                <li>Architecture: &nbsp; Transformer</li>
                <li>Input Dim: &nbsp; 15,000</li>
                <li>Output Dim: &nbsp; 15,000</li>
              </ul>
              <ul>
                <li>Hidden Dim: &nbsp; 256</li>
                <li>Pff Dim: &nbsp; 512</li>
                <li>Model Params: &nbsp; 15,488,664</li>
                <li>Model Size: &nbsp; 60.085 MB</li>                
              </ul>
            </div>
          </li>

          <div class="half-spacer"></div>
          <li>Training Setup
            <div class="dual-container">
              <ul>
                <li>Num Epochs: &nbsp; 10</li>
                <li>Batch Size: &nbsp; 32</li>
                <li>Learning Rate: &nbsp; 5e-4</li>
                <li>LR Scheduler: &nbsp; ReduceLROnPlateau</li>
              </ul>
              <ul>
                <li>Optimizer: &nbsp; AdamW</li>
                <li>Apply Gradient Checkpoint: &nbsp; True</li>
                <li>Gradient Accumulation Steps: &nbsp; 4</li>
                <li>Early Stop Patient: &nbsp; 3</li> 
              </ul>
            </div>
          </li>
        </ul>
      </div>
    </div>


<!-- 4. Result -->
    <h2 id="4">&hairsp; 4. &hairsp; Result</h2>
    <div class="chapter-box">

      <div class="list-container">
        <ul>
          <li>Result&hairsp; Table</li>

            <table class="result-table">
              <thead>
                <tr>
                  <th>Training Type</th>
                  <th>Score</th>
                  <th>Epoch Time</th>
                  <th>Avg GPU</th>
                  <th>Max GPU</th>
                </tr>
              </thead>
              <tbody>
                <tr>
                  <td>Standard</td>
                  <td>3.36</td>
                  <td>0m 11s</td>
                  <td>0.20GB</td>
                  <td>0.85GB</td>
                </tr>
                <tr>
                  <td>Generative</td>
                  <td>0.00</td>
                  <td>4m 40s</td>
                  <td>1.92GB</td>
                  <td>3.86GB</td>
                </tr>
                <tr>
                  <td>Consecutive</td>
                  <td>0.66</td>
                  <td>4m 49s</td>
                  <td>1.92GB</td>
                  <td>3.86GB</td>
                </tr>
                <tr>
                  <td>Augement</td>
                  <td>11.95</td>
                  <td>1m 00s</td>
                  <td>0.21GB</td>
                  <td>0.87GB</td>
                </tr>

              </tbody>
            </table>
          



          <div class="half-spacer"></div>  
          <li>Result&hairsp; Analysis</li>
            <p> &nbsp; 두 가지 과제에서 모두 Generative Training은 성능과 효율성 측면에서 가장 낮은 결과를 보이며, 효용성이 매우 떨어짐을 확인할 수 있습니다.
              Transformer의 학습에서 Generative Training이 얼마나 비효율적인지 직접 확인할 수 있었습니다. 
            </p>
        </ul>
      </div>
    </div>


<!-- 5. Conclusion -->
    <h2 id="5">&hairsp; 5. &hairsp; Conclusion</h2>
    <div class="chapter-box">
      <p>&nbsp; 프로젝트의 시작에서 가용한 데이터의 수가 제한적인 경우, Generative Training 방식이 Transformer 성능 증진에 도움이 될 것이라는 가설을 세웠습니다. 결과를 확인해보면, Transformer와 Generative Training은 상성이 매우 좋지 않음을 확인할 수 있었습니다. 이를 통해 가설이 틀렸음을 검증함과 동시에, Transformer의 성능 개선을 위해 Data의 확보가 얼마나 중요한지 다시 한번 확인할 수 있었습니다.
      </p>

    </div>


<!-- 6. Reference -->
    <h2 id="6">&hairsp; 6. &hairsp; Reference</h2>
    <div class="chapter-box">
      <a class="reference" href="https://arxiv.org/abs/1706.03762">&nbsp; Attention Is All You Need</a> 
    </div>

  </div>
</div>


<div class="pagination">
  <a href="{{ '/scheduled_sampling' | relative_url }}" class="btn-prev"><span>Scheduled Sampling</span></a>
  <a href="{{ '/gan_train' | relative_url }}" class="btn-next"><span>SeqGAN Training</span></a>
</div>