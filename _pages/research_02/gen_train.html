---
layout: default
permalink: /gen_train
---


<div id="detail">
  <h1 class="title">Generative Training Research</h1>  
  <div class="post">

    <hr>
    <div class="callout">
      <p class="highlighted-text"><span>Project Objective</span></p>
      <div class="small-spacer"></div>
      <p>&nbsp; 이 프로젝트에서 소규모 데이터 셋에서 Transformer의 Generative Training 방식의 효용성을 검증합니다. <br>
      일반적으로 Transformer는 Attention을 활용한 병렬 학습의 효율성을 바탕으로, 대규모 데이터셋에서 학습을 진행합니다. 하지만 가용한 데이터가 매우 제한적인 경우, 오히려 생성적 학습이 효과적일 수 있다는 가설을 세우고, 이를 검증하는 일련의 실험을 진행합니다. 생성 과정에서 효율성을 증진시키기 위해 Cache를 활용하는 Transformer를 직접 구현
      </p>
    </div>

    <div class="tblContents">
      <ul>

        <li>
          <a href="#1">1. &hairsp; Introduction</a>
        </li>

        <li>
          <a href="#2">2. &hairsp; Background</a>
        </li>

        <li>
          <a href="#3">3. &hairsp; Training</a>
        </li>

        <li>
          <a href="#4">4. &hairsp; Experimental Setup</a>
        </li>

        <li>
          <a href="#5">5. &hairsp; Result</a>
        </li>

        <li>
          <a href="#6">6. &hairsp; Conclusion</a>
        </li>

        <li>
          <a href="#7">7. &hairsp; Reference</a>
        </li>

      </ul>
    </div>
    <hr>


<!-- 1. Introduction -->    
    <h2 id="1">&hairsp; 1. &hairsp; Introduction</h2>
    <div class="chapter-box">

      <div class="list-container">
        <ul>
          <li>Description</li>
            <p> &nbsp; 일반적으로 Transformer는 <br>
            본 프로젝트의 주요 목표는 Exposure Bias를 최소화하여 모델이 훈련 시와 테스트 시에 일관된 예측을 수행할 수 있도록 하는 것입니다. 이를 위해 Auxiliary Training을 도입하며, 특히 First Token Prediction을 목적으로 합니다.
            </p>

          <br class="half-spacer">
          <li>Objective
            <ul>
              <li style="font-weight: normal;">소규모 데이터셋에서 Generative Training의 효용성 검증</li>
              <li style="font-weight: normal;">Cache를 활용해 효울적인 생성이 가능한 Transformer 구현</li>
              <li style="font-weight: normal;">데이터를 증가시킨 상황에서의 일반적인 훈련 방식에서의 성능과도 비교하며, Transformer 학습 방식 및 상황별 성능 변화 확인</li>
            </ul>
          </li>

        </ul>
      </div>

    </div>


<!-- 2. Background -->    
    <h2 id="2">&hairsp; 2. &hairsp; Background</h2>
    <div class="chapter-box">

      <div class="list-container">
        <ul>
          <li>Transformer
            <p>&nbsp; Transformer Seq2Seq 모델은 언어 생성 작업에서 뛰어난 성과를 보여주며, Attention 메커니즘을 통해 문맥을 파악하는 능력으로 자연어 처리 분야에서 널리 활용되고 있습니다. 그러나, 특히 긴 문장에서 Exposure Bias라는 문제가 나타나기도 합니다.
            </p>
          </li>


          <div class="spacer"></div>
          <li>Cache Usable Transformer
            <div class="code-container">
              <div class="code-snippet">
                <pre><code class="python">class DecoderLayer(nn.TransformerDecoderLayer):
    def forward(
        self,
        x,
        memory=None,
        e_mask=None,
        d_mask=None,
        use_cache=False
    ):

        if not use_cache:
            return super().forward(
                x,
                memory,
                memory_key_padding_mask=e_mask,
                tgt_mask=d_mask
            )


        last_token = x[:, -1:, :]

        # self attention part
        _x = self.self_attn(last_token, x, x)[0]

        last_token = last_token + self.dropout1(_x)
        last_token = self.norm1(last_token)


        # encoder-decoder attention
        _x = self.multihead_attn(
            last_token, memory, memory,
            key_padding_mask=e_mask,
        )[0]

        last_token = last_token + self.dropout2(_x)
        last_token = self.norm2(last_token)

        # final feed-forward network
        _x = self.activation(self.linear1(last_token))
        _x = self.linear2(self.dropout(_x))
        last_token = last_token + self.dropout3(_x)
        last_token = self.norm3(last_token)
        
        return last_token



class Decoder(nn.TransformerDecoder):

    def forward(
        self,
        x,
        memory=None,
        cache=None,
        e_mask=None,
        d_mask=None,
        use_cache=True
    ):

        output = x

        #In case of not using Cache
        if not use_cache:
            for layer in self.layers:
                output = layer(output, memory, e_mask, d_mask, False)
            return output, None

        #In case of using Cache
        new_token_cache = []
        for idx, layer in enumerate(self.layers):
            output = layer(output, memory, use_cache=True)
            new_token_cache.append(output)
            
            if cache is not None:  
                output = torch.cat([cache[idx], output], dim=1)

        new_cache = torch.stack(new_token_cache, dim=0)

        if cache is not None:
            new_cache = torch.cat([cache, new_cache], dim=2)

        return output, new_cache




class Transformer(nn.Module):
    def __init__(self, config):
        super(Transformer, self).__init__()

        self.bos_id = config.bos_id
        self.eos_id = config.eos_id
        self.pad_id = config.pad_id

        self.device = config.device
        self.max_len = config.max_len
        self.model_type = config.model_type
        self.vocab_size = config.vocab_size

        self.enc_emb = Embeddings(config)
        self.encoder = Encoder(config)

        self.dec_emb = Embeddings(config)
        self.decoder = Decoder(
            DecoderLayer(
                d_model=config.hidden_dim, 
                nhead=config.n_heads, 
                dim_feedforward=config.pff_dim,
                batch_first=True
            ),
            num_layers=config.n_layers,
        )

        self.generator = nn.Linear(config.hidden_dim, self.vocab_size)

        self.out = namedtuple('Out', 'logit loss')
        self.criterion = nn.CrossEntropyLoss()


    @staticmethod
    def shift_y(y):
        return y[:, :-1], y[:, 1:]


    def pad_mask(self, x):
        return x == self.pad_id


    def dec_mask(self, x):
        sz = x.size(1)
        return torch.triu(torch.full((sz, sz), float('-inf')), diagonal=1).to(self.device)


    def encode(self, x, x_mask):
        x = self.enc_emb(x)
        x = self.encoder(x, x_mask)
        return x


    def decode(
        self, x, memory, cache=None, 
        e_mask=None, d_mask=None, use_cache=False
        ):
        
        x = self.dec_emb(x)
        x, cache = self.decoder(x, memory, cache, e_mask, d_mask, use_cache)
        return x, cache        
        

    def teacher_forcing_forward(self, x, y):
        y, label = self.shift_y(y)
        
        e_mask = self.pad_mask(x)
        d_mask = self.dec_mask(y)

        memory = self.encode(x, e_mask)

        dec_out, _ = self.decode(y, memory, None, e_mask, d_mask, use_cache=False)
        logit = self.generator(dec_out)

        self.out.logit = logit
        self.out.loss = self.criterion(
            logit.contiguous().view(-1, self.vocab_size), 
            label.contiguous().view(-1)
        )

        return self.out
    

    def generative_forward(self, x, y):

        _, label = self.shift_y(y)
        batch_size, output_len = label.shape
        logit = torch.empty(batch_size, output_len, self.vocab_size).to(self.device)
        

        pred = torch.zeros((batch_size, 1), dtype=torch.long)
        pred = pred.fill_(self.bos_id).to(self.device)

        cache=None
        e_mask = self.pad_mask(x)
        memory = self.encode(x, e_mask)

        for idx in range(1, output_len+1):
            y = pred[:, :idx]
            d_out, cache = self.decode(y, memory, cache, e_mask, use_cache=True)

            curr_logit = self.generator(d_out[:, -1:, :])
            curr_pred = curr_logit.argmax(dim=-1)

            logit[:, idx-1:idx, :] = curr_logit
            pred = torch.cat([pred, curr_pred], dim=1)
        
        self.out.logit = logit
        self.out.loss = self.criterion(
            logit.contiguous().view(-1, self.vocab_size), 
            label.contiguous().view(-1)
        )        

        return self.out
        


    def forward(self, x, y):
        if self.model_type == 'generative':
            return self.generative_forward(x, y)
        return self.teacher_forcing_forward(x, y)</code></pre>
              </div>
            </div>
          </li>


        <ul>          
      </div>
    </div>


<!-- 2. Training -->    
    <h2 id="3">3. &hairsp; Training</h2>
    <div class="chapter-box">

      <div class="list-container">
        <ul>
          <li>Standard Training
            <p>&nbsp; Auxiliary Training은 Main이 되는 Training Objective에 더불어 추가적인 Training Objective를 두고 학습을 진행하는 방식을 의미합니다.
              다만, 일반적인 생성방식으로 학습하는 것은 너무 오랜시간이 소요되기 때문에, 효율성을 위해 생성을 위한 디코딩과정에 Cache를 활용하는 모델을 구현하여 사용합니다.
              코드 구현은 아래와 같습니다.
            </p>
          </li>

          <div class="spacer"></div>
          <li>Data Augment Training
            <p>&nbsp; Auxiliary Training은 Main이 되는 Training Objective에 더불어 추가적인 Training Objective를 두고 학습을 진행하는 방식을 의미합니다.
              다만, 일반적인 생성방식으로 학습하는 것은 너무 오랜시간이 소요되기 때문에, 효율성을 위해 생성을 위한 디코딩과정에 Cache를 활용하는 모델을 구현하여 사용합니다.
              코드 구현은 아래와 같습니다.
            </p>
          </li>

          <div class="spacer"></div>
          <li>Generative Training
            <p>&nbsp; Auxiliary Training은 Main이 되는 Training Objective에 더불어 추가적인 Training Objective를 두고 학습을 진행하는 방식을 의미합니다.
              다만, 일반적인 생성방식으로 학습하는 것은 너무 오랜시간이 소요되기 때문에, 효율성을 위해 생성을 위한 디코딩과정에 Cache를 활용하는 모델을 구현하여 사용합니다.
              코드 구현은 아래와 같습니다.
            </p>
          </li>

          <div class="spacer"></div>
          <li>Consecutive Training
            <p>&nbsp; Auxiliary Training은 Main이 되는 Training Objective에 더불어 추가적인 Training Objective를 두고 학습을 진행하는 방식을 의미합니다.
              다만, 일반적인 생성방식으로 학습하는 것은 너무 오랜시간이 소요되기 때문에, 효율성을 위해 생성을 위한 디코딩과정에 Cache를 활용하는 모델을 구현하여 사용합니다.
              코드 구현은 아래와 같습니다.
            </p>
          </li>

        </ul>          
      </div>
    </div>


<!-- 3. Experimental Setup -->
    <h2 id="3">&hairsp; 3. &hairsp; Experimental Setup</h2>
    <div class="chapter-box">

      <div class="list-container">
        <ul>
          <li>Data Setup
            <div class="dual-container">
              <ul>
                <li>Translation Task: &nbsp; WMT'14 En-De</li>
                <li>Dialogue Task: &nbsp; Daily Dialogue</li>
                <li>Tokenizer: &nbsp; BPE Tokenizer</li>
                <li>Vocab Size: &nbsp; 10,000</li>
              </ul>
              <ul>
                <li>Train Data Volumn: &nbsp; 50,000</li>
                <li>Train Data Volumn for Augment Training: &nbsp; 50,000</li>
                <li>Valid Data Volumn: &nbsp; 5,000</li>
                <li>Test Data Volumn: &nbsp; 100</li>
                                
              </ul>
            </div>
          </li>

          <div class="half-spacer"></div>
          <li>Model Setup
            <div class="dual-container">
              <ul>
                <li>PLE Architecture: &nbsp; AlBERT</li>
                <li>PLE Name: &nbsp; albert-v2</li>
                <li>Input Dim: &nbsp; 10,000</li>
                <li>Output Dim: &nbsp; 10,000</li>
              </ul>
              <ul>
                <li>Embedding Dim: &nbsp; 512</li>
                <li>Hidden Dim: &nbsp; 512</li>
                <li>Model Params: &nbsp; 000</li>
                <li>Model Size: &nbsp; 000</li>                
              </ul>
            </div>
          </li>

          <div class="half-spacer"></div>
          <li>Training Setup
            <div class="dual-container">
              <ul>
                <li>Num Epochs: &nbsp; 10</li>
                <li>Batch Size: &nbsp; 32</li>
                <li>Learning Rate: &nbsp; 5e-4</li>
                <li>LR Scheduler: &nbsp; pleature</li>
              </ul>
              <ul>
                <li>Optimizer: &nbsp; AdamW</li>
                <li>Gradient Accumulation Steps: &nbsp; 4
                <li>Teacher Forcing Ratio: &nbsp; 0.5</li>                
              </ul>
            </div>
          </li>
        </ul>
      </div>
    </div>


<!-- 4. Result -->
    <h2 id="4">&hairsp; 4. &hairsp; Result</h2>
    <div class="chapter-box">

      <div class="list-container">
        <ul>
          <li>Machine Translation</li>

            <table class="result-table">
              <thead>
                <tr>
                  <th>Training Type</th>
                  <th>Score</th>
                  <th>Epoch Time</th>
                  <th>Avg GPU</th>
                  <th>Max GPU</th>
                </tr>
              </thead>
              <tbody>
                <tr>
                  <td>Standard</td>
                  <td>3.36</td>
                  <td>0m 11s</td>
                  <td>0.20GB</td>
                  <td>0.85GB</td>
                </tr>
                <tr>
                  <td>Generative</td>
                  <td>0.00</td>
                  <td>4m 40s</td>
                  <td>1.92GB</td>
                  <td>3.86GB</td>
                </tr>
                <tr>
                  <td>Consecutive</td>
                  <td>0.66</td>
                  <td>4m 49s</td>
                  <td>1.92GB</td>
                  <td>3.86GB</td>
                </tr>
                <tr>
                  <td>Augement</td>
                  <td>11.95</td>
                  <td>1m 00s</td>
                  <td>0.21GB</td>
                  <td>0.87GB</td>
                </tr>

              </tbody>
            </table>
          
          <div class="half-spacer"></div>
          <li>Dialogue Generation</li>
            <table class="result-table">
              <thead>
                <tr>
                  <th>Training Type</th>
                  <th>Score</th>
                  <th>Epoch Time</th>
                  <th>Avg GPU</th>
                  <th>Max GPU</th>
                </tr>
              </thead>
              <tbody>
                <tr>
                  <td>Standard</td>
                  <td>0.41</td>
                  <td>0m 10s</td>
                  <td>0.20GB</td>
                  <td>0.83GB</td>
                </tr>
                <tr>
                  <td>Generative</td>
                  <td>0.00</td>
                  <td>4m 35s</td>
                  <td>1.88GB</td>
                  <td>3.85GB</td>
                </tr>
                <tr>
                  <td>Consecutive</td>
                  <td>0.10</td>
                  <td>4m 40s</td>
                  <td>1.88GB</td>
                  <td>3.85GB</td>
                </tr>
                <tr>
                  <td>Augement</td>
                  <td>2.67</td>
                  <td>0m 45s</td>
                  <td>0.20GB</td>
                  <td>0.85GB</td>
                </tr>

              </tbody>
            </table>


          <div class="half-spacer"></div>  
          <li>Result Analysis</li>
            <p> &nbsp; 두 가지 과제에서 모두 Generative Training은 성능과 효율성 측면에서 가장 낮은 결과를 보이며, 효용성이 매우 떨어짐을 확인할 수 있습니다.
              Transformer의 학습에서 Generative Training이 얼마나 비효율적인지 직접 확인할 수 있었습니다.
            </p>
        </ul>
      </div>
    </div>


<!-- 5. Conclusion -->
    <h2 id="5">&hairsp; 5. &hairsp; Conclusion</h2>
    <div class="chapter-box">

      <div class="list-container">
        <ul>
          <li>RNN, LSTM, GRU 을 활용한 seq2seq 모델링 방식의 이해</li>
            <p>&nbsp; 토큰의 벡터 변환을 위한 Embedding Layer와 실제 연산을 위한 순환 신경망 레이어만으로 구성된 단순산 Encoder, Decoder 구조로 이루어져 있으며, 
            </p>

          <div class="half-spacer"></div>
          <li>세 가지 순환 신경망의 성능 비교 검증</li>
            <p>LSTM은 게이트 메커니즘을 통해 장기 의존성 문제를 해결하는 데 강점을 보입니다. 복잡한 문맥을 학습하고 기억하는 데 탁월하며, 긴 시퀀스 데이터에서 뛰어난 성능을 발휘할 수 있습니다. 하지만 LSTM은 더 많은 파라미터를 가지고 있어서 더 많은 데이터와 계산 리소스가 필요할 수 있습니다.
            </p>
            

          <br class="half-spacer">
          <li>최소 기준치 확립</li>
            <p>앞서 언급했듯이 순환 신경망을 사용한 Seq2Seq모델은 앞으로 다루게 될 다양한 Seq2Seq 모델들의 시작점입니다. 고도화 된 모델일지라도 하이퍼 파라미터 설정이나, 훈련 방식 등의 이유에서 잘못된 성능이 나올수도 있습니다. 이때, 최소한의 동작은 하는 구나 하는 판단의 근거로 다음과 같은 스코어를 활용할 수 있습니다. <br> 
              기계번역 BLEU Score: 00.00, 대화 생성 Rouge Score: 00.00, 문서 요약 Rouge Score: 00.00 이하의 성능ㅇ
            </p>

        </ul>
      </div>
    </div>


<!-- 6. Reference -->
    <h2 id="6">&hairsp; 6. &hairsp; Reference</h2>
    <div class="chapter-box">
      <a class="reference" href="https://arxiv.org/abs/1706.03762">&nbsp; Attention Is All You Need</a> 
    </div>

  </div>
</div>


<div class="pagination">
  <a href="{{ '/scheduled_sampling' | relative_url }}" class="btn-prev"><span>Scheduled Sampling</span></a>
  <a href="{{ '/gan_train' | relative_url }}" class="btn-next"><span>SeqGAN Training</span></a>
</div>