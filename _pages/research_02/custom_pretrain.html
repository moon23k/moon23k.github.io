---
layout: default
permalink: /custom_pretrain
---


<div id="detail">
  <h1 class="title">Customized PreTraining Analytics</h1>  
  <div class="post">

    <hr>
    <div class="callout">
      <p class="highlighted-text"><span>Project Objective</span></p>
      <div class="small-spacer"></div>
      <p>&nbsp; 다양한 예시를 통해 사전학습 모델을 사용하는 것이 매우 효과적이라는 것이 입증되었습니다.
        대부분의 사전학습 모델들의 사이즈는 매우 크며, 다양한 분야에 적용가능하도록 일반화 되어 있습니다.

        사전학습을 원하는 데이터 도메인에서 적용한다면, 특정 태스크에서는 더 좋은 성능을 보일 것이라는 가설을 세우고, 직접 검증해보는 것을 목표로 합니다.

        이 프로젝트에서는 실제 적용을 원하는 특정 태스크의 데이터를 바탕으로 다양한 전략의 사전학습을 직접 수행하고, 
        이렇게 사전학습된 모델의 실제 성능을 비교해봅니다. 
      </p>
    </div>

    <div class="tblContents">
      <ul>

        <li>
          <a href="#1">1. Pre Train</a>
          <ul>
            <li>
              <a href="#1.1">1.1 Language Modeling</a>
            </li>
            <li>
              <a href="#1.2">1.2 Masked Language Modeling</a>
            </li>
            <li>
              <a href="#1.3">1.3 Pre Training Model</a>
            </li>          
          </ul>
        </li>
        
        <li>
          <a href="#2">2. Architecure</a>
        </li>

        <li>
          <a href="#3">3. Experimental Setup</a>
        </li>

        <li>
          <a href="#4">4. Results</a>
        </li>

        <li>
          <a href="#5">5. Conclusion</a>
        </li>

        <li>
          <a href="#6">6. Reference</a>
        </li>

      </ul>
    </div>
    <hr>


    <h2>1. &hairsp; Pre Train</h2>

      <div class="half-spacer"></div>
      <h3>Pretrained Model</h3>
      <div class="small-spacer"></div>

      <p>
        &nbsp; 자연어 처리에서 대규모 데이터와 모델 사이즈를 기반으로 사전학습된 모델들은, 뛰어난 언어 이해 및 처리 능력을 보유 하며, 다양한 분야에서 좋은 성능을 보이고 있습니ㅏㄷ.
        하지만, 모델의 사이즈가 과도하게 크다는 점과, 필요이상의 지식이 너무 많다는 점이 특정 상황에서는 불필요한 경우가 있을 수도 있음.
        이를 해결하며, 기존 사전학습 모델에 대한 의존도를 낮출수 있는 소규모 데이터에서의 Custom 사전학습 모델의 성능 비교라는 연구의 부재를 해결하기 위해, 이 프로젝트를 구상 및 진행.
      </p>


      <div class="spacer"></div>
      <h3>Custom Pre Train</h3>
      <div class="small-spacer"></div>
      <p> &nbsp; 여러가지 과제에서 적용가능한 범용성을 얻긴하는데, 너무 많은 정보를 포함하고 있어. 지나치게 비대함.
        조금 더 narrow down된 지식만을 사전학습시키는 것의 필요성도 있음.
        그래서 이 프로젝트에서는 사용자의 필요에 맞게 사전학습을 진행.

        세 가지 경우의 수를 비교할 예정
        하나는 사전학습을 하지 않은 기본 Transformer 모델
        둘째는 BERT와 같이 Encoder만을 사전학습시킨 모델
        셋째는 BART와 같이 Encoder와 Decoder를 모두 사전학습 시킨 모델


        이 과정에서 사전학습을 위한 Training Objective는 MLM만 사용.
      </p>  




    <h2>2. &hairsp; Experimental Setup</h2>

      <div class="small-spacer"></div>
      <h3>Data Setup</h3>
      <div class="small-spacer"></div>
      <p>Sequence Classification을 위한 대표적 데이터 셋 중하나인 AG_News를 사용.
        데이터의 크기는 Train/ Valid/ Test 1000/ 100/ 100으로 설정
        총 네가지 데이터 Label을 모든 데이터에서 고르게 분배해서 한쪽으로 치우치지 않도록 가공
      </p>

      <div class="spacer"></div>
      <h3>Model Setup</h3>
      <div class="small-spacer"></div>
      <p>대표적인 PreTrained Model인 BERT를 사용.
      </p>


      <div class="spacer"></div>
      <h3>Training Setup</h3>
      <div class="small-spacer"></div>
      <p>대표적인 PreTrained Model인 BERT를 사용.
      </p>


    <h2>3. &hairsp; Result</h2>
    <div class="small-spacer"></div>
    <table class="result-table">
      <thead>
        <tr>
          <th>Training Strategy</th>
          <th>Machine Translation</th>
          <th>Dialogue Generation</th>
          <th>Text Summarizaiton</th>
        </tr>
      </thead>
      <tbody>
        <tr>
          <td>Vanilla Training</td>
          <td>-</td>
          <td>-</td>
          <td>-</td>
        </tr>
        <tr>
          <td>Auxiliary 0.1</td>
          <td>결과2-1</td>
          <td>결과2-2</td>
          <td>결과2-3</td>
        </tr>
        <tr>
          <td>Auxiliary 0.3</td>
          <td>결과2-1</td>
          <td>결과2-2</td>
          <td>결과2-3</td>
        </tr>

        <tr>
          <td>Auxiliary 0.5</td>
          <td>결과2-1</td>
          <td>결과2-2</td>
          <td>결과2-3</td>
        </tr>

      </tbody>
    </table>


    <h2>4. &hairsp; Conclusion</h2>
    <div class="small-spacer"></div>
    <p>결과 분석.
    </p>


  </div>
</div>

<div class="pagination">
  <a href="{{ '/peft' | relative_url }}" class="btn-prev"></a>
  <a href="{{ '/gen_train' | relative_url }}" class="btn-next"></a>
</div>