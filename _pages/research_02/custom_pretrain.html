---
layout: default
permalink: /custom_pretrain
---


<div id="detail">
  <h1 class="title">Customized PreTraining Analytics</h1>  
  <div class="post">

    <hr>
    <div class="callout">
      <p class="highlighted-text"><span>Project Objective</span></p>
      <div class="small-spacer"></div>
      <p>&nbsp; 이 프로젝트에서는 다양한 사전학습 방법론을 통해 Down Stream Task 데이터로 모델을 사전학습 시키고, 그 성능을 비교 검증해봅니다.
      사전 학습 Objective로는 Language Modeling과 Masked Language Modeling을 변인으로 하고, 사전 학습 모델에서는 Encoder만 사전학습 하는 경우와, 전체 모델을 사전학습하는 두가지 변인을 둠으로써, 총 4가지 경우의 수를 비교합니다.
      성능비교를 위한 과제로는 기계번역, 대화생성, 문서요약을 선정했으며, 결과는 ~~하게 나왔습니다. 
      </p>
    </div>


    <div class="tblContents">
      <ul>

        <li>
          <a href="#1">1. &hairsp; Introduction</a>
        </li>

        <li>
          <a href="#2">2. &hairsp; Training Methodology</a>
        </li>

        <li>
          <a href="#3">3. &hairsp; Experimental Setup</a>
        </li>

        <li>
          <a href="#4">4. &hairsp; Result</a>
        </li>

        <li>
          <a href="#5">5. &hairsp; Conclusion</a>
        </li>

        <li>
          <a href="#6">6. &hairsp; Reference</a>
        </li>

      </ul>
    </div>
    <hr>



<!-- 1. Introduction -->    
    <h2 id="1">1. &hairsp; Introduction</h2>
    <div class="chapter-box">

      <div class="list-container">
        <ul>
          <li>Description</li>
            <p> &nbsp; 자연어 처리에서 대규모 데이터와 모델 사이즈를 기반으로 사전학습된 모델들은, 뛰어난 언어 이해 및 처리 능력을 보유 하며, 다양한 분야에서 좋은 성능을 보이고 있습니다.
            하지만, 모델의 사이즈가 과도하게 크다는 점과, 필요이상의 지식이 너무 많다는 점이 특정 상황에서는 불필요한 경우가 있을 수도 있음.
            이를 해결하며, 기존 사전학습 모델에 대한 의존도를 낮출수 있는 소규모 데이터에서의 Custom 사전학습 모델의 성능 비교라는 연구의 부재를 해결하기 위해, 이 프로젝트를 구상 및 진행.
            </p>

          <div class="half-spacer"></div>
          <li>Objective
            <ul>
              <li style="font-weight: normal;">RNN, LSTM, GRU 을 활용한 seq2seq 모델링 방식의 이해</li>
              <li style="font-weight: normal;">세 가지 순환 신경망의 성능 비교 검증</li>
              <li style="font-weight: normal;">가설에 대한 검증</li>
              <li style="font-weight: normal;">실험 결과를 통해, 이후 자연어 생성을 위한 모델링 성능의 기준점을 확립</li>
            </ul>
          </li>

        </ul>
      </div>

    </div>


<!-- Training -->    
    <h2 id="2">2. &hairsp; Training Strategy</h2>
      <div class="half-spacer"></div>
      <h3>Pretrained Model</h3>
      <div class="small-spacer"></div>

      <p>
        &nbsp; 
      </p>


      <div class="spacer"></div>
      <h3>Custom Pre Train</h3>
      <div class="small-spacer"></div>
      <p> &nbsp; 여러가지 과제에서 적용가능한 범용성을 얻긴하는데, 너무 많은 정보를 포함하고 있어. 지나치게 비대함.
        조금 더 narrow down된 지식만을 사전학습시키는 것의 필요성도 있음.
        그래서 이 프로젝트에서는 사용자의 필요에 맞게 사전학습을 진행.

        세 가지 경우의 수를 비교할 예정
        하나는 사전학습을 하지 않은 기본 Transformer 모델
        둘째는 BERT와 같이 Encoder만을 사전학습시킨 모델
        셋째는 BART와 같이 Encoder와 Decoder를 모두 사전학습 시킨 모델


        이 과정에서 사전학습을 위한 Training Objective는 MLM만 사용.
      </p>  




    <h2>2. &hairsp; Experimental Setup</h2>

      <div class="small-spacer"></div>
      <h3>Data Setup</h3>
      <div class="small-spacer"></div>
      <p>Sequence Classification을 위한 대표적 데이터 셋 중하나인 AG_News를 사용.
        데이터의 크기는 Train/ Valid/ Test 1000/ 100/ 100으로 설정
        총 네가지 데이터 Label을 모든 데이터에서 고르게 분배해서 한쪽으로 치우치지 않도록 가공
      </p>

      <div class="spacer"></div>
      <h3>Model Setup</h3>
      <div class="small-spacer"></div>
      <p>대표적인 PreTrained Model인 BERT를 사용.
      </p>


      <div class="spacer"></div>
      <h3>Training Setup</h3>
      <div class="small-spacer"></div>
      <p>대표적인 PreTrained Model인 BERT를 사용.
      </p>


    <h2>3. &hairsp; Result</h2>
    <div class="small-spacer"></div>
    <table class="result-table">
      <thead>
        <tr>
          <th>Training Strategy</th>
          <th>Machine Translation</th>
          <th>Dialogue Generation</th>
          <th>Text Summarizaiton</th>
        </tr>
      </thead>
      <tbody>
        <tr>
          <td>Vanilla Training</td>
          <td>-</td>
          <td>-</td>
          <td>-</td>
        </tr>
        <tr>
          <td>Auxiliary 0.1</td>
          <td>결과2-1</td>
          <td>결과2-2</td>
          <td>결과2-3</td>
        </tr>
        <tr>
          <td>Auxiliary 0.3</td>
          <td>결과2-1</td>
          <td>결과2-2</td>
          <td>결과2-3</td>
        </tr>

        <tr>
          <td>Auxiliary 0.5</td>
          <td>결과2-1</td>
          <td>결과2-2</td>
          <td>결과2-3</td>
        </tr>

      </tbody>
    </table>


<!-- 6. Conclusion -->
    <h2 id="6">6. &hairsp; Conclusion</h2>
    <div class="chapter-box">

      <div class="list-container">
        <ul>
          <li>RNN, LSTM, GRU 을 활용한 seq2seq 모델링 방식의 이해</li>
            <p>&nbsp; 토큰의 벡터 변환을 위한 Embedding Layer와 실제 연산을 위한 순환 신경망 레이어만으로 구성된 단순산 Encoder, Decoder 구조로 이루어져 있으며, 
            </p>

          <br class="half-spacer">
          <li>세 가지 순환 신경망의 성능 비교 검증</li>
            <p>LSTM은 게이트 메커니즘을 통해 장기 의존성 문제를 해결하는 데 강점을 보입니다. 복잡한 문맥을 학습하고 기억하는 데 탁월하며, 긴 시퀀스 데이터에서 뛰어난 성능을 발휘할 수 있습니다. 하지만 LSTM은 더 많은 파라미터를 가지고 있어서 더 많은 데이터와 계산 리소스가 필요할 수 있습니다.
            </p>

          <br class="half-spacer">
          <li>가설 검증
            <ul>
              <li>LSTM, GRU, RNN 순으로 높은 성능</li>
                <p> LSTM의 연산과정이 가장 복잡하고, 다양한 GATE를 사용하기 떄문에, 좋은 성능을 낼 것이라고 예측했으나, 되려 복잡한 연산으로 인해, 학습 시 수렴에 어려움이 생겼습니다.
                오히려 GATE를 사용하면서도, 연산의 단순화를 도모한 GRU의 학습 성능이 가장 좋은 것을 확인했습니다
                </p>

              <li>RNN, GRU, LSTM 순으로 빠른 학습
                <p> 학습 속도 및 GPU 사용량으로 확인해본 효율성은 가설과 마찬가지로 RNN, GRU, LSTM 순으로 나타났습니다.
                </p>
              </li>
            </ul>
          </li>
            

          <br class="half-spacer">
          <li>최소 기준치 확립</li>
            <p>앞서 언급했듯이 순환 신경망을 사용한 Seq2Seq모델은 앞으로 다루게 될 다양한 Seq2Seq 모델들의 시작점입니다. 고도화 된 모델일지라도 하이퍼 파라미터 설정이나, 훈련 방식 등의 이유에서 잘못된 성능이 나올수도 있습니다. 이때, 최소한의 동작은 하는 구나 하는 판단의 근거로 다음과 같은 스코어를 활용할 수 있습니다. <br> 
              기계번역 BLEU Score: 00.00, 대화 생성 Rouge Score: 00.00, 문서 요약 Rouge Score: 00.00 이하의 성능ㅇ
            </p>

        </ul>
      </div>
    </div>


  </div>
</div>

<div class="pagination">
  <a href="{{ '/peft' | relative_url }}" class="btn-prev"></a>
  <a href="{{ '/gen_train' | relative_url }}" class="btn-next"></a>
</div>