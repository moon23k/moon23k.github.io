---
layout: default
permalink: /ease
---


<div id="detail">
  <h1 class="title">Exposure bias&hairsp; Alleviation&hairsp; Strategy&hairsp; Exploration</h1>  
  <div class="post">

    <hr>
    <div class="callout">
      <p class="highlighted-text"><span>Project Objective</span></p>
      <div class="small-spacer"></div>
      <p>&nbsp; 이 프로젝트에서 자연어 생성 모델의 고질적인 문제점인 <b>Exposure Bias</b>의 완화를 위한 세 가지 방법론을 제시하고, 그 실효성을 직접 검증합니다. 각 방법론 Auxiliary, Recurrent Decoding, Generative 학습법입니다. 
      모델 아키텍처는 효율적인 생성을 위해 캐싱 기능을 추가한 표준 트랜스포머를 사용하며, 대표적인 자연어 생성 과제인 기계 번역과제를 통해 생성 능력의 향상 여부를 면밀히 조사합니다.  <br> 
      프로젝트의 결과, 세가지 방법론에서 모두 기본적인 미세조정보다 우수한 생성 능력을 학습 시킬 수 있음을 확인했습니다. 자세한 내용은 아래의 글에서 다루며, 프로젝트의 구현 코드는 github.com/moon23k/EASE에서 확인 하실 수 있습니다
      </p>
    </div>

    <div class="tblContents">
      <ul>

        <li>
          <a href="#1">1. &hairsp; Introduction</a>
        </li>

        <li>
          <a href="#2">2. &hairsp; Background</a>
        </li>

        <li>
          <a href="#3">3. &hairsp; Training</a>
        </li>

        <li>
          <a href="#4">4. &hairsp; Experimental Setup</a>
        </li>

        <li>
          <a href="#5">5. &hairsp; Result</a>
        </li>

        <li>
          <a href="#6">6. &hairsp; Conclusion</a>
        </li>

        <li>
          <a href="#7">7. &hairsp; Reference</a>
        </li>

      </ul>
    </div>
    <hr>


<!-- 1. Introduction -->    
    <h2 id="1">&hairsp; 1.&hairsp; Introduction</h2>
    <div class="chapter-box">

      <div class="list-container">
        <ul>
          <li>Project&hairsp; Description</li>
            <p> &nbsp; 트랜스포머 모델은 자연어 생성 과정에서 많은 성과를 이뤘지만, 생성 과정에서 잘못된 토큰의 예측으로 전체적인 문장의 생성과정이 잘못된 방향으로 편향되는 현상인 <b>Exposure Bias</b>에 대한 도전이 남아있습니다. 이 프로젝트는 세 가지 방법론을 활용하여 트랜스포머 모델을 미세조정함으로써, Exposure Bias를 완화시키는 것을 목표로 합니다. 이때 사용하는 세가지 방법론은 <b>Auxiliary Training, Recurrent Decode Training, Generative Training</b>입니다. Auxiliary Training은 MLE라는 주요 학습 목표와 더불어 첫번째 토큰을 예측하는 보조 학습 목표를 활용하며, Recurrent Decode Training은 Scheduled Sampling에서 영감을 받아, 디코더의 출력값을 다시금 디코더의 입력값으로 사용합니다. 그리고 Generative Training은 실제 생성 과정과 동일한 방식으로 학습을 진행하되, 캐싱을 활용해 학습의 일부분에서만 수행해 학습 효율성을 크게 저하시키지 않는 선에서 활용합니다. 이 모든 방법론의 성능 평가는 대표적인 자연어 생성 과제인 기계번역 과제에서 수행하며, 이를 위한 데이터로는 WMT14 En-De 데이터셋을 선정했습니다.
            </p>

          <br class="spacer">
          <br class="spacer">
          
          <li>Project&hairsp; Objective
            <ul>
              <li style="font-weight: normal;">Auxiliary Training을 통한 Exposure Bias 개선 여부 확인</li>
              <li style="font-weight: normal;">Auxiliary Training Objective로 First Token Prediction의 적절성 검증</li>
              <li style="font-weight: normal;">Auxiliary Training Ratio에 따른 성능 검토</li>
            </ul>
          </li>

        </ul>
      </div>

    </div>


<!-- 2. Background -->    
    <h2 id="2">2. &hairsp; Background</h2>
    <div class="chapter-box">

      <div class="list-container">
        <ul>
          <li>Exposure Bias
            <p>&nbsp; Exposure Bias는 모델이 훈련할 때와 테스트할 때 서로 다른 토큰의 분포에 노출되어 발생하는 차이에서 기인하는 자연어 생성 모델의 고질적인 문제점입니다. 특히 트랜스포머는 어텐션 기법을 활용한 병렬학습 과정에서 이전 시점의 정답 토큰이 주어지는 Teacher Forcing 방식을 사용합니다. 이는 안정적이고 효율적인 학습을 가능케 하지만, 실제 추론과정에서는 정답 토큰이 주어지지 않고, 오로지 모델의 이전 예측 토큰에만 의존하므로, Exposure Bias라는 문제점이 크게 부각됩니다. 
            </p>
          </li>

          <br class="half-spacer">
          <li>Fine Tuning
            <p>&nbsp; Exposure Bias는 모델이 훈련할 때와 테스트할 때 서로 다른 토큰의 분포에 노출되어 발생하는 차이를 나타냅니다. 이는 모델이 훈련 단계에서는 실제로 관측된 토큰을 보고 예측을 수행하면서 발생하며, 이로 인해 생성된 문장의 일관성과 품질에 영향을 미칠 수 있습니다.
            </p>
          </li>


        <ul>          
      </div>
    </div>


<!-- 3. Training Strategy -->    
    <h2 id="3">&hairsp; 3.&hairsp; Fine Tuning Strategy</h2>
    <div class="chapter-box">

      <div class="list-container">
        <ul>
          <li>Standard Fine Tuning
            <p>&nbsp; Auxiliary Training은 Main이 되는 Training Objective에 더불어 추가적인 Training Objective를 두고 학습을 진행하는 방식을 의미합니다.
            </p>
          </li>


          <br class="spacer">
          <br class="spacer">
          <li>Auxiliary Fine Tuning
            <p>&nbsp; Auxiliary Training은 Main이 되는 Training Objective에 더불어 추가적인 Training Objective를 두고 학습을 진행하는 방식을 의미합니다.
            </p>
          </li>


          <br class="spacer">
          <br class="spacer">
          <li>Recurrent Decode Fine Tuning
            <p>&nbsp; Auxiliary Training은 Main이 되는 Training Objective에 더불어 추가적인 Training Objective를 두고 학습을 진행하는 방식을 의미합니다.
            </p>
          </li>


          <br class="spacer">
          <br class="spacer">
          <li>Generative Fine Tuning
            <p>&nbsp; Auxiliary Training은 Main이 되는 Training Objective에 더불어 추가적인 Training Objective를 두고 학습을 진행하는 방식을 의미합니다.
            </p>
          </li>

        <ul>          
      </div>
    </div>


<!-- 4. Experimental Setup -->
    <h2 id="5">&hairsp; 4.&hairsp; Experimental Setup</h2>
    <div class="chapter-box">

      <div class="list-container">
        <ul>
          <li>Data Setup
            <div class="dual-container">
              <ul>
                <li>Translation Task: &nbsp; WMT'14 En-De</li>
                <li>Dialogue Task: &nbsp; Daily Dialogue</li>
                <li>Summarization Task: &nbsp; CNN Daily</li>
                <li>Tokenizer: &nbsp; AlBERT Tokenizer</li>
              </ul>
              <ul>
                <li>Train Data Volumn: &nbsp; 50,000</li>
                <li>Valid Data Volumn: &nbsp; 5,000</li>
                <li>Test Data Volumn: &nbsp; 100</li>
                <li>Vocab Size: &nbsp; 10,000</li>                
              </ul>
            </div>
          </li>

          <div class="spacer"></div>
          <li>Model Setup
            <div class="dual-container">
              <ul>
                <li>PLE Architecture: &nbsp; AlBERT</li>
                <li>PLE Name: &nbsp; albert-v2</li>
                <li>Input Dim: &nbsp; 10,000</li>
                <li>Output Dim: &nbsp; 10,000</li>
              </ul>
              <ul>
                <li>Embedding Dim: &nbsp; 512</li>
                <li>Hidden Dim: &nbsp; 512</li>
                <li>Model Params: &nbsp; 000</li>
                <li>Model Size: &nbsp; 000</li>                
              </ul>
            </div>
          </li>

          <div class="spacer"></div>
          <li>Training Setup
            <div class="dual-container">
              <ul>
                <li>Num Epochs: &nbsp; 10</li>
                <li>Batch Size: &nbsp; 32</li>
                <li>Learning Rate: &nbsp; 5e-4</li>
                <li>LR Scheduler: &nbsp; ReduceLROnPlateau</li>
              </ul>
              <ul>
                <li>Optimizer: &nbsp; AdamW</li>
                <li>Apply Mixed Precision: &nbsp; True</li>
                <li>Gradient Accumulation Steps: &nbsp; 4
                <li>Teacher Forcing Ratio: &nbsp; 0.5</li>                
              </ul>
            </div>
          </li>
        </ul>
      </div>
    </div>


<!-- 4. Result -->
    <h2 id="4">&hairsp; 4.&hairsp; Result</h2>
    <div class="chapter-box">

      <div class="list-container">
        <ul>
          <li>Result Table</li>

            <table class="result-table">
              <thead>
                <tr>
                  <th>Strategy</th>
                  <th>Score</th>
                  <th>Epoch Time</th>
                  <th>Avg GPU</th>
                  <th>Max GPU</th>
                </tr>
              </thead>
              <tbody>
                <tr>
                  <td>Baseline</td>
                  <td>12.99</td>
                  <td>0m 44s</td>
                  <td>0.20GB</td>
                  <td>0.95GB</td>
                </tr>
                <tr>
                  <td>Vanilla FineTuning</td>
                  <td>2.11</td>
                  <td>1m 00s</td>
                  <td>0.20GB</td>
                  <td>0.95GB</td>
                </tr>
                <tr>
                  <td>Auxiliary FineTuning</td>
                  <td>7.34</td>
                  <td>1m 00s</td>
                  <td>0.20GB</td>
                  <td>0.95GB</td>
                </tr>
                <tr>
                  <td>Recurrent Decode FineTuning</td>
                  <td>6.94</td>
                  <td>1m 00s</td>
                  <td>0.20GB</td>
                  <td>0.95GB</td>
                </tr>

                <tr>
                  <td>Generative FineTuning</td>
                  <td>6.94</td>
                  <td>1m 00s</td>
                  <td>0.20GB</td>
                  <td>0.95GB</td>
                </tr>                

              </tbody>
            </table>
          

          <br class="spacer">
          <br class="spacer">  
          <li>Result Analysis</li>
            <p> &nbsp; RNN은 다양한 정보를 함유하기에 지나치게 단순하고, LSTM은 GATE구조가 복잡하기에, 그만큼 영향을 받은 요소가 많아 학습 수렴이 어렵습니다. 때문에 모든 학습 결과에서는 GATE를 사용하면서도 단순화된 연산으로 학습의 이점이 큰 GRU가 가장 뛰어난 성능을 보임을 알 수 있습니다. 
            </p>
        </ul>
      </div>
    </div>


<!-- 5. Conclusion -->
    <h2 id="5">&hairsp; 5.&hairsp; Conclusion</h2>
    <div class="chapter-box">

      <div class="list-container">
        <ul>
          <li>보조 학습 목표의 효용성 검증</li>
            <p>&nbsp; 토큰의 벡터 변환을 위한 Embedding Layer와 실제 연산을 위한 순환 신경망 레이어만으로 구성된 단순산 Encoder, Decoder 구조로 이루어져 있으며, 
            </p>


          <br class="half-spacer">
          <li>자기 회귀적 디코딩 방식의 효용성 검증
            <ul>
              <li>LSTM, GRU, RNN 순으로 높은 성능</li>
                <p> LSTM의 연산과정이 가장 복잡하고, 다양한 GATE를 사용하기 떄문에, 좋은 성능을 낼 것이라고 예측했으나, 되려 복잡한 연산으로 인해, 학습 시 수렴에 어려움이 생겼습니다.
                오히려 GATE를 사용하면서도, 연산의 단순화를 도모한 GRU의 학습 성능이 가장 좋은 것을 확인했습니다
                </p>

              <li>RNN, GRU, LSTM 순으로 빠른 학습
                <p> 학습 속도 및 GPU 사용량으로 확인해본 효율성은 가설과 마찬가지로 RNN, GRU, LSTM 순으로 나타났습니다.
                </p>
              </li>
            </ul>
          </li>
            

          <br class="half-spacer">
          <li>생성적 학습 방식의 사용법 제시</li>
            <p>트랜스포머 모델을 사용한 생성적 학습은 효율성과 효용성 모든 측면에서 좋지 않은 모습을 보입니다. 트랜스포머라는 모델자체가 대규모데이터를 기반으로 다량학습에 용이한 모델 구조를 띄고 있기 때문에. 
            </p>

        </ul>
      </div>
    </div>


<!-- 6. Reference -->
    <h2 id="6">&hairsp; 6.&hairsp; Reference</h2>
    <div class="chapter-box">
      <a class="reference" href="https://arxiv.org/abs/1706.03762">&nbsp; Attention Is All You Need</a><br>
      <a class="reference" href="https://arxiv.org/abs/2204.01171">&nbsp; Why Exposure Bias Matters</a> 
      
    </div>

  </div>
</div>


<div class="pagination">
  <a href="{{ '/transformer_fusion' | relative_url }}" class="btn-prev"><span>Transformer Fusion</span></a>
  <a href="{{ '/scheduled_sampling' | relative_url }}" class="btn-next"><span>Scheduled Sampling</span></a>
</div>